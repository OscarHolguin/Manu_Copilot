{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3d1ee1-3372-4984-979b-b76705b5928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade llama_index llama-index-llms-azure-openai llama-index-graph-stores-neo4j llama-index-embeddings-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "823ef567-bc8f-4e2b-92c7-9c5b7c389191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilitiesPOC import *\n",
    "\n",
    "from auths import openaisecret, embeddings_secret, adls_secret\n",
    "\n",
    "storage_account =  '{\"storage_account_key\": \"\",\"storage_container\": \"manufacturing-copilot\",\"storage_account\": \"euwdsrg03rsta07dls01\"}'\n",
    "path = \"Documents/Embeddings/\"\n",
    "\n",
    "\n",
    "llmconfig= '{\"mode\": \"langchain\", \"auth\": \"token\", \"model\": \"gpt-35-turbo-16k\", \"deployment_name\": \"mxdrca\",\"api_version\": \"2023-05-15\",\"api_key_secret\":\"\",\"api_endpoint\": \"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "embedconfig= '{\"embedmode\": \"langchain\",\"model\":\"text-embedding-ada-002\",\"deployment_name\":\"azure_embedding\",\"api_key_secret\":\"\",\"embeddingurl\":\"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "\n",
    "\n",
    "storage_account = json.loads(storage_account)\n",
    "storage_account_key, container, storage_account = storage_account.get('storage_account_key'),storage_account.get('storage_container'),storage_account.get('storage_account')\n",
    "\n",
    "#read from file\n",
    "storage_account_key = adls_secret\n",
    "\n",
    "llmconfig = json.loads(llmconfig)\n",
    "embedconfig = json.loads(embedconfig)\n",
    "\n",
    "embedmode, embedmodel,embed_deployment,embed_key, embed_url = embedconfig.values()\n",
    "mode, auth, model, deployment_name, api_version, api_key_secret, api_endpoint = llmconfig.values()\n",
    "\n",
    "api_key_secret = openaisecret\n",
    "embed_key = embeddings_secret\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#GETTING THE SECRETS\n",
    "api_key = api_key_secret\n",
    "\n",
    "\n",
    "embed_key = embed_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b16c8ade-2780-4e66-ba5d-288c0c652ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:29:51 [INFO]: Request URL: 'https://euwdsrg03rsta07dls01.dfs.core.windows.net/manufacturing-copilot?resource=REDACTED&directory=REDACTED&recursive=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Accept': 'application/json'\n",
      "    'User-Agent': 'azsdk-python-storage-dfs/12.14.0 Python/3.11.0 (Windows-10-10.0.22621-SP0)'\n",
      "    'x-ms-date': 'REDACTED'\n",
      "    'x-ms-client-request-id': '837c2b06-d1c9-11ee-aacc-141333031892'\n",
      "    'Authorization': 'REDACTED'\n",
      "No body was attached to the request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to use the LLM use it as msg = HumanMessage(content= your query) \n",
      " llm(messages = [msg]) \n",
      "Using Azure OpenAI\n",
      "Using Azure OpenAI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:29:52 [INFO]: Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json;charset=utf-8'\n",
      "    'Server': 'Windows-Azure-HDFS/1.0 Microsoft-HTTPAPI/2.0'\n",
      "    'x-ms-request-id': '3fae32aa-801f-0036-6ed6-65bc79000000'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'x-ms-client-request-id': '837c2b06-d1c9-11ee-aacc-141333031892'\n",
      "    'Date': 'Thu, 22 Feb 2024 21:29:51 GMT'\n",
      "2024-02-22 14:29:52 [INFO]: Request URL: 'https://euwdsrg03rsta07dls01.blob.core.windows.net/manufacturing-copilot/Documents/Embeddings/embeddings_12022024.pkl'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-range': 'REDACTED'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Accept': 'application/xml'\n",
      "    'User-Agent': 'azsdk-python-storage-dfs/12.14.0 Python/3.11.0 (Windows-10-10.0.22621-SP0)'\n",
      "    'x-ms-date': 'REDACTED'\n",
      "    'x-ms-client-request-id': '84056a2d-d1c9-11ee-9b3c-141333031892'\n",
      "    'Authorization': 'REDACTED'\n",
      "No body was attached to the request\n",
      "2024-02-22 14:29:53 [INFO]: Response status: 206\n",
      "Response headers:\n",
      "    'Content-Length': '73944'\n",
      "    'Content-Type': 'application/octet-stream'\n",
      "    'Content-Range': 'REDACTED'\n",
      "    'Last-Modified': 'Mon, 12 Feb 2024 11:04:17 GMT'\n",
      "    'Accept-Ranges': 'REDACTED'\n",
      "    'ETag': '\"0x8DC2BBA5B6060C1\"'\n",
      "    'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'\n",
      "    'x-ms-request-id': '8cbb74eb-001e-004a-25d6-659286000000'\n",
      "    'x-ms-client-request-id': '84056a2d-d1c9-11ee-9b3c-141333031892'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'x-ms-resource-type': 'REDACTED'\n",
      "    'x-ms-creation-time': 'REDACTED'\n",
      "    'x-ms-blob-content-md5': 'REDACTED'\n",
      "    'x-ms-lease-status': 'REDACTED'\n",
      "    'x-ms-lease-state': 'REDACTED'\n",
      "    'x-ms-blob-type': 'REDACTED'\n",
      "    'Content-Disposition': 'REDACTED'\n",
      "    'x-ms-server-encrypted': 'REDACTED'\n",
      "    'x-ms-owner': 'REDACTED'\n",
      "    'x-ms-group': 'REDACTED'\n",
      "    'x-ms-permissions': 'REDACTED'\n",
      "    'x-ms-acl': 'REDACTED'\n",
      "    'Date': 'Thu, 22 Feb 2024 21:29:52 GMT'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using langchain graph class\n"
     ]
    }
   ],
   "source": [
    "# DBTITLE 1,Define LLM \n",
    "#DEFINE LLM USING AZURE OPEN AI\n",
    "llmrca = getllm(openai=True,azure =True,mode = mode, auth=auth,model=model,dep_name = deployment_name,api_version= api_version,\n",
    "                     api_key =api_key,api_base = api_endpoint)\n",
    "\n",
    "\n",
    "embeddings_model  = getembeddings(model=embedmodel,dep_name=embed_deployment,api_key=embed_key,api_base=embed_url)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "mount_point = '/mnt/copilotembedds'\n",
    "\n",
    "\n",
    "#@async_retry\n",
    "def read_embeddings(fromadls=False,dbricks=False,**kwargs):\n",
    "    if fromadls:\n",
    "        if dbricks:\n",
    "            #reads it into a dataframe first from the mount point\n",
    "            storage_account=storage_account,container=container,mount_point=mount_point,path=path,storage_account_key=storage_account_key\n",
    "            for mount in dbutils.fs.mounts():\n",
    "                if mount.mountPoint.startswith(mount_point):\n",
    "                    print(\"Unmounting mnt point\")\n",
    "                    dbutils.fs.unmount(mount.mountPoint)\n",
    "            \n",
    "            #mount to path\n",
    "            dbutils.fs.refreshMounts() #refresh mounts to avoid errors\n",
    "\n",
    "            mntfiles = mount_to_local(storage_account, container, mount_point,path, storage_account_key)\n",
    "            #reads into dataframe\n",
    "            adlsembedspath = [e.path for e in dbutils.fs.ls(mntfiles)][-1] #read last file \n",
    "            adlsembedspath= adlsembedspath.replace(\"dbfs:\",\"/dbfs\")\n",
    "            df = pd.read_pickle(adlsembedspath)\n",
    "\n",
    "        else:\n",
    "            #read from adls but not from databricks\n",
    "            from azure.storage.filedatalake import DataLakeServiceClient\n",
    "            storage_account,container,path,storage_account_key=kwargs.get(\"storage_account\"),kwargs.get(\"container\"),kwargs.get(\"path\"),kwargs.get(\"storage_account_key\")\n",
    "            connstr = f\"DefaultEndpointsProtocol=https;AccountName={storage_account};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "            service = DataLakeServiceClient.from_connection_string(conn_str=connstr)\n",
    "            file_system_client = service.get_file_system_client(container)\n",
    "            paths = file_system_client.get_paths(path=path)\n",
    "            files = [p for p in paths if not p.is_directory]\n",
    "            files.sort(key=lambda x: x.last_modified)\n",
    "            last_file = files[-1].name\n",
    "            file_client = file_system_client.get_file_client(last_file)\n",
    "            \n",
    "            # Download the file content and load it as a Python object\n",
    "            download = file_client.download_file()\n",
    "            downloaded_bytes = download.readall()\n",
    "            df = pickle.loads(downloaded_bytes)\n",
    "            #df = pd.read_pickle(data)\n",
    "            \n",
    "    else:\n",
    "        #read from local FS\n",
    "        adlsembedspath = [n for n in os.listdir(path)][-1]\n",
    "        \n",
    "        df = pd.read_pickle(adlsembedspath)\n",
    "    #returns the texts and embeddings\n",
    "    txtlist = [t for t in df.texts]\n",
    "    embeddings = [e for e in df.embeddings]\n",
    "    return txtlist,embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_embeddings_fromfs(path):\n",
    "    #service = DataLakeServiceClient.from_connection_string(conn_str=\"my_connection_string\")\n",
    "\n",
    "    adlsembedspath = [n for n in os.listdir(path)][-1]        \n",
    "    df = pd.read_pickle(path+\"/\"+adlsembedspath)\n",
    "    #returns the texts and embeddings\n",
    "    txtlist = [t for t in df.texts]\n",
    "    embeddings = [e for e in df.embeddings]\n",
    "    return txtlist,embeddings\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "loaded_embeddings = read_embeddings(fromadls=True,dbricks=False,storage_account=storage_account,container=container,path=path,storage_account_key=storage_account_key)#read_embeddings_fromfs(path)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import ClassVar\n",
    "import pickle\n",
    "\n",
    "@dataclass \n",
    "class RAG2:\n",
    "    memory_conversation  = None\n",
    "    def __init__(self,llm,graph,type=\"langchain\",embeddings=None,usememory=False):\n",
    "        self.llm = llm\n",
    "        self.graph = graph\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "        self.usememory = usememory\n",
    "\n",
    "    @staticmethod\n",
    "    def memory(type='langchain',memory_key=\"chat_history\",output_key='result',rmessages=True,readmem=None):\n",
    "        if readmem:\n",
    "            if type.lower()==\"langchain\":\n",
    "                from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "                memory = ConversationBufferMemory(\n",
    "                memory_key=memory_key, return_messages=rmessages)\n",
    "                #output_key=output_key)\n",
    "                \n",
    "                readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "\n",
    "                return memory\n",
    "        else: \n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def buildPrompt():\n",
    "        from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate,PromptTemplate\n",
    "        promptcop = r\"\"\" Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "        Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "        you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "        ----\n",
    "        {context}\n",
    "        ---- \"\"\"\n",
    "        general_system_template =promptcop \n",
    "        general_user_template = \"Question:```{question}```\"\n",
    "        messages = [\n",
    "        SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "        HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "        ]\n",
    "        copilotprompt = ChatPromptTemplate.from_messages( messages )\n",
    "        return copilotprompt\n",
    "        \n",
    "    \n",
    "\n",
    "    def vector_retriever(self,store=\"chroma\",**kwargs):\n",
    "        texts = kwargs.get(\"texts\")\n",
    "        if store.lower()==\"chroma\":\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "\n",
    "            from langchain.chains import RetrievalQA\n",
    "            persist_directory = kwargs.get(\"persist_directory\",\"filesdb\")\n",
    "            search_kwargs = kwargs.get(\"search_kwargs\",{\"k\": 3})\n",
    "            from langchain.vectorstores import Chroma\n",
    "            \n",
    "            db = Chroma.from_documents(texts,self.embeddings,persist_directory= persist_directory)\n",
    "            vec_retriever = db.as_retriever(search_kwargs=search_kwargs)\n",
    "         \n",
    "            return vec_retriever  \n",
    "\n",
    "            \n",
    "        elif store.lower()==\"faiss\":\n",
    "            #to be implemented\n",
    "            from langchain_community.vectorstores import FAISS\n",
    "            from langchain_core.output_parsers import StrOutputParser\n",
    "            from langchain_core.prompts import ChatPromptTemplate\n",
    "            from langchain_core.runnables import RunnablePassthrough\n",
    "            from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "            if kwargs.get(\"text_embeddings\"):\n",
    "                text_embeddings =kwargs.get(\"text_embeddings\")\n",
    "                vectorstore = FAISS.from_embeddings(text_embeddings=list(zip(text_embeddings[0],text_embeddings[1])), embedding = self.embeddings)\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                return retriever\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                template = \"\"\"Answer the question based only on the following context:{context} \n",
    "                Question: {query}\"\"\"\n",
    "                prompt = ChatPromptTemplate.from_template(template)\n",
    "                retrieval_chain = ({\"context\": retriever, \"query\": RunnablePassthrough()} | prompt\n",
    "                                    | self.llm| StrOutputParser())\n",
    "                return retrieval_chain\n",
    "        \n",
    "    def build_chain(self,**kwargs):\n",
    "        #memory = self.memory() if (readmem := self.memory()) else None\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        \n",
    "        #PROMPT TEMPLATE        \n",
    "        from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate,PromptTemplate\n",
    "        #####\n",
    "\n",
    "        memory = self.usememory\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "\n",
    "        if memory:\n",
    "            prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {question}\")\n",
    "        else:\n",
    "            prompt = PromptTemplate(input_variables=[\"question\"],template =\"{question}\")\n",
    "\n",
    "        self.memory_conversation = self.memory(readmem=memory) if not self.memory_conversation else self.memory_conversation\n",
    "\n",
    "        if kwargs.get(\"falkor\"):\n",
    "            from langchain.chains import FalkorDBQAChain\n",
    "            chain = FalkorDBQAChain.from_llm(llm=self.llm, graph=self.graph, verbose=verbose, memory=self.memory_conversation, **kwargs)\n",
    "        else:\n",
    "            if method.lower()==\"graph\":\n",
    "                #general chain for graphs\n",
    "                print(\"building graph chain\")\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                #Prompt template\n",
    "                qa_prompt = self.buildPrompt()#ChatPromptTemplate.from_messages( messages )\n",
    "\n",
    "                chain = GraphCypherQAChain.from_llm(llm=self.llm, prompt=prompt,graph=self.graph,validate_cypher = True, \n",
    "                                                    verbose=verbose,memory=self.memory_conversation, promptTemplate=qa_prompt,**kwargs)\n",
    "                chain.input_key = \"question\"\n",
    "\n",
    "            elif method.lower()==\"vector\":\n",
    "                print(\"Building vector chain \")\n",
    "                retriever = kwargs.get(\"vec_retriever\")\n",
    "                from langchain.chains import RetrievalQA\n",
    "                from langchain.chains import ConversationalRetrievalChain\n",
    "                \n",
    "                # template = (\"Combine the chat history and follow up question into \"\n",
    "                # \"a standalone question. Chat History: {chat_history}\"\n",
    "                # \"Follow up question: {query} \")\n",
    "                template = (\"Every time you respond you should start with This is Copilot your AI assistant\")\n",
    "                prompt = PromptTemplate.from_template(template)\n",
    "                \n",
    "\n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,return_source_documents=True,verbose=verbose)\n",
    "                self.qa_prompt = self.buildPrompt()#ChatPromptTemplate.from_messages( messages )\n",
    "                \n",
    "                chain = ConversationalRetrievalChain.from_llm(self.llm,retriever,memory=self.memory_conversation,get_chat_history=lambda h :h,\n",
    "                                                              verbose=verbose,\n",
    "                                                              combine_docs_chain_kwargs={\"prompt\": self.qa_prompt}) \n",
    "                \n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,\n",
    "                #                                    return_source_documents=True,verbose=verbose,memory=self.memory_conversation,prompt=prompt)\n",
    "        return chain\n",
    "\n",
    "    def get_agent_executor(self,**kwargs):\n",
    "        #combines vector and graph retrievers\n",
    "        vector = self.get_retriever(method=\"graph\")\n",
    "        pass\n",
    "\n",
    "\n",
    "    def get_retriever(self,**kwargs):\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        complexity = kwargs.get(\"complexity\",\"simple\")\n",
    "        \n",
    "        if method.lower()=='graph':\n",
    "            if self.type.lower()==\"langchain\":\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                if complexity.lower()=='simple':\n",
    "                    print(\"SIMPLE RAG LANGCHAIN\")\n",
    "                    if usememory := kwargs.get(\"usememory\") or self.memory_conversation:\n",
    "                        print(\"With Read memory\")\n",
    "                    \n",
    "                    chain = self.build_chain(**kwargs)\n",
    "                    return chain\n",
    "                \n",
    "                    \n",
    "            elif self.type.lower()==\"llama\":\n",
    "                if complexity.lower()=='simple':\n",
    "                    nl2graph = kwargs.get(\"nl2graph\",False)\n",
    "                    print(nl2graph)\n",
    "                    from llama_index.query_engine import RetrieverQueryEngine\n",
    "                    from llama_index.retrievers import KnowledgeGraphRAGRetriever                    \n",
    "                    graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=kwargs.get(\"storage_context\"),\n",
    "                    service_context=kwargs.get(\"service_context\"),\n",
    "                    llm=self.llm,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=nl2graph)\n",
    "                    query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))\n",
    "                    return query_engine\n",
    "\n",
    "                \n",
    "                elif complexity.lower()==\"custom\":\n",
    "                    if self.type.lower()==\"llama\":\n",
    "                        from llama_index import get_response_synthesizer\n",
    "                        from llama_index.query_engine import RetrieverQueryEngine\n",
    "                        # create custom retriever\n",
    "                        vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "                        kg_retriever = KGTableRetriever(index=kg_index, retriever_mode=\"keyword\", include_text=False)\n",
    "                        custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "                        # create response synthesizer\n",
    "                        response_synthesizer = get_response_synthesizer(service_context=service_context,response_mode=\"tree_summarize\")\n",
    "                        custom_query_engine = RetrieverQueryEngine(retriever=custom_retriever,response_synthesizer=response_synthesizer)\n",
    "                        return custom_query_engine\n",
    "                    elif self.type.lower()=='langchain':\n",
    "                        from langchain.agents import initialize_agent, Tool\n",
    "                        #combine vector and graph retrievers\n",
    "                        simple = self.get_retriever(\"simple\",**kwargs)\n",
    "                        \n",
    "                        tools = [\n",
    "                            Tool(name= \"GraphRAG\",\n",
    "                                 func= simple.run,\n",
    "                                 description= \"Simple RAG from knowledge graph\"),\n",
    "                            #add more RAG tools\n",
    "                        ]\n",
    "        elif method.lower()==\"vector\":\n",
    "            print(\"Getting vector retriever\")\n",
    "            vec_retriever = self.vector_retriever(**kwargs)\n",
    "\n",
    "            chain = self.build_chain(vec_retriever=vec_retriever,**kwargs)\n",
    "            return chain\n",
    "\n",
    "\n",
    "\n",
    "    def response(self,query,complexity=\"simple\",**kwargs):\n",
    "        method = kwargs.get(\"method\",\"vector\")\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "        retriever = self.get_retriever(**kwargs)\n",
    "\n",
    "        try:\n",
    "            if method.lower()==\"vector\":\n",
    "                print(\"I AM USING VECTOR LETS GO\")\n",
    "                return retriever({\"question\": query}).get(\"answer\")\n",
    "                #result =  retriever({\"question\": query}).get('answer')\n",
    "                \n",
    "            elif method.lower()==\"graph\":\n",
    "                if self.type.lower()=='langchain':\n",
    "                    print('using retriever({\"query\": query}).get(\"answer\")')\n",
    "                    #result = retriever({\"question\": query}).get('answer')#\n",
    "                    result = retriever.invoke({\"question\": query}).get('result') \n",
    "                    pickled_str = pickle.dumps(self.memory_conversation)\n",
    "                elif self.type.lower()=='llama':\n",
    "                    result = retriever.query(query)\n",
    "                    if 'Empty Response' in result.response:\n",
    "                        raise(\"Error empty result or no information found\")\n",
    "                    else:\n",
    "                        return result.response\n",
    "            \n",
    "            if not result or \"I'm sorry\" or \"I apologize\" in result:\n",
    "                print('Not using RAG')\n",
    "                raise (\"Error empty result or no information found in the KG\")\n",
    "            elif \"I'm sorry, but I don't have\" in result:\n",
    "                raise(\"ERROR 404\")\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if self.type.lower()=='langchain':\n",
    "                from langchain.schema import HumanMessage\n",
    "                #message = HumanMessage(content=query)\n",
    "                from langchain.chains import ConversationChain\n",
    "                from langchain.prompts.prompt import PromptTemplate\n",
    "                if self.usememory:\n",
    "                    print(\"Normal LLM response with same memory\")\n",
    "                    #prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {query}\")\n",
    "                    from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "                    from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate,PromptTemplate\n",
    "\n",
    "                    general_system_template = r\"\"\" \n",
    "                    Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                    Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                    you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                    ----\n",
    "                    {context}\n",
    "                    ----\n",
    "                    \"\"\"\n",
    "                    general_user_template = \"Question:```{question}```\"\n",
    "                    messages = [\n",
    "                                SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                                HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                    ]\n",
    "\n",
    "                    prompt = self.buildPrompt()#ChatPromptTemplate(messages=[messages])\n",
    "\n",
    "                    chat = ConversationChain(llm=self.llm,verbose=kwargs.get(\"verbose\",True),prompt=prompt,memory=self.memory_conversation ,get_chat_history=lambda h :h,)#input_key=\"question\")\n",
    "                    chat.input_key = \"question\"\n",
    "                    return chat.invoke({\"question\":query}).get(\"response\")\n",
    "                else:\n",
    "                    from langchain.schema import HumanMessage\n",
    "                    message = HumanMessage(content=query)\n",
    "                    return self.llm.invoke([message])\n",
    "\n",
    "            else:\n",
    "                #return response with llama index self.llm(response)\n",
    "                pass\n",
    "            \n",
    "\n",
    "# COMMAND ----------\n",
    "#start graph\n",
    "url =\"neo4j+s://b75e0bd8.databases.neo4j.io\"\n",
    "username =\"neo4j\"\n",
    "password = \"aziMmumvuVru-rgeKuO0zT7mEtTxu29Jgrmp6lFKv0w\"\n",
    "graph_store = get_graphdb(\"langchain\",graphdb = \"neo4j\",url=url,username=username,password=password)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b413a12a-eb7c-4f86-a8d3-21c548e5f240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vector retriever\n",
      "Building vector chain \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:26:33 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "        Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "        you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "        ----\n",
      "        ﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0001198\n",
      "Issue Description: Variations in the quality and accuracy of produced machined parts on Assembly Line 7\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.51 ohms, Voltage fluctuation : 4%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "        ---- \n",
      "Human: Question:```where and why did the production stoppage occurred```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:26:34 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a thorough analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) should be conducted. This analysis will help identify the specific factors that led to the stoppage, such as equipment malfunction, environmental conditions, maintenance issues, or other contributing factors.\n"
     ]
    }
   ],
   "source": [
    "rag2 = RAG2\n",
    "vecrag = rag2(llmrca,graph_store,type=\"langchain\",embeddings = embeddings_model,usememory=True)\n",
    "vectorchain = vecrag.get_retriever(method=\"vector\",text_embeddings = loaded_embeddings,store=\"faiss\")\n",
    "\n",
    "print(vectorchain({\"question\": \"where and why did the production stoppage occurred\"}).get(\"answer\")) #if use memory we call it like this \n",
    "print(vecrag.response(query=\"where and why did the production stoppage occurred\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8f95529-57cb-4674-b450-c501a713a1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vector retriever\n",
      "Building vector chain \n",
      "I AM USING VECTOR LETS GO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:30:50 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "        Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "        you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "        ----\n",
      "        ﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0001198\n",
      "Issue Description: Variations in the quality and accuracy of produced machined parts on Assembly Line 7\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.51 ohms, Voltage fluctuation : 4%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "        ---- \n",
      "Human: Question:```where and why did the production stoppage occurred```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:30:51 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a combination of factors, including the motor surface temperature reaching 50 degrees Celsius, which is higher than the ambient temperature of 22 degrees Celsius. Additionally, the load on the motor was at 70%, and there was a 5% voltage stability issue. These factors likely led to an overheating of the motor, causing it to malfunction and resulting in the production stoppage.\n",
      "Getting vector retriever\n",
      "Building vector chain \n",
      "I AM USING VECTOR LETS GO\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='where and why did the production stoppage occurred'), AIMessage(content='Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a combination of factors, including the motor surface temperature reaching 50 degrees Celsius, which is higher than the ambient temperature of 22 degrees Celsius. Additionally, the load on the motor was at 70%, and there was a 5% voltage stability issue. These factors likely led to an overheating of the motor, causing it to malfunction and resulting in the production stoppage.')]\n",
      "Follow Up Input: what was my previous question?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:30:51 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-22 14:30:52 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "        Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "        you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "        ----\n",
      "        ﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "RCA Reference Guide\n",
      "Please refer the systematic sequence of steps listed below to methodically identify and address root\n",
      "cause issues contributing to various issues in production quality.\n",
      "1)Check the cooling mechanisms: Standard Airflow rate through cooling vents is >50 CFM.\n",
      "2)Evaluate ambient temperature around the motor: Standard Value is 20-25 degrees Celsius.\n",
      "3)Motor surface temperature during operation:  Standard Value is <60 degrees Celsius.\n",
      "If any of the values in steps 1,2 and 3 are found to breach standard values, the likely cause\n",
      "would be Inadequate cooling leading to motor overheating. If not, proceed to the next steps\n",
      "4)Check the rated power of the motor: Standard Value is 1.5 HP\n",
      "5)Assess the load on the motor: Standard Value is <80% of rated current.\n",
      "6)Check user workloads: Standard value is <80% of rated workload.\n",
      "If any of the values in steps 4,5 and 6 are found to breach standard values, the likely cause\n",
      "would be Overloading the motor beyond its capacity. If not, proceed to the next steps\n",
      "7)Voltage stability: standard value is +-5%\n",
      "8)Measure resistance in power wiring: Standard power wiring resistance is <0.1 ohms.\n",
      "9)Monitor fluctuations during operation: Standard Value is <2% variation.\n",
      "If any of the values in steps 7,8 and 9 are found to breach standard values, the likely cause\n",
      "would be Insufficient power supply causing fluctuations. If not, proceed to the next steps\n",
      "10)Review maintenance logs: Frequency standard is Monthly.\n",
      "11)Inspect for dust accumulation on motor surfaces: standard value is <1 mm thickness.\n",
      "12)Measure lubrication frequency : Standard Value is every 50 operating hours.\n",
      "If any of the values in steps 10,11 and 12 are found to breach standard values, the likely\n",
      "cause would be lack of regular maintenance, including\n",
      "cleaning and lubrication. If not, the root cause is a new one which is not listed.\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "        ---- \n",
      "Human: Question:```What was the question I asked before?```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:30:52 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I apologize, but I am unable to retrieve the previous question as I am an AI assistant and do not have access to the conversation history. However, I am here to assist you with any new questions or concerns you may have.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='where and why did the production stoppage occurred'), AIMessage(content='Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a combination of factors, including the motor surface temperature reaching 50 degrees Celsius, which is higher than the ambient temperature of 22 degrees Celsius. Additionally, the load on the motor was at 70%, and there was a 5% voltage stability issue. These factors likely led to an overheating of the motor, causing it to malfunction and resulting in the production stoppage.'), HumanMessage(content='what was my previous question?'), AIMessage(content='I apologize, but I am unable to retrieve the previous question as I am an AI assistant and do not have access to the conversation history. However, I am here to assist you with any new questions or concerns you may have.')]), return_messages=True, memory_key='chat_history')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag2 = RAG2\n",
    "vecrag = rag2(llmrca,graph_store,type=\"langchain\",embeddings = embeddings_model,usememory=True)\n",
    "\n",
    "print(vecrag.response(query=\"where and why did the production stoppage occurred\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "print(vecrag.response(query=\"what was my previous question?\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "vecrag.memory_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fce3f3aa-0cbe-4a44-ace6-0f2e12b4a12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vector retriever\n",
      "Building vector chain \n",
      "SIMPLE RAG LANGCHAIN\n",
      "With Read memory\n",
      "building graph chain\n"
     ]
    }
   ],
   "source": [
    "#vector\n",
    "vectorchain= vecrag.get_retriever(method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings,verbose=False)\n",
    "## graoh CHAIN \n",
    "graphchain= vecrag.get_retriever(method=\"graph\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6991a7-573e-4060-b94a-b5312907937b",
   "metadata": {},
   "source": [
    "Testing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d61e05bd-2d29-4d4f-8513-597a3055ae49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:31:50 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-22 14:31:50 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-22 14:31:50 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are four incident tickets.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorchain.invoke({\"question\":\"How many tickets are there?\"}).get(\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60c691db-5002-42ee-bd21-02664bd71074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:31:53 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (s:Entity)-[:CONCLUDES]->(c:Entity)\n",
      "RETURN c\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'c': {'id': 'only \"about 13% of methane emissions are covered\"'}}, {'c': {'id': 'only \"about 13% of methane emissions are covered by methane mitigation policies\"'}}, {'c': {'id': 'effectiveness of methane mitigation policies \"is far from clear\"'}}]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:31:53 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphchain.invoke({\"question\":\"what did the study concluded\"}).get(\"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a69ac14-c683-446d-a8bb-3bead5a61510",
   "metadata": {},
   "source": [
    "## Langchain tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f06435e-7b5b-4161-bea8-08047cfa8b51",
   "metadata": {},
   "source": [
    "To combine vector retriever and Knowledge Graphs we can use langchain tools and an agent executor which combines it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8c88e66-6bbc-45ba-85a9-19d50fd8ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, create_structured_chat_agent\n",
    "\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51c0c388-b1a0-4fb1-ae85-ce9bbce98394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to use the LLM use it as msg = HumanMessage(content= your query) \n",
      " llm(messages = [msg]) \n",
      "Using Azure OpenAI\n",
      "Note: to use the LLM use it as msg = HumanMessage(content= your query) \n",
      " llm(messages = [msg]) \n",
      "Using Azure OpenAI\n",
      "Llama's version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\o_hol\\AppData\\Roaming\\Python\\Python311\\site-packages\\scipy\\__init__.py:169: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#loading \n",
    "llmazure =getllm(openai=True,azure =True,mode = \"langchain\", auth=\"token\",model=\"gpt-35-turbo-16k\",dep_name = \"chat-bot-trail\",            \n",
    "                  api_version= \"2023-12-01-preview\", api_key = \"1bbc7a5ceb0e4677888f22d1eb4c8617\",api_base = \"https://chat-bot-iot.openai.azure.com/\")#([message])\n",
    "#LLAMA'S VERSION\n",
    "llmazure2 =getllm(openai=True,azure =True,mode = \"llama\", auth=\"token\",model=\"gpt-35-turbo-16k\",dep_name = \"chat-bot-trail\",            \n",
    "                  api_version= \"2023-05-15\", api_key = \"1bbc7a5ceb0e4677888f22d1eb4c8617\",api_base = \"https://chat-bot-iot.openai.azure.com/\")#([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11da30b2-6611-4fbb-829c-2653dc6603ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding vector retriever as tool\n",
    "toolslgc = [\n",
    "    \n",
    "    Tool(\n",
    "        name=\"copilot_retriever\",\n",
    "        func=lambda q:vectorchain.invoke({\"question\":q}).get(\"answer\"),\n",
    "        description=\"useful for when you want to answer questions about RCA documents and tickets using FAISS.\",\n",
    "        return_direct=False,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Copilot_Knowledge\",\n",
    "        func=lambda q:graphchain.invoke({\"question\":q}).get(\"result\"),\n",
    "        description=\"useful for when you want to answer questions about neo4j kg using langchain.\",\n",
    "        return_direct=False,\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "llm_with_tools= llmazure.bind_tools(toolslgc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42899724-1336-443a-8889-fa8a20be9ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:34:10 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-22 14:34:11 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-22 14:34:11 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first incident ticket describes a sudden disruption in Production Line 2 during the morning shift due to the unexpected shutdown of the Benchmill, which impacted the production schedule.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:34:12 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (:Entity)-[:CONCLUDES]->(conclusion:Entity)\n",
      "RETURN conclusion\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'conclusion': {'id': 'only \"about 13% of methane emissions are covered\"'}}, {'conclusion': {'id': 'only \"about 13% of methane emissions are covered by methane mitigation policies\"'}}, {'conclusion': {'id': 'effectiveness of methane mitigation policies \"is far from clear\"'}}]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:34:12 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.\n"
     ]
    }
   ],
   "source": [
    "print(toolslgc[0].run(\"what is the first incident ticket about?\"))\n",
    "print(toolslgc[1].run(\"what did the study concluded?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c3af6041-20e8-477f-ad71-885d7eef7cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchainhub in c:\\users\\o_hol\\.conda\\envs\\copilotrca\\lib\\site-packages (0.1.14)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\o_hol\\.conda\\envs\\copilotrca\\lib\\site-packages (from langchainhub) (2.31.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in c:\\users\\o_hol\\.conda\\envs\\copilotrca\\lib\\site-packages (from langchainhub) (2.31.0.20240218)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\o_hol\\.conda\\envs\\copilotrca\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\o_hol\\.conda\\envs\\copilotrca\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\o_hol\\.conda\\envs\\copilotrca\\lib\\site-packages (from requests<3,>=2->langchainhub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\o_hol\\.conda\\envs\\copilotrca\\lib\\site-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of C:\\Users\\o_hol\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib-3.6.3-py3.11-nspkg.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"<frozen site>\", line 186, in addpackage\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"<frozen importlib._bootstrap>\", line 570, in module_from_spec\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\n",
      "\n",
      "Remainder of file ignored\n"
     ]
    }
   ],
   "source": [
    "!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4a86dc3-f437-4811-aa7c-96f37e5ee284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "agent = create_openai_tools_agent(llmazure, toolslgc, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=toolslgc, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e45ccc5-e6c0-44e3-9441-6d97aab79dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "querytemplate= \"try using the functions Copilot_Knowledge first to answer the users query if no answer then the copilot retriever, this is the query:{question}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "318c3a35-3c2d-4317-9653-5758b6a9858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:35:16 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Copilot_Knowledge` with `tell me everything you know about`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:35:18 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (e1:Entity)-[r]->(e2:Entity)\n",
      "RETURN e1, r, e2\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'e1': {'id': 'pair'}, 'r': ({'id': 'pair'}, 'CHALLENGE', {'id': 'theories of planetary formation'}), 'e2': {'id': 'theories of planetary formation'}}, {'e1': {'id': 'pair'}, 'r': ({'id': 'pair'}, 'BE', {'id': 'lowest-mass star hosting'}), 'e2': {'id': 'lowest-mass star hosting'}}, {'e1': {'id': 'Five employees'}, 'r': ({'id': 'Five employees'}, 'PUBLISH', {'id': 'tropical cyclone report'}), 'e2': {'id': 'tropical cyclone report'}}, {'e1': {'id': 'August'}, 'r': ({'id': 'August'}, 'REACH', {'id': 'new record high temperature'}), 'e2': {'id': 'new record high temperature'}}, {'e1': {'id': \"world's oceans\"}, 'r': ({'id': \"world's oceans\"}, 'REACH', {'id': 'new record high temperature'}), 'e2': {'id': 'new record high temperature'}}, {'e1': {'id': 'Hurricane Ian'}, 'r': ({'id': 'Hurricane Ian'}, 'UPGRADES', {'id': 'Category 4 to Category 5'}), 'e2': {'id': 'Category 4 to Category 5'}}, {'e1': {'id': 'Hurricane Ian'}, 'r': ({'id': 'Hurricane Ian'}, 'CAUSED', {'id': '$112.9 billion worth of damage'}), 'e2': {'id': '$112.9 billion worth of damage'}}, {'e1': {'id': 'Hurricane Ian'}, 'r': ({'id': 'Hurricane Ian'}, 'STRIKES', {'id': 'Florida'}), 'e2': {'id': 'Florida'}}, {'e1': {'id': 'oxygen partial pressure'}, 'r': ({'id': 'oxygen partial pressure'}, 'IS_ABOVE', {'id': '18%'}), 'e2': {'id': '18%'}}, {'e1': {'id': 'emissions'}, 'r': ({'id': 'emissions'}, 'RISE', {'id': 'chlorofluorocarbons'}), 'e2': {'id': 'chlorofluorocarbons'}}]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:35:20 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-22 14:35:20 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mI know several pieces of information about different topics. Here are some of them:\n",
      "\n",
      "1. There are theories of planetary formation that have been challenged.\n",
      "2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\n",
      "3. Five employees have published a tropical cyclone report.\n",
      "4. In August, there was a new record high temperature reached.\n",
      "5. The world's oceans have also reached a new record high temperature.\n",
      "6. Hurricane Ian was upgraded from Category 4 to Category 5.\n",
      "7. Hurricane Ian caused $112.9 billion worth of damage.\n",
      "8. Hurricane Ian struck Florida.\n",
      "9. The oxygen partial pressure is above 18%.\n",
      "10. Emissions of chlorofluorocarbons have risen.\n",
      "\n",
      "Please let me know if you would like more information on any specific topic.\u001b[0m\u001b[32;1m\u001b[1;3mHere is some information that I know about different topics:\n",
      "\n",
      "1. There are theories of planetary formation that have been challenged.\n",
      "2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\n",
      "3. Five employees have published a tropical cyclone report.\n",
      "4. In August, there was a new record high temperature reached.\n",
      "5. The world's oceans have also reached a new record high temperature.\n",
      "6. Hurricane Ian was upgraded from Category 4 to Category 5.\n",
      "7. Hurricane Ian caused $112.9 billion worth of damage.\n",
      "8. Hurricane Ian struck Florida.\n",
      "9. The oxygen partial pressure is above 18%.\n",
      "10. Emissions of chlorofluorocarbons have risen.\n",
      "\n",
      "If you have any specific questions or need more information on any of these topics, please let me know.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Here is some information that I know about different topics:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nIf you have any specific questions or need more information on any of these topics, please let me know.\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS ANSWER SHOULD COME FROM THE GRAPH\n",
    "agent_executor.invoke({\"input\": \n",
    "                       querytemplate.format(question=\"tell me everything you know about\")}).get(\"output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a12c371e-03fc-4170-954e-544302fb0270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:35:28 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Copilot_Knowledge` with `Give me an RCA on ticket 1`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:35:29 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:35:29 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mI'm sorry, but I don't have any information on ticket 1.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:35:30 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `copilot_retriever` with `Give me an RCA on ticket 1`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:35:30 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-22 14:35:31 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-22 14:35:34 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mTicket 1 (INC0007654) is related to a sudden disruption on Production Line 2 during the morning shift. The disruption was caused by the unexpected shutdown of the Benchmill, which impacted the production schedule. The parameter values associated with this incident are as follows: Airflow Rate: 70 CFM, Ambient Temperature: 35 degrees Celsius, Motor surface temperature: 80 degrees Celsius, Motor rated power: 1.5 HP, Load on motor: 70%, User workload: 60%, Voltage stability: 5%, Power wiring resistance: 0.01 ohms, Voltage fluctuation: 1%. The maintenance logs show a monthly frequency, and there is a dust accumulation of 0.5 mm thickness.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 14:35:34 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mTicket 1 (INC0007654) is related to a sudden disruption on Production Line 2 during the morning shift. The disruption was caused by the unexpected shutdown of the Benchmill, which impacted the production schedule. The parameter values associated with this incident are as follows: Airflow Rate: 70 CFM, Ambient Temperature: 35 degrees Celsius, Motor surface temperature: 80 degrees Celsius, Motor rated power: 1.5 HP, Load on motor: 70%, User workload: 60%, Voltage stability: 5%, Power wiring resistance: 0.01 ohms, Voltage fluctuation: 1%. The maintenance logs show a monthly frequency, and there is a dust accumulation of 0.5 mm thickness.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ticket 1 (INC0007654) is related to a sudden disruption on Production Line 2 during the morning shift. The disruption was caused by the unexpected shutdown of the Benchmill, which impacted the production schedule. The parameter values associated with this incident are as follows: Airflow Rate: 70 CFM, Ambient Temperature: 35 degrees Celsius, Motor surface temperature: 80 degrees Celsius, Motor rated power: 1.5 HP, Load on motor: 70%, User workload: 60%, Voltage stability: 5%, Power wiring resistance: 0.01 ohms, Voltage fluctuation: 1%. The maintenance logs show a monthly frequency, and there is a dust accumulation of 0.5 mm thickness.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS ANSWER SHOULD COME FROM THE VECTOR\n",
    "agent_executor.invoke({\"input\": \n",
    "                       querytemplate.format(question=\"Give me an RCA on ticket 1\")}).get(\"output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e7f7e-e650-40e6-b5b2-b96dd008db09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a80bbe-3da8-4037-84ae-ad2cf44bc61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5994bd6f-b814-4eb6-8346-afdb77b59782",
   "metadata": {},
   "source": [
    "### LLama index graph test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e260804f-e0b7-4603-91fb-aacebee1f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "# @dataclass \n",
    "# class RAG2:\n",
    "#     memory_conversation  = None\n",
    "#     def __init__(self,llm,graph,type=\"langchain\",embeddings=None,usememory=False):\n",
    "#         self.llm = llm\n",
    "#         self.graph = graph\n",
    "#         self.type = type\n",
    "#         self.embeddings = embeddings\n",
    "#         self.usememory = usememory\n",
    "\n",
    "#     @staticmethod\n",
    "#     def memory(type='langchain',memory_key=\"chat_history\",output_key='result',rmessages=True,readmem=None):\n",
    "#         if readmem:\n",
    "#             if type.lower()==\"langchain\":\n",
    "#                 from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "#                 memory = ConversationBufferMemory(\n",
    "#                 memory_key=memory_key, return_messages=rmessages)\n",
    "#                 #output_key=output_key)\n",
    "                \n",
    "#                 readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "\n",
    "#                 return memory\n",
    "#         else: \n",
    "#             return None\n",
    "    \n",
    "\n",
    "#     def vector_retriever(self,store=\"chroma\",**kwargs):\n",
    "#         texts = kwargs.get(\"texts\")\n",
    "#         if store.lower()==\"chroma\":\n",
    "#             if not self.embeddings:\n",
    "#                 self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "\n",
    "#             from langchain.chains import RetrievalQA\n",
    "#             persist_directory = kwargs.get(\"persist_directory\",\"filesdb\")\n",
    "#             search_kwargs = kwargs.get(\"search_kwargs\",{\"k\": 3})\n",
    "#             from langchain.vectorstores import Chroma\n",
    "            \n",
    "#             db = Chroma.from_documents(texts,self.embeddings,persist_directory= persist_directory)\n",
    "#             vec_retriever = db.as_retriever(search_kwargs=search_kwargs)\n",
    "         \n",
    "#             return vec_retriever  \n",
    "\n",
    "            \n",
    "#         elif store.lower()==\"faiss\":\n",
    "#             #to be implemented\n",
    "#             from langchain_community.vectorstores import FAISS\n",
    "#             from langchain_core.output_parsers import StrOutputParser\n",
    "#             from langchain_core.prompts import ChatPromptTemplate\n",
    "#             from langchain_core.runnables import RunnablePassthrough\n",
    "#             from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "#             if not self.embeddings:\n",
    "#                 self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "#             if kwargs.get(\"text_embeddings\"):\n",
    "#                 text_embeddings =kwargs.get(\"text_embeddings\")\n",
    "#                 vectorstore = FAISS.from_embeddings(text_embeddings=list(zip(text_embeddings[0],text_embeddings[1])), embedding = self.embeddings)\n",
    "#                 retriever = vectorstore.as_retriever()\n",
    "#                 return retriever\n",
    "            \n",
    "#             else:\n",
    "            \n",
    "#                 vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())\n",
    "#                 retriever = vectorstore.as_retriever()\n",
    "#                 template = \"\"\"Answer the question based only on the following context:{context} \n",
    "#                 Question: {query}\"\"\"\n",
    "#                 prompt = ChatPromptTemplate.from_template(template)\n",
    "#                 retrieval_chain = ({\"context\": retriever, \"query\": RunnablePassthrough()} | prompt\n",
    "#                                     | self.llm| StrOutputParser())\n",
    "#                 return retrieval_chain\n",
    "        \n",
    "#     def build_chain(self,**kwargs):\n",
    "#         #memory = self.memory() if (readmem := self.memory()) else None\n",
    "#         method = kwargs.get(\"method\",\"graph\")\n",
    "        \n",
    "#         #PROMPT TEMPLATE        \n",
    "#         from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate,PromptTemplate\n",
    "\n",
    "        \n",
    "#         #####\n",
    "\n",
    "#         memory = self.usememory\n",
    "#         verbose = kwargs.get(\"verbose\",True)\n",
    "\n",
    "#         if memory:\n",
    "#             prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {question}\")\n",
    "#         else:\n",
    "#             prompt = PromptTemplate(input_variables=[\"question\"],template =\"{question}\")\n",
    "\n",
    "#         self.memory_conversation = self.memory(readmem=memory) if not self.memory_conversation else self.memory_conversation\n",
    "\n",
    "#         if kwargs.get(\"falkor\"):\n",
    "#             from langchain.chains import FalkorDBQAChain\n",
    "#             chain = FalkorDBQAChain.from_llm(llm=self.llm, graph=self.graph, verbose=verbose, memory=self.memory_conversation, **kwargs)\n",
    "#         else:\n",
    "#             if method.lower()==\"graph\":\n",
    "#                 #general chain for graphs\n",
    "#                 print(\"building graph chain\")\n",
    "#                 from langchain.chains import GraphCypherQAChain\n",
    "#                 #Prompt template\n",
    "                        \n",
    "#                 general_system_template = r\"\"\" \n",
    "#                 Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "#                 Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "#                 you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "#                 ----\n",
    "#                 {context}\n",
    "#                 ----\n",
    "#                 \"\"\"\n",
    "#                 general_user_template = \"Question:```{question}```\"\n",
    "#                 messages = [\n",
    "#                             SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "#                             HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "#                 ]\n",
    "#                 qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "\n",
    "#                 chain = GraphCypherQAChain.from_llm(llm=self.llm, prompt=prompt,graph=self.graph,validate_cypher = True, \n",
    "#                                                     verbose=verbose,memory=self.memory_conversation, promptTemplate=qa_prompt,**kwargs)\n",
    "#                 chain.input_key = \"question\"\n",
    "\n",
    "#             elif method.lower()==\"vector\":\n",
    "#                 print(\"Building vector chain \")\n",
    "#                 retriever = kwargs.get(\"vec_retriever\")\n",
    "#                 from langchain.chains import RetrievalQA\n",
    "#                 from langchain.chains import ConversationalRetrievalChain\n",
    "                \n",
    "#                 # template = (\"Combine the chat history and follow up question into \"\n",
    "#                 # \"a standalone question. Chat History: {chat_history}\"\n",
    "#                 # \"Follow up question: {query} \")\n",
    "#                 template = (\"Every time you respond you should start with This is Copilot your AI assistant\")\n",
    "#                 prompt = PromptTemplate.from_template(template)\n",
    "                \n",
    "\n",
    "#                 #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,return_source_documents=True,verbose=verbose)\n",
    "                        \n",
    "#                 general_system_template = r\"\"\" \n",
    "#                 Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "#                 Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "#                 you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "#                 ----\n",
    "#                 {context}\n",
    "#                 ----\n",
    "#                 \"\"\"\n",
    "#                 general_user_template = \"Question:```{question}```\"\n",
    "#                 messages = [\n",
    "#                             SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "#                             HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "#                 ]\n",
    "#                 qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "                \n",
    "#                 chain = ConversationalRetrievalChain.from_llm(self.llm,retriever,memory=self.memory_conversation,get_chat_history=lambda h :h,\n",
    "#                                                               verbose=verbose,\n",
    "#                                                               combine_docs_chain_kwargs={\"prompt\": qa_prompt}) \n",
    "                \n",
    "#                 #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,\n",
    "#                 #                                    return_source_documents=True,verbose=verbose,memory=self.memory_conversation,prompt=prompt)\n",
    "\n",
    "#         return chain\n",
    "\n",
    "\n",
    "#     def get_retriever(self,**kwargs):\n",
    "#         method = kwargs.get(\"method\",\"graph\")\n",
    "#         complexity = kwargs.get(\"complexity\",\"simple\")\n",
    "        \n",
    "#         if method.lower()=='graph':\n",
    "#             if self.type.lower()==\"langchain\":\n",
    "#                 from langchain.chains import GraphCypherQAChain\n",
    "#                 if complexity.lower()=='simple':\n",
    "#                     print(\"SIMPLE RAG LANGCHAIN\")\n",
    "#                     if usememory := kwargs.get(\"usememory\") or self.memory_conversation:\n",
    "#                         print(\"With Read memory\")\n",
    "                    \n",
    "#                     chain = self.build_chain(**kwargs)\n",
    "#                     return chain\n",
    "                \n",
    "                    \n",
    "#             elif self.type.lower()==\"llama\":\n",
    "#                 if complexity.lower()=='simple':\n",
    "#                     nl2graph = kwargs.get(\"nl2graph\",False)\n",
    "#                     print(nl2graph)\n",
    "#                     from llama_index.query_engine import RetrieverQueryEngine\n",
    "#                     from llama_index.retrievers import KnowledgeGraphRAGRetriever                    \n",
    "#                     graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "#                     storage_context=kwargs.get(\"storage_context\"),\n",
    "#                     service_context=kwargs.get(\"service_context\"),\n",
    "#                     llm=self.llm,\n",
    "#                     verbose=True,\n",
    "#                     with_nl2graphquery=nl2graph)\n",
    "#                     query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))\n",
    "#                     return query_engine\n",
    "\n",
    "                \n",
    "#                 elif complexity.lower()==\"custom\":\n",
    "#                     if self.type.lower()==\"llama\":\n",
    "#                         from llama_index import get_response_synthesizer\n",
    "#                         from llama_index.query_engine import RetrieverQueryEngine\n",
    "#                         # create custom retriever\n",
    "#                         vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "#                         kg_retriever = KGTableRetriever(index=kg_index, retriever_mode=\"keyword\", include_text=False)\n",
    "#                         custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "#                         # create response synthesizer\n",
    "#                         response_synthesizer = get_response_synthesizer(service_context=service_context,response_mode=\"tree_summarize\")\n",
    "#                         custom_query_engine = RetrieverQueryEngine(retriever=custom_retriever,response_synthesizer=response_synthesizer)\n",
    "#                         return custom_query_engine\n",
    "#                     elif self.type.lower()=='langchain':\n",
    "#                         from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "#                         simple = self.get_retriever(\"simple\",**kwargs)\n",
    "                        \n",
    "#                         tools = [\n",
    "#                             Tool(name= \"GraphRAG\",\n",
    "#                                  func= simple.run,\n",
    "#                                  description= \"Simple RAG from knowledge graph\"),\n",
    "#                             #add more RAG tools\n",
    "#                         ]\n",
    "#         elif method.lower()==\"vector\":\n",
    "#             print(\"Getting vector retriever\")\n",
    "#             vec_retriever = self.vector_retriever(**kwargs)\n",
    "\n",
    "#             chain = self.build_chain(vec_retriever=vec_retriever,**kwargs)\n",
    "#             return chain\n",
    "\n",
    "\n",
    "\n",
    "#     def response(self,query,complexity=\"simple\",**kwargs):\n",
    "#         method = kwargs.get(\"method\",\"vector\")\n",
    "#         verbose = kwargs.get(\"verbose\",True)\n",
    "#         retriever = self.get_retriever(**kwargs)\n",
    "#         try:\n",
    "#             if method.lower()==\"vector\":\n",
    "#                 print(\"I AM USING VECTOR LETS GO\")\n",
    "#                 result =  retriever({\"question\": query}).get('answer')\n",
    "                \n",
    "#             elif method.lower()==\"graph\":\n",
    "#                 if self.type.lower()=='langchain':\n",
    "#                     print('using retriever({\"query\": query}).get(\"answer\")')\n",
    "#                     #result = retriever({\"question\": query}).get('answer')#\n",
    "#                     result = retriever.invoke({\"question\": query}).get('result') \n",
    "#                     pickled_str = pickle.dumps(self.memory_conversation)\n",
    "#                 elif self.type.lower()=='llama':\n",
    "#                     result = retriever.query(query)\n",
    "#                     if 'Empty Response' in result.response:\n",
    "#                         raise(\"Error empty result or no information found\")\n",
    "#                     else:\n",
    "#                         return result.response\n",
    "            \n",
    "#             if not result or \"I'm sorry, but I don't have\" in result:\n",
    "#                 print('Not using RAG')\n",
    "#                 raise (\"Error empty result or no information found in the KG\")\n",
    "#             elif \"I'm sorry, but I don't have\" in result:\n",
    "#                 raise(\"ERROR 404\")\n",
    "#             else:\n",
    "#                 return result\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             if self.type.lower()=='langchain':\n",
    "#                 from langchain.schema import HumanMessage\n",
    "#                 #message = HumanMessage(content=query)\n",
    "#                 from langchain.chains import ConversationChain\n",
    "#                 from langchain.prompts.prompt import PromptTemplate\n",
    "#                 if self.usememory:\n",
    "#                     print(\"Normal LLM response with same memory\")\n",
    "#                     from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "#                     system_message = SystemMessage(content=\"Ypu are a helpful AI manufacturing assistant\")\n",
    "\n",
    "#                     prompt = ChatPromptTemplate(messages=[system_message])\n",
    "\n",
    "#                     #prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {query}\")\n",
    "#                     chat = ConversationChain(llm=self.llm,verbose=kwargs.get(\"verbose\",True),prompt=prompt,memory=self.memory_conversation ,input_key=\"question\")\n",
    "#                     return chat.invoke({\"question\":query}).get(\"response\")\n",
    "#                 else:\n",
    "#                     from langchain.schema import HumanMessage\n",
    "#                     message = HumanMessage(content=query)\n",
    "#                     return self.llm.invoke([message])\n",
    "\n",
    "#             else:\n",
    "#                 #return response with llama index self.llm(response)\n",
    "#                 pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65e16c-4c52-4307-abb0-565660ef9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getllm(openai=True, azure=False, mode=\"langchain\",**kwargs):\n",
    "#     print(\"Note: to use the LLM use it as msg = HumanMessage(content= your query) \\n llm(messages = [msg]) \")\n",
    "#     if openai:\n",
    "#         temperature = kwargs.get(\"temperature\",0)\n",
    "#         if azure:\n",
    "#             print(\"Using Azure OpenAI\")\n",
    "#             # from langchain.chat_models import AzureChatOpenAI\n",
    "#             from langchain_openai import AzureChatOpenAI\n",
    "#             auth = kwargs.get(\"auth\")\n",
    "#             if auth.lower()==\"token\":\n",
    "#                 if mode.lower()==\"langchain\":\n",
    "#                     params = {\"model\": kwargs.get(\"model\"),\n",
    "#                         \"azure_deployment\": kwargs.get(\"dep_name\"),\n",
    "#                         \"openai_api_version\": kwargs.get(\"api_version\"),\n",
    "#                         \"temperature\": temperature,\n",
    "#                         \"openai_api_key\": kwargs.get(\"api_key\"),\n",
    "#                         \"azure_endpoint\": kwargs.get(\"api_base\"),\n",
    "#                         \"max_tokens\":kwargs.get(\"max_tokens\",256)}\n",
    "#                     llm = AzureChatOpenAI(**params)\n",
    "#                 elif mode.lower()==\"llama\":\n",
    "#                     print(\"Llama's version\")\n",
    "#                     #from llama_index.llms import AzureOpenAI\n",
    "#                     from llama_index.llms.azure_openai import AzureOpenAI\n",
    "#                     import openai\n",
    "#                     openai.api_type = \"azure\"\n",
    "#                     # openai.api_base =  kwargs.get(\"api_base\") #\"INSERT AZURE API BASE\"\n",
    "#                     # openai.api_version = kwargs.get(\"api_version\",\"2023-05-15\")\n",
    "                    \n",
    "#                     llm = AzureOpenAI(engine= kwargs.get(\"dep_name\"),temperature=0,model=kwargs.get(\"model\"),\n",
    "#                                      azure_deployment =kwargs.get(\"dep_name\"),azure_endpoint = kwargs.get(\"api_base\"),\n",
    "#                                      api_version = kwargs.get(\"api_version\",\"2023-05-15\"),api_key= kwargs.get(\"api_key\"))\n",
    "#                 return llm\n",
    "#             elif auth.lower() == \"azure_ad\":\n",
    "#                 #from azure.identity import DefaultAzureCredential\n",
    "#                 #credential = DefaultAzureCredential()\n",
    "#                 #Set the API type to `azure_ad`\n",
    "#                 #os.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n",
    "#                ##Set the API_KEY to the token from the Azure credential\n",
    "#                 #os.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token            elif auth.lower()==\"azure_ad_provider\":\n",
    "#                 llm = \"Not implemented\"\n",
    "#             elif auth == \"azure ad_provider\":\n",
    "#                 #managed_identity:\n",
    "#                 #from azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential   \n",
    "#                 #credential = ChainedTokenCredential(\n",
    "#                 #ManagedIdentityCredential(),\n",
    "#                 #AzureCliCredential())\n",
    "#                 llm = \"not implemented\"\n",
    "#         else:\n",
    "#             from langchain_openai import OpenAI\n",
    "#             api_base = kwargs.get(\"api_base_url\")\n",
    "#             api_key =kwargs.get(\"api_key\",\"NULL\")\n",
    "#             llm = OpenAI(\n",
    "#                          base_url = api_base,\n",
    "#                          openai_api_key = api_key,**kwargs)\n",
    "#             return llm \n",
    "#     else: #huggingface model\n",
    "#         pass    \n",
    "\n",
    "\n",
    "\n",
    "# def llm_cost(llm,message) -> \"Total cost in USD\":\n",
    "#     from langchain.callbacks import get_openai_callback\n",
    "#     with get_openai_callback() as cb:\n",
    "#         llm([message])\n",
    "#         cost=format(cb.total_cost, '.6f')\n",
    "#         print(f\"Total Cost (USD): ${cost}\")\n",
    "#     return cost\n",
    "\n",
    "\n",
    "\n",
    "#     def get_embeddings(self,documents=None,lib='langchain'):\n",
    "# #         if not documents:\n",
    "# #             documents = pass\n",
    "#         #huggingface or langchain\n",
    "#         if lib=='langchain':\n",
    "#             from langchain.embeddings import OpenAIEmbeddings\n",
    "#             embeddings_model = OpenAIEmbeddings()\n",
    "#             embeddings = embeddings_model.embed_documents(documents)\n",
    "#             return emebeddings\n",
    "#         else:\n",
    "#             from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#             embeddings = HuggingFaceEmbeddings(model_name=self.huggingembed)\n",
    "#             return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a157a285-ee20-4dda-98cc-21029df6c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_graphdb(args,**kwargs):\n",
    "#     graphdb = kwargs.get(\"graphdb\")\n",
    "#     url,username,password,database = kwargs.get(\"url\"), kwargs.get(\"username\"), kwargs.get(\"password\"), kwargs.get(\"database\")\n",
    "#     if args.lower()=='llama':\n",
    "#         #from llama_index.storage.storage_context import StorageContext\n",
    "#         from llama_index.core import StorageContext\n",
    "#         print('Using LLama Index graph class')\n",
    "#         if graphdb.lower()=='neo4j' or 'neo4j' in url:\n",
    "#             from llama_index.graph_stores.neo4j import Neo4jGraphStore\n",
    "\n",
    "#             graph_store = Neo4jGraphStore(\n",
    "#                 username=username,\n",
    "#                 password=password,\n",
    "#                 url=url,\n",
    "#                 database=database,\n",
    "#             )\n",
    "#             #build storage context\n",
    "#             storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "#             return graph_store, storage_context\n",
    "#         elif graphdb.lower()=='nebula':\n",
    "#             from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "#             #TBD get varibales from kwargs\n",
    "#             graph_store = NebulaGraphStore(\n",
    "#             space_name=kwargs.get(\"space_name\"),\n",
    "#             edge_types=kwargs.get(\"edge_types\"),\n",
    "#             rel_prop_names=kwargs.get(\"rel_prop_names\"),\n",
    "#             tags=kwargs.get(\"tags\"))\n",
    "#             storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "#             return graph_store, storage_context\n",
    "        \n",
    "#         else:\n",
    "#             raise(\"Error not implemented for that graph database\")\n",
    "    \n",
    "\n",
    "#     elif args.lower()=='langchain':\n",
    "#         print('Using langchain graph class')\n",
    "#         if graphdb.lower()=='neo4j' or 'neo4j' in url:\n",
    "#             from langchain.graphs import Neo4jGraph\n",
    "#             graph= Neo4jGraph(\n",
    "#                url=url,\n",
    "#                username=username,\n",
    "#                password=password\n",
    "#             )\n",
    "    \n",
    "#             return graph\n",
    "#         elif graphdb.lower()==\"falkordb\":\n",
    "#             print(\"Using falkordb\")\n",
    "#             from langchain.graphs import FalkorDBGraph\n",
    "#             graph = FalkorDBGraph(database = database,host=url,port=6379, username=username,password = pwd)\n",
    "\n",
    "\n",
    "#         else:\n",
    "#             raise(\"Error not implemented for that graph database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749eb20-c10e-4c53-ab6b-e8add1cbb6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "14ddb8fd-c907-4ea3-987a-eb7b1f293439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using langchain graph class\n",
      "Using LLama Index graph class\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Neo4jGraphStore' from 'llama_index.graph_stores' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m graph_store \u001b[38;5;241m=\u001b[39m get_graphdb(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain\u001b[39m\u001b[38;5;124m\"\u001b[39m,graphdb\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneo4j\u001b[39m\u001b[38;5;124m\"\u001b[39m,url\u001b[38;5;241m=\u001b[39murl,username\u001b[38;5;241m=\u001b[39musername,password\u001b[38;5;241m=\u001b[39mpassword)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#GRAPH STORE LLAMA \u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m graph_store_llama, storage_context \u001b[38;5;241m=\u001b[39m get_graphdb(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama\u001b[39m\u001b[38;5;124m\"\u001b[39m,graphdb\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneo4j\u001b[39m\u001b[38;5;124m\"\u001b[39m,url\u001b[38;5;241m=\u001b[39murl,username\u001b[38;5;241m=\u001b[39musername,password\u001b[38;5;241m=\u001b[39mpassword)\n",
      "File \u001b[1;32m~\\OneDrive\\Documentos\\Respaldo\\PredatorCODECH\\DataScience\\AI\\Corporate\\Manufacturing-Copilot\\Manu_Copilot\\utilitiesPOC.py:917\u001b[0m, in \u001b[0;36mget_graphdb\u001b[1;34m(args, **kwargs)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsing LLama Index graph class\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graphdb\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneo4j\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneo4j\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m url:\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_stores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Neo4jGraphStore\n\u001b[0;32m    919\u001b[0m     graph_store \u001b[38;5;241m=\u001b[39m Neo4jGraphStore(\n\u001b[0;32m    920\u001b[0m         username\u001b[38;5;241m=\u001b[39musername,\n\u001b[0;32m    921\u001b[0m         password\u001b[38;5;241m=\u001b[39mpassword,\n\u001b[0;32m    922\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    923\u001b[0m         database\u001b[38;5;241m=\u001b[39mdatabase,\n\u001b[0;32m    924\u001b[0m     )\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;66;03m#build storage context\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Neo4jGraphStore' from 'llama_index.graph_stores' (unknown location)"
     ]
    }
   ],
   "source": [
    "#INITIATE THE GRAPH STORE\n",
    "url = \"neo4j+s://b75e0bd8.databases.neo4j.io\"\n",
    "username= \"neo4j\"\n",
    "password = \"aziMmumvuVru-rgeKuO0zT7mEtTxu29Jgrmp6lFKv0w\"\n",
    "graph_store = get_graphdb(\"langchain\",graphdb=\"neo4j\",url=url,username=username,password=password)\n",
    "#GRAPH STORE LLAMA \n",
    "graph_store_llama, storage_context = get_graphdb(\"llama\",graphdb=\"neo4j\",url=url,username=username,password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eee31228-85f8-4707-86b2-94bde63dba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import KnowledgeGraphQueryEngine\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import KnowledgeGraphRAGRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d267e1-325b-443f-a9d6-52ae97922a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag2 = RAG2(llmazure2,graph_store_llama,type=\"llama\")\n",
    "service_context= \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9630d1e-31c2-4967-abed-1c5fc7761072",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'storage_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m graph_rag_retriever \u001b[38;5;241m=\u001b[39m KnowledgeGraphRAGRetriever(        \n\u001b[1;32m----> 2\u001b[0m                     storage_context\u001b[38;5;241m=\u001b[39mstorage_context,\n\u001b[0;32m      3\u001b[0m                     llm\u001b[38;5;241m=\u001b[39mllmazure2,\n\u001b[0;32m      4\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      5\u001b[0m                     with_nl2graphquery\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'storage_context' is not defined"
     ]
    }
   ],
   "source": [
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=storage_context,\n",
    "                    llm=llmazure2,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=True)\n",
    "\n",
    "#query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e46cce-9a6f-4951-ad10-8d894982d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=storage_context,\n",
    "                    llm=llmazure2,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e62cda-4fec-4eba-b306-ebe74ad79771",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmazure2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3d41d-7a75-409a-9917-80451f5bcbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    graph_rag_retriever,\n",
    "    llm = llmazure2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a93430-eefd-47e5-8a82-acaa12288645",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.query(\n",
    "    \"what did the study concluded\",\n",
    ").response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d69df8-f956-4e76-9bd9-0873061b10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag_retriever_with_nl2graphquery = KnowledgeGraphRAGRetriever(\n",
    "    storage_context=storage_context,\n",
    "    verbose=True,\n",
    "    with_nl2graphquery=True,\n",
    "    llm = llmazure2\n",
    ")\n",
    "\n",
    "query_engine_with_nl2graphquery = RetrieverQueryEngine.from_args(\n",
    "    graph_rag_retriever_with_nl2graphquery,\n",
    "    llm = llmazure2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e977b64-12bd-43d3-8982-9540894bc499",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_with_nl2graphquery.query(\n",
    "    \"what did the study concluded\",\n",
    ").response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932060df-425a-46ce-abb3-32e7cf0e2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add it to langchain tools\n",
    "from llama_index.core.langchain_helpers.agents import (\n",
    "    IndexToolConfig,\n",
    "    LlamaIndexTool,\n",
    ")\n",
    "\n",
    "tool_config = IndexToolConfig(\n",
    "    query_engine=query_engine_with_nl2graphquery,\n",
    "    name=f\"KG Index\",\n",
    "    description=f\"useful for when you want to answer queries about neo4j\",\n",
    "    tool_kwargs={\"return_direct\": False},\n",
    ")\n",
    "\n",
    "tool = LlamaIndexTool.from_tool_config(tool_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32b41b-e05d-410f-9c5b-11e686a6c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool.run(\"tell me all you know about a study\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa833d45-a18a-4ef1-9570-6c8e384bc66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa5b50-9096-4603-8efa-837e7b08f496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744d16e-332e-489a-97d2-440681c883e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6b1a6-966c-41f9-83b1-4230307c9d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b1105-67e5-4799-8f73-b73da416dbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d298b499-ac0c-4fc7-afac-e540346eef65",
   "metadata": {},
   "source": [
    "## Other test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff353c-dfd4-4f1f-8fd3-4b134442c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, create_structured_chat_agent\n",
    "\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c780fb-f2f8-4009-842a-0c2e50536173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llmazure.bind_tools(tool)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LlamaIndex\",\n",
    "        func=lambda q: query_engine_with_nl2graphquery.query(q),\n",
    "        description=\"useful for when you want to answer questions about neo4j kg.\",\n",
    "        return_direct=False,\n",
    "    ),\n",
    "]\n",
    "\n",
    "llm_with_tools= llmazure.bind_tools(tools)\n",
    "\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7378a32b-8678-40e3-a233-989f0a604ed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tools[0].run(\"what did the study concluded?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d37981-5ee0-4b42-baae-b28495943933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits.conversational_retrieval.openai_functions import create_conversational_retrieval_agent\n",
    "\n",
    "agent_executor = create_conversational_retrieval_agent(llm=llmazure, tools=[\n",
    "    tools[0]], verbose=True)\n",
    "agent_executor({\"input\": \"what did the study concluded about the lifespan?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcdd7ae-e994-4868-8769-8675411bf489",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke(\"what did the study concluded?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b0dab-5b23-401e-8699-aeb25e2f2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful manufacturing AI Assistant, you specialize in Quality department but have expertise in general manufacturing\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "    \n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97d154-e2d9-4e90-b991-840cdc6d2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5dd06-4151-41a3-ab4c-67accf75cec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
