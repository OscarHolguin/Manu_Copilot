{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c3d1ee1-3372-4984-979b-b76705b5928b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (0.10.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting llama_index\n",
      "  Downloading llama_index-0.10.6-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: llama-index-llms-azure-openai in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: llama-index-graph-stores-neo4j in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: llama-index-embeddings-azure-openai in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.10.5)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.2)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.3)\n",
      "Requirement already satisfied: azure-identity<2.0.0,>=1.15.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-llms-azure-openai) (1.15.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-llms-azure-openai) (0.24.1)\n",
      "Requirement already satisfied: neo4j<6.0.0,>=5.16.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-graph-stores-neo4j) (5.16.0)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.23.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.30.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (42.0.2)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.24.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.26.0)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.1.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (0.5.14)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (2023.12.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.5.8)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.26.3)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.11.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (2.1.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (0.5.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (4.12.3)\n",
      "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (0.0.2)\n",
      "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (1.23.22)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (4.0.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from neo4j<6.0.0,>=5.16.0->llama-index-graph-stores-neo4j) (2023.3.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (0.17.3)\n",
      "Requirement already satisfied: idna in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.3.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-core<2.0.0,>=1.23.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (2.5)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.16.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx->llama-index-llms-azure-openai) (0.14.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx->llama-index-llms-azure-openai) (3.5.0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2.0.0,>=1.24.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.8.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (23.2)\n",
      "Requirement already satisfied: portalocker<3,>=1.6 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.8.2)\n",
      "Requirement already satisfied: click in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.5.3)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.22 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (1.23.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama_index) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.0->llama_index) (3.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama_index) (2023.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.21)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from portalocker<3,>=1.6->msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (305.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.14.6)\n",
      "Downloading llama_index-0.10.6-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: llama_index\n",
      "  Attempting uninstall: llama_index\n",
      "    Found existing installation: llama-index 0.10.5\n",
      "    Uninstalling llama-index-0.10.5:\n",
      "      Successfully uninstalled llama-index-0.10.5\n",
      "Successfully installed llama_index-0.10.6\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade llama_index llama-index-llms-azure-openai llama-index-graph-stores-neo4j llama-index-embeddings-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16c8ade-2780-4e66-ba5d-288c0c652ec1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pwd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Databricks notebook source\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# MAGIC %run ./utilitiesPOC\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# COMMAND ----------\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutilitiesPOC\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mauths\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m openaisecret, embeddings_secret, adls_secret\n\u001b[0;32m     10\u001b[0m storage_account \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_account_key\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_container\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanufacturing-copilot\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_account\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuwdsrg03rsta07dls01\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive - EY\\Documents\\Engagements\\LLMs\\Manufacturing Copilot\\POC\\ChatbotPOC\\Chatbot\\utilitiesPOC.py:683\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;66;03m# COMMAND ----------\u001b[39;00m\n\u001b[0;32m    680\u001b[0m \n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# DBTITLE 1,MANUFACTURING COPILOT\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m--> 683\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader, UnstructuredPDFLoader, OnlinePDFLoader\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter, TextSplitter\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\document_loaders\\__init__.py:32\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m document_loaders\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# If not in interactive env, raise warning.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interactive_env():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\document_loaders\\__init__.py:163\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morg_mode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnstructuredOrgModeLoader\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    151\u001b[0m     AmazonTextractPDFLoader,\n\u001b[0;32m    152\u001b[0m     MathpixPDFLoader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     UnstructuredPDFLoader,\n\u001b[0;32m    162\u001b[0m )\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpebblo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PebbloSafeLoader\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolars_dataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolarsDataFrameLoader\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpowerpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnstructuredPowerPointLoader\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_community\\document_loaders\\pebblo.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpwd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPStatus\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pwd'"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %run ./utilitiesPOC\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from utilitiesPOC import *\n",
    "\n",
    "from auths import openaisecret, embeddings_secret, adls_secret\n",
    "\n",
    "storage_account =  '{\"storage_account_key\": \"\",\"storage_container\": \"manufacturing-copilot\",\"storage_account\": \"euwdsrg03rsta07dls01\"}'\n",
    "path = \"Documents/Embeddings/\"\n",
    "\n",
    "\n",
    "\n",
    "llmconfig= '{\"mode\": \"langchain\", \"auth\": \"token\", \"model\": \"gpt-35-turbo-16k\", \"deployment_name\": \"mxdrca\",\"api_version\": \"2023-05-15\",\"api_key_secret\":\"\",\"api_endpoint\": \"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "embedconfig= '{\"embedmode\": \"langchain\",\"model\":\"text-embedding-ada-002\",\"deployment_name\":\"azure_embedding\",\"api_key_secret\":\"\",\"embeddingurl\":\"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "\n",
    "storage_account = json.loads(storage_account)\n",
    "storage_account_key, container, storage_account = storage_account.get('storage_account_key'),storage_account.get('storage_container'),storage_account.get('storage_account')\n",
    "\n",
    "#read from file\n",
    "storage_account_key = adls_secret\n",
    "\n",
    "llmconfig = json.loads(llmconfig)\n",
    "embedconfig = json.loads(embedconfig)\n",
    "\n",
    "embedmode, embedmodel,embed_deployment,embed_key, embed_url = embedconfig.values()\n",
    "mode, auth, model, deployment_name, api_version, api_key_secret, api_endpoint = llmconfig.values()\n",
    "\n",
    "api_key_secret = openaisecret\n",
    "embed_key = embeddings_secret\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#GETTING THE SECRETS\n",
    "api_key = api_key_secret\n",
    "\n",
    "\n",
    "embed_key = embed_key\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Define LLM \n",
    "#DEFINE LLM USING AZURE OPEN AI\n",
    "llmrca = getllm(openai=True,azure =True,mode = mode, auth=auth,model=model,dep_name = deployment_name,api_version= api_version,\n",
    "                     api_key =api_key,api_base = api_endpoint)\n",
    "\n",
    "\n",
    "embeddings_model  = getembeddings(model=embedmodel,dep_name=embed_deployment,api_key=embed_key,api_base=embed_url)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "mount_point = '/mnt/copilotembedds'\n",
    "\n",
    "\n",
    "#@async_retry\n",
    "def read_embeddings(fromadls=False,dbricks=False,**kwargs):\n",
    "    if fromadls:\n",
    "        if dbricks:\n",
    "            #reads it into a dataframe first from the mount point\n",
    "            storage_account=storage_account,container=container,mount_point=mount_point,path=path,storage_account_key=storage_account_key\n",
    "            for mount in dbutils.fs.mounts():\n",
    "                if mount.mountPoint.startswith(mount_point):\n",
    "                    print(\"Unmounting mnt point\")\n",
    "                    dbutils.fs.unmount(mount.mountPoint)\n",
    "            \n",
    "            #mount to path\n",
    "            dbutils.fs.refreshMounts() #refresh mounts to avoid errors\n",
    "\n",
    "            mntfiles = mount_to_local(storage_account, container, mount_point,path, storage_account_key)\n",
    "            #reads into dataframe\n",
    "            adlsembedspath = [e.path for e in dbutils.fs.ls(mntfiles)][-1] #read last file \n",
    "            adlsembedspath= adlsembedspath.replace(\"dbfs:\",\"/dbfs\")\n",
    "            df = pd.read_pickle(adlsembedspath)\n",
    "\n",
    "        else:\n",
    "            #read from adls but not from databricks\n",
    "            from azure.storage.filedatalake import DataLakeServiceClient\n",
    "            storage_account,container,path,storage_account_key=kwargs.get(\"storage_account\"),kwargs.get(\"container\"),kwargs.get(\"path\"),kwargs.get(\"storage_account_key\")\n",
    "            connstr = f\"DefaultEndpointsProtocol=https;AccountName={storage_account};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "            service = DataLakeServiceClient.from_connection_string(conn_str=connstr)\n",
    "            file_system_client = service.get_file_system_client(container)\n",
    "            paths = file_system_client.get_paths(path=path)\n",
    "            files = [p for p in paths if not p.is_directory]\n",
    "            files.sort(key=lambda x: x.last_modified)\n",
    "            last_file = files[-1].name\n",
    "            file_client = file_system_client.get_file_client(last_file)\n",
    "            \n",
    "            # Download the file content and load it as a Python object\n",
    "            download = file_client.download_file()\n",
    "            downloaded_bytes = download.readall()\n",
    "            df = pickle.loads(downloaded_bytes)\n",
    "            #df = pd.read_pickle(data)\n",
    "            \n",
    "    else:\n",
    "        #read from local FS\n",
    "        adlsembedspath = [n for n in os.listdir(path)][-1]\n",
    "        \n",
    "        df = pd.read_pickle(adlsembedspath)\n",
    "    #returns the texts and embeddings\n",
    "    txtlist = [t for t in df.texts]\n",
    "    embeddings = [e for e in df.embeddings]\n",
    "    return txtlist,embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_embeddings_fromfs(path):\n",
    "    #service = DataLakeServiceClient.from_connection_string(conn_str=\"my_connection_string\")\n",
    "\n",
    "    adlsembedspath = [n for n in os.listdir(path)][-1]        \n",
    "    df = pd.read_pickle(path+\"/\"+adlsembedspath)\n",
    "    #returns the texts and embeddings\n",
    "    txtlist = [t for t in df.texts]\n",
    "    embeddings = [e for e in df.embeddings]\n",
    "    return txtlist,embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "loaded_embeddings = read_embeddings(fromadls=True,dbricks=False,storage_account=storage_account,container=container,path=path,storage_account_key=storage_account_key)#read_embeddings_fromfs(path)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import ClassVar\n",
    "import pickle\n",
    "\n",
    "@dataclass \n",
    "class RAG2:\n",
    "    memory_conversation  = None\n",
    "    def __init__(self,llm,graph,type=\"langchain\",embeddings=None,usememory=False):\n",
    "        self.llm = llm\n",
    "        self.graph = graph\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "        self.usememory = usememory\n",
    "\n",
    "    @staticmethod\n",
    "    def memory(type='langchain',memory_key=\"chat_history\",output_key='result',rmessages=True,readmem=None):\n",
    "        if readmem:\n",
    "            if type.lower()==\"langchain\":\n",
    "                from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "                memory = ConversationBufferMemory(\n",
    "                memory_key=memory_key, return_messages=rmessages)\n",
    "                #output_key=output_key)\n",
    "                \n",
    "                readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "\n",
    "                return memory\n",
    "        else: \n",
    "            return None\n",
    "    \n",
    "\n",
    "    def vector_retriever(self,store=\"chroma\",**kwargs):\n",
    "        texts = kwargs.get(\"texts\")\n",
    "        if store.lower()==\"chroma\":\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "\n",
    "            from langchain.chains import RetrievalQA\n",
    "            persist_directory = kwargs.get(\"persist_directory\",\"filesdb\")\n",
    "            search_kwargs = kwargs.get(\"search_kwargs\",{\"k\": 3})\n",
    "            from langchain.vectorstores import Chroma\n",
    "            \n",
    "            db = Chroma.from_documents(texts,self.embeddings,persist_directory= persist_directory)\n",
    "            vec_retriever = db.as_retriever(search_kwargs=search_kwargs)\n",
    "         \n",
    "            return vec_retriever  \n",
    "\n",
    "            \n",
    "        elif store.lower()==\"faiss\":\n",
    "            #to be implemented\n",
    "            from langchain_community.vectorstores import FAISS\n",
    "            from langchain_core.output_parsers import StrOutputParser\n",
    "            from langchain_core.prompts import ChatPromptTemplate\n",
    "            from langchain_core.runnables import RunnablePassthrough\n",
    "            from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "            if kwargs.get(\"text_embeddings\"):\n",
    "                text_embeddings =kwargs.get(\"text_embeddings\")\n",
    "                vectorstore = FAISS.from_embeddings(text_embeddings=list(zip(text_embeddings[0],text_embeddings[1])), embedding = self.embeddings)\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                return retriever\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                template = \"\"\"Answer the question based only on the following context:{context} \n",
    "                Question: {query}\"\"\"\n",
    "                prompt = ChatPromptTemplate.from_template(template)\n",
    "                retrieval_chain = ({\"context\": retriever, \"query\": RunnablePassthrough()} | prompt\n",
    "                                    | self.llm| StrOutputParser())\n",
    "                return retrieval_chain\n",
    "        \n",
    "    def build_chain(self,**kwargs):\n",
    "        #memory = self.memory() if (readmem := self.memory()) else None\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        \n",
    "        #PROMPT TEMPLATE        \n",
    "        from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate,PromptTemplate\n",
    "        #####\n",
    "\n",
    "        memory = self.usememory\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "\n",
    "        if memory:\n",
    "            prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {question}\")\n",
    "        else:\n",
    "            prompt = PromptTemplate(input_variables=[\"question\"],template =\"{question}\")\n",
    "\n",
    "        self.memory_conversation = self.memory(readmem=memory) if not self.memory_conversation else self.memory_conversation\n",
    "\n",
    "        if kwargs.get(\"falkor\"):\n",
    "            from langchain.chains import FalkorDBQAChain\n",
    "            chain = FalkorDBQAChain.from_llm(llm=self.llm, graph=self.graph, verbose=verbose, memory=self.memory_conversation, **kwargs)\n",
    "        else:\n",
    "            if method.lower()==\"graph\":\n",
    "                #general chain for graphs\n",
    "                print(\"building graph chain\")\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                #Prompt template\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "\n",
    "                chain = GraphCypherQAChain.from_llm(llm=self.llm, prompt=prompt,graph=self.graph,validate_cypher = True, \n",
    "                                                    verbose=verbose,memory=self.memory_conversation, promptTemplate=qa_prompt,**kwargs)\n",
    "                chain.input_key = \"question\"\n",
    "\n",
    "            elif method.lower()==\"vector\":\n",
    "                print(\"Building vector chain \")\n",
    "                retriever = kwargs.get(\"vec_retriever\")\n",
    "                from langchain.chains import RetrievalQA\n",
    "                from langchain.chains import ConversationalRetrievalChain\n",
    "                \n",
    "                # template = (\"Combine the chat history and follow up question into \"\n",
    "                # \"a standalone question. Chat History: {chat_history}\"\n",
    "                # \"Follow up question: {query} \")\n",
    "                template = (\"Every time you respond you should start with This is Copilot your AI assistant\")\n",
    "                prompt = PromptTemplate.from_template(template)\n",
    "                \n",
    "\n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,return_source_documents=True,verbose=verbose)\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "                \n",
    "                chain = ConversationalRetrievalChain.from_llm(self.llm,retriever,memory=self.memory_conversation,get_chat_history=lambda h :h,\n",
    "                                                              verbose=verbose,\n",
    "                                                              combine_docs_chain_kwargs={\"prompt\": qa_prompt}) \n",
    "                \n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,\n",
    "                #                                    return_source_documents=True,verbose=verbose,memory=self.memory_conversation,prompt=prompt)\n",
    "\n",
    "        return chain\n",
    "\n",
    "\n",
    "    def get_retriever(self,**kwargs):\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        complexity = kwargs.get(\"complexity\",\"simple\")\n",
    "        \n",
    "        if method.lower()=='graph':\n",
    "            if self.type.lower()==\"langchain\":\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                if complexity.lower()=='simple':\n",
    "                    print(\"SIMPLE RAG LANGCHAIN\")\n",
    "                    if usememory := kwargs.get(\"usememory\") or self.memory_conversation:\n",
    "                        print(\"With Read memory\")\n",
    "                    \n",
    "                    chain = self.build_chain(**kwargs)\n",
    "                    return chain\n",
    "                \n",
    "                    \n",
    "            elif self.type.lower()==\"llama\":\n",
    "                if complexity.lower()=='simple':\n",
    "                    nl2graph = kwargs.get(\"nl2graph\",False)\n",
    "                    print(nl2graph)\n",
    "                    from llama_index.query_engine import RetrieverQueryEngine\n",
    "                    from llama_index.retrievers import KnowledgeGraphRAGRetriever                    \n",
    "                    graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=kwargs.get(\"storage_context\"),\n",
    "                    service_context=kwargs.get(\"service_context\"),\n",
    "                    llm=self.llm,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=nl2graph)\n",
    "                    query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))\n",
    "                    return query_engine\n",
    "\n",
    "                \n",
    "                elif complexity.lower()==\"custom\":\n",
    "                    if self.type.lower()==\"llama\":\n",
    "                        from llama_index import get_response_synthesizer\n",
    "                        from llama_index.query_engine import RetrieverQueryEngine\n",
    "                        # create custom retriever\n",
    "                        vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "                        kg_retriever = KGTableRetriever(index=kg_index, retriever_mode=\"keyword\", include_text=False)\n",
    "                        custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "                        # create response synthesizer\n",
    "                        response_synthesizer = get_response_synthesizer(service_context=service_context,response_mode=\"tree_summarize\")\n",
    "                        custom_query_engine = RetrieverQueryEngine(retriever=custom_retriever,response_synthesizer=response_synthesizer)\n",
    "                        return custom_query_engine\n",
    "                    elif self.type.lower()=='langchain':\n",
    "                        from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "                        simple = self.get_retriever(\"simple\",**kwargs)\n",
    "                        \n",
    "                        tools = [\n",
    "                            Tool(name= \"GraphRAG\",\n",
    "                                 func= simple.run,\n",
    "                                 description= \"Simple RAG from knowledge graph\"),\n",
    "                            #add more RAG tools\n",
    "                        ]\n",
    "        elif method.lower()==\"vector\":\n",
    "            print(\"Getting vector retriever\")\n",
    "            vec_retriever = self.vector_retriever(**kwargs)\n",
    "\n",
    "            chain = self.build_chain(vec_retriever=vec_retriever,**kwargs)\n",
    "            return chain\n",
    "\n",
    "\n",
    "\n",
    "    def response(self,query,complexity=\"simple\",**kwargs):\n",
    "        method = kwargs.get(\"method\",\"vector\")\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "        retriever = self.get_retriever(**kwargs)\n",
    "        try:\n",
    "            if method.lower()==\"vector\":\n",
    "                print(\"I AM USING VECTOR LETS GO\")\n",
    "                result =  retriever({\"question\": query}).get('answer')\n",
    "                \n",
    "            elif method.lower()==\"graph\":\n",
    "                if self.type.lower()=='langchain':\n",
    "                    print('using retriever({\"query\": query}).get(\"answer\")')\n",
    "                    #result = retriever({\"question\": query}).get('answer')#\n",
    "                    result = retriever.invoke({\"question\": query}).get('result') \n",
    "                    pickled_str = pickle.dumps(self.memory_conversation)\n",
    "                elif self.type.lower()=='llama':\n",
    "                    result = retriever.query(query)\n",
    "                    if 'Empty Response' in result.response:\n",
    "                        raise(\"Error empty result or no information found\")\n",
    "                    else:\n",
    "                        return result.response\n",
    "            \n",
    "            if not result or \"I'm sorry, but I don't have\" in result:\n",
    "                print('Not using RAG')\n",
    "                raise (\"Error empty result or no information found in the KG\")\n",
    "            elif \"I'm sorry, but I don't have\" in result:\n",
    "                raise(\"ERROR 404\")\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if self.type.lower()=='langchain':\n",
    "                from langchain.schema import HumanMessage\n",
    "                #message = HumanMessage(content=query)\n",
    "                from langchain.chains import ConversationChain\n",
    "                from langchain.prompts.prompt import PromptTemplate\n",
    "                if self.usememory:\n",
    "                    print(\"Normal LLM response with same memory\")\n",
    "                    prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {query}\")\n",
    "                    chat = ConversationChain(llm=self.llm,verbose=kwargs.get(\"verbose\",True),prompt=prompt,memory=self.memory_conversation ,input_key=\"question\")\n",
    "                    return chat.invoke({\"question\":query}).get(\"response\")\n",
    "                else:\n",
    "                    from langchain.schema import HumanMessage\n",
    "                    message = HumanMessage(content=query)\n",
    "                    return self.llm.invoke([message])\n",
    "\n",
    "            else:\n",
    "                #return response with llama index self.llm(response)\n",
    "                pass\n",
    "            \n",
    "\n",
    "# COMMAND ----------\n",
    "#start graph\n",
    "url =\"neo4j+s://b75e0bd8.databases.neo4j.io\"\n",
    "username =\"neo4j\"\n",
    "password = \"aziMmumvuVru-rgeKuO0zT7mEtTxu29Jgrmp6lFKv0w\"\n",
    "graph_store = get_graphdb(\"langchain\",graphdb = \"neo4j\",url=url,username=username,password=password)\n",
    "\n",
    "\n",
    "#call RAG class initialize vector class \n",
    "rag2 = RAG2\n",
    "vecrag = rag2(llmrca,graph_store,type=\"langchain\",embeddings = embeddings_model,usememory=True)\n",
    "#vectorchain = vecrag.get_retriever(method=\"vector\",text_embeddings = loaded_embeddings,store=\"faiss\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "\n",
    "    # print(vectorchain({\"question\": \"where and why did the production stoppage occurred\"}).get(\"answer\")) #if use memory we call it like this \n",
    "    # print(vectorchain({\"question\": \"what was my previous question?\"}).get(\"answer\"))\n",
    "    # #\n",
    "    print(vecrag.response(query=\"where and why did the production stoppage occurred\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "    print(vecrag.response(query=\"what was my previous question?\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "# COMMAND ----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032827ad-4f2f-4c03-aa24-97404d1cda00",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag2 = RAG2\n",
    "vecrag = rag2(llmrca,graph_store,type=\"langchain\",embeddings = embeddings_model,usememory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f95529-57cb-4674-b450-c501a713a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecrag.response(query=\"where and why did the production stoppage occurred\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "print(vecrag.response(query=\"what was my previous question?\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994bd6f-b814-4eb6-8346-afdb77b59782",
   "metadata": {},
   "source": [
    "### LLama index graph test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65e16c-4c52-4307-abb0-565660ef9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecrag.graph.get_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0c388-b1a0-4fb1-ae85-ce9bbce98394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading llama llm\n",
    "#LLAMA'S VERSION\n",
    "llmazure2 =getllm(openai=True,azure =True,mode = \"llama\", auth=\"token\",model=\"gpt-35-turbo-16k\",dep_name = \"chat-bot-trail\",            \n",
    "                  api_version= \"2023-05-15\", api_key = \"1bbc7a5ceb0e4677888f22d1eb4c8617\",api_base = \"https://chat-bot-iot.openai.azure.com/\")#([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27bd8636-4dfb-439c-b575-fa59a681008c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_index.core.indices.document_summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AzureOpenAI\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\llm\\Lib\\site-packages\\llama_index\\llms\\azure_openai\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mazure_openai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     AzureOpenAI,\n\u001b[0;32m      3\u001b[0m     SyncAzureOpenAI,\n\u001b[0;32m      4\u001b[0m     AsyncAzureOpenAI,\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAzureOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSyncAzureOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsyncAzureOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\llm\\Lib\\site-packages\\llama_index\\llms\\azure_openai\\base.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, Optional, Sequence\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhttpx\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatMessage\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbridge\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, PrivateAttr, root_validator\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CallbackManager\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llama_index\\core\\__init__.py:18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmock_embed_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MockEmbedding\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# indices\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# loading\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     ComposableGraph,\n\u001b[0;32m     20\u001b[0m     DocumentSummaryIndex,\n\u001b[0;32m     21\u001b[0m     GPTDocumentSummaryIndex,\n\u001b[0;32m     22\u001b[0m     GPTKeywordTableIndex,\n\u001b[0;32m     23\u001b[0m     GPTListIndex,\n\u001b[0;32m     24\u001b[0m     GPTRAKEKeywordTableIndex,\n\u001b[0;32m     25\u001b[0m     GPTSimpleKeywordTableIndex,\n\u001b[0;32m     26\u001b[0m     GPTTreeIndex,\n\u001b[0;32m     27\u001b[0m     GPTVectorStoreIndex,\n\u001b[0;32m     28\u001b[0m     KeywordTableIndex,\n\u001b[0;32m     29\u001b[0m     KnowledgeGraphIndex,\n\u001b[0;32m     30\u001b[0m     ListIndex,\n\u001b[0;32m     31\u001b[0m     RAKEKeywordTableIndex,\n\u001b[0;32m     32\u001b[0m     SimpleKeywordTableIndex,\n\u001b[0;32m     33\u001b[0m     SummaryIndex,\n\u001b[0;32m     34\u001b[0m     TreeIndex,\n\u001b[0;32m     35\u001b[0m     VectorStoreIndex,\n\u001b[0;32m     36\u001b[0m     load_graph_from_storage,\n\u001b[0;32m     37\u001b[0m     load_index_from_storage,\n\u001b[0;32m     38\u001b[0m     load_indices_from_storage,\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# structured\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstruct_store\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     SQLDocumentContextBuilder,\n\u001b[0;32m     44\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\llama_index\\core\\indices\\__init__.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# indices\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomposability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComposableGraph\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_summary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     DocumentSummaryIndex,\n\u001b[0;32m      7\u001b[0m     GPTDocumentSummaryIndex,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_summary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentSummaryIndex\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mempty\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmptyIndex, GPTEmptyIndex\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.core.indices.document_summary'"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.azure_openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ddb8fd-c907-4ea3-987a-eb7b1f293439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d267e1-325b-443f-a9d6-52ae97922a80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
