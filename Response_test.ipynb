{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c3d1ee1-3372-4984-979b-b76705b5928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade llama_index llama-index-llms-azure-openai llama-index-graph-stores-neo4j llama-index-embeddings-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "823ef567-bc8f-4e2b-92c7-9c5b7c389191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilitiesPOC import *\n",
    "\n",
    "from auths import openaisecret, embeddings_secret, adls_secret\n",
    "\n",
    "storage_account =  '{\"storage_account_key\": \"\",\"storage_container\": \"manufacturing-copilot\",\"storage_account\": \"euwdsrg03rsta07dls01\"}'\n",
    "path = \"Documents/Embeddings/\"\n",
    "\n",
    "\n",
    "\n",
    "llmconfig= '{\"mode\": \"langchain\", \"auth\": \"token\", \"model\": \"gpt-35-turbo-16k\", \"deployment_name\": \"mxdrca\",\"api_version\": \"2023-05-15\",\"api_key_secret\":\"\",\"api_endpoint\": \"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "embedconfig= '{\"embedmode\": \"langchain\",\"model\":\"text-embedding-ada-002\",\"deployment_name\":\"azure_embedding\",\"api_key_secret\":\"\",\"embeddingurl\":\"https://usedoai0efaoa03.openai.azure.com/\"}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b16c8ade-2780-4e66-ba5d-288c0c652ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 12:17:07 [INFO]: Request URL: 'https://euwdsrg03rsta07dls01.dfs.core.windows.net/manufacturing-copilot?resource=REDACTED&directory=REDACTED&recursive=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Accept': 'application/json'\n",
      "    'User-Agent': 'azsdk-python-storage-dfs/12.14.0 Python/3.11.0 (Windows-10-10.0.22621-SP0)'\n",
      "    'x-ms-date': 'REDACTED'\n",
      "    'x-ms-client-request-id': 'a372b8c0-d024-11ee-8083-141333031892'\n",
      "    'Authorization': 'REDACTED'\n",
      "No body was attached to the request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to use the LLM use it as msg = HumanMessage(content= your query) \n",
      " llm(messages = [msg]) \n",
      "Using Azure OpenAI\n",
      "Using Azure OpenAI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 12:17:08 [INFO]: Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json;charset=utf-8'\n",
      "    'Server': 'Windows-Azure-HDFS/1.0 Microsoft-HTTPAPI/2.0'\n",
      "    'x-ms-request-id': '036efaa2-c01f-0027-2d31-6426cd000000'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'x-ms-client-request-id': 'a372b8c0-d024-11ee-8083-141333031892'\n",
      "    'Date': 'Tue, 20 Feb 2024 19:17:07 GMT'\n",
      "2024-02-20 12:17:08 [INFO]: Request URL: 'https://euwdsrg03rsta07dls01.blob.core.windows.net/manufacturing-copilot/Documents/Embeddings/embeddings_12022024.pkl'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-range': 'REDACTED'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Accept': 'application/xml'\n",
      "    'User-Agent': 'azsdk-python-storage-dfs/12.14.0 Python/3.11.0 (Windows-10-10.0.22621-SP0)'\n",
      "    'x-ms-date': 'REDACTED'\n",
      "    'x-ms-client-request-id': 'a3e5c598-d024-11ee-b157-141333031892'\n",
      "    'Authorization': 'REDACTED'\n",
      "No body was attached to the request\n",
      "2024-02-20 12:17:08 [INFO]: Response status: 206\n",
      "Response headers:\n",
      "    'Content-Length': '73944'\n",
      "    'Content-Type': 'application/octet-stream'\n",
      "    'Content-Range': 'REDACTED'\n",
      "    'Last-Modified': 'Mon, 12 Feb 2024 11:04:17 GMT'\n",
      "    'Accept-Ranges': 'REDACTED'\n",
      "    'ETag': '\"0x8DC2BBA5B6060C1\"'\n",
      "    'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'\n",
      "    'x-ms-request-id': 'd09489a7-401e-0029-7c31-640f7d000000'\n",
      "    'x-ms-client-request-id': 'a3e5c598-d024-11ee-b157-141333031892'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'x-ms-resource-type': 'REDACTED'\n",
      "    'x-ms-creation-time': 'REDACTED'\n",
      "    'x-ms-blob-content-md5': 'REDACTED'\n",
      "    'x-ms-lease-status': 'REDACTED'\n",
      "    'x-ms-lease-state': 'REDACTED'\n",
      "    'x-ms-blob-type': 'REDACTED'\n",
      "    'Content-Disposition': 'REDACTED'\n",
      "    'x-ms-server-encrypted': 'REDACTED'\n",
      "    'x-ms-owner': 'REDACTED'\n",
      "    'x-ms-group': 'REDACTED'\n",
      "    'x-ms-permissions': 'REDACTED'\n",
      "    'x-ms-acl': 'REDACTED'\n",
      "    'Date': 'Tue, 20 Feb 2024 19:17:07 GMT'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using langchain graph class\n",
      "Getting vector retriever\n",
      "Building vector chain \n",
      "I AM USING VECTOR LETS GO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 12:17:10 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  \n",
      "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "                ----\n",
      "                ﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0001198\n",
      "Issue Description: Variations in the quality and accuracy of produced machined parts on Assembly Line 7\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.51 ohms, Voltage fluctuation : 4%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "                ----\n",
      "                \n",
      "Human: Question:```where and why did the production stoppage occurred```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 12:17:13 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a detailed analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, factors such as dust accumulation and lubrication frequency could also impact the overall performance of the assembly line. A thorough investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.\n",
      "Getting vector retriever\n",
      "Building vector chain \n",
      "I AM USING VECTOR LETS GO\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='where and why did the production stoppage occurred'), AIMessage(content='Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a detailed analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, factors such as dust accumulation and lubrication frequency could also impact the overall performance of the assembly line. A thorough investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.')]\n",
      "Follow Up Input: what was my previous question?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 12:17:14 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-20 12:17:14 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  \n",
      "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "                ----\n",
      "                ﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "RCA Reference Guide\n",
      "Please refer the systematic sequence of steps listed below to methodically identify and address root\n",
      "cause issues contributing to various issues in production quality.\n",
      "1)Check the cooling mechanisms: Standard Airflow rate through cooling vents is >50 CFM.\n",
      "2)Evaluate ambient temperature around the motor: Standard Value is 20-25 degrees Celsius.\n",
      "3)Motor surface temperature during operation:  Standard Value is <60 degrees Celsius.\n",
      "If any of the values in steps 1,2 and 3 are found to breach standard values, the likely cause\n",
      "would be Inadequate cooling leading to motor overheating. If not, proceed to the next steps\n",
      "4)Check the rated power of the motor: Standard Value is 1.5 HP\n",
      "5)Assess the load on the motor: Standard Value is <80% of rated current.\n",
      "6)Check user workloads: Standard value is <80% of rated workload.\n",
      "If any of the values in steps 4,5 and 6 are found to breach standard values, the likely cause\n",
      "would be Overloading the motor beyond its capacity. If not, proceed to the next steps\n",
      "7)Voltage stability: standard value is +-5%\n",
      "8)Measure resistance in power wiring: Standard power wiring resistance is <0.1 ohms.\n",
      "9)Monitor fluctuations during operation: Standard Value is <2% variation.\n",
      "If any of the values in steps 7,8 and 9 are found to breach standard values, the likely cause\n",
      "would be Insufficient power supply causing fluctuations. If not, proceed to the next steps\n",
      "10)Review maintenance logs: Frequency standard is Monthly.\n",
      "11)Inspect for dust accumulation on motor surfaces: standard value is <1 mm thickness.\n",
      "12)Measure lubrication frequency : Standard Value is every 50 operating hours.\n",
      "If any of the values in steps 10,11 and 12 are found to breach standard values, the likely\n",
      "cause would be lack of regular maintenance, including\n",
      "cleaning and lubrication. If not, the root cause is a new one which is not listed.\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "                ----\n",
      "                \n",
      "Human: Question:```What was the question I asked before?```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 12:17:14 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I apologize, but I am an AI assistant and do not have the capability to remember previous questions. Is there anything else I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %run ./utilitiesPOC\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from utilitiesPOC import *\n",
    "\n",
    "from auths import openaisecret, embeddings_secret, adls_secret\n",
    "\n",
    "storage_account =  '{\"storage_account_key\": \"\",\"storage_container\": \"manufacturing-copilot\",\"storage_account\": \"euwdsrg03rsta07dls01\"}'\n",
    "path = \"Documents/Embeddings/\"\n",
    "\n",
    "\n",
    "\n",
    "llmconfig= '{\"mode\": \"langchain\", \"auth\": \"token\", \"model\": \"gpt-35-turbo-16k\", \"deployment_name\": \"mxdrca\",\"api_version\": \"2023-05-15\",\"api_key_secret\":\"\",\"api_endpoint\": \"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "embedconfig= '{\"embedmode\": \"langchain\",\"model\":\"text-embedding-ada-002\",\"deployment_name\":\"azure_embedding\",\"api_key_secret\":\"\",\"embeddingurl\":\"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "\n",
    "storage_account = json.loads(storage_account)\n",
    "storage_account_key, container, storage_account = storage_account.get('storage_account_key'),storage_account.get('storage_container'),storage_account.get('storage_account')\n",
    "\n",
    "#read from file\n",
    "storage_account_key = adls_secret\n",
    "\n",
    "llmconfig = json.loads(llmconfig)\n",
    "embedconfig = json.loads(embedconfig)\n",
    "\n",
    "embedmode, embedmodel,embed_deployment,embed_key, embed_url = embedconfig.values()\n",
    "mode, auth, model, deployment_name, api_version, api_key_secret, api_endpoint = llmconfig.values()\n",
    "\n",
    "api_key_secret = openaisecret\n",
    "embed_key = embeddings_secret\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#GETTING THE SECRETS\n",
    "api_key = api_key_secret\n",
    "\n",
    "\n",
    "embed_key = embed_key\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Define LLM \n",
    "#DEFINE LLM USING AZURE OPEN AI\n",
    "llmrca = getllm(openai=True,azure =True,mode = mode, auth=auth,model=model,dep_name = deployment_name,api_version= api_version,\n",
    "                     api_key =api_key,api_base = api_endpoint)\n",
    "\n",
    "\n",
    "embeddings_model  = getembeddings(model=embedmodel,dep_name=embed_deployment,api_key=embed_key,api_base=embed_url)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "mount_point = '/mnt/copilotembedds'\n",
    "\n",
    "\n",
    "#@async_retry\n",
    "def read_embeddings(fromadls=False,dbricks=False,**kwargs):\n",
    "    if fromadls:\n",
    "        if dbricks:\n",
    "            #reads it into a dataframe first from the mount point\n",
    "            storage_account=storage_account,container=container,mount_point=mount_point,path=path,storage_account_key=storage_account_key\n",
    "            for mount in dbutils.fs.mounts():\n",
    "                if mount.mountPoint.startswith(mount_point):\n",
    "                    print(\"Unmounting mnt point\")\n",
    "                    dbutils.fs.unmount(mount.mountPoint)\n",
    "            \n",
    "            #mount to path\n",
    "            dbutils.fs.refreshMounts() #refresh mounts to avoid errors\n",
    "\n",
    "            mntfiles = mount_to_local(storage_account, container, mount_point,path, storage_account_key)\n",
    "            #reads into dataframe\n",
    "            adlsembedspath = [e.path for e in dbutils.fs.ls(mntfiles)][-1] #read last file \n",
    "            adlsembedspath= adlsembedspath.replace(\"dbfs:\",\"/dbfs\")\n",
    "            df = pd.read_pickle(adlsembedspath)\n",
    "\n",
    "        else:\n",
    "            #read from adls but not from databricks\n",
    "            from azure.storage.filedatalake import DataLakeServiceClient\n",
    "            storage_account,container,path,storage_account_key=kwargs.get(\"storage_account\"),kwargs.get(\"container\"),kwargs.get(\"path\"),kwargs.get(\"storage_account_key\")\n",
    "            connstr = f\"DefaultEndpointsProtocol=https;AccountName={storage_account};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "            service = DataLakeServiceClient.from_connection_string(conn_str=connstr)\n",
    "            file_system_client = service.get_file_system_client(container)\n",
    "            paths = file_system_client.get_paths(path=path)\n",
    "            files = [p for p in paths if not p.is_directory]\n",
    "            files.sort(key=lambda x: x.last_modified)\n",
    "            last_file = files[-1].name\n",
    "            file_client = file_system_client.get_file_client(last_file)\n",
    "            \n",
    "            # Download the file content and load it as a Python object\n",
    "            download = file_client.download_file()\n",
    "            downloaded_bytes = download.readall()\n",
    "            df = pickle.loads(downloaded_bytes)\n",
    "            #df = pd.read_pickle(data)\n",
    "            \n",
    "    else:\n",
    "        #read from local FS\n",
    "        adlsembedspath = [n for n in os.listdir(path)][-1]\n",
    "        \n",
    "        df = pd.read_pickle(adlsembedspath)\n",
    "    #returns the texts and embeddings\n",
    "    txtlist = [t for t in df.texts]\n",
    "    embeddings = [e for e in df.embeddings]\n",
    "    return txtlist,embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_embeddings_fromfs(path):\n",
    "    #service = DataLakeServiceClient.from_connection_string(conn_str=\"my_connection_string\")\n",
    "\n",
    "    adlsembedspath = [n for n in os.listdir(path)][-1]        \n",
    "    df = pd.read_pickle(path+\"/\"+adlsembedspath)\n",
    "    #returns the texts and embeddings\n",
    "    txtlist = [t for t in df.texts]\n",
    "    embeddings = [e for e in df.embeddings]\n",
    "    return txtlist,embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "loaded_embeddings = read_embeddings(fromadls=True,dbricks=False,storage_account=storage_account,container=container,path=path,storage_account_key=storage_account_key)#read_embeddings_fromfs(path)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import ClassVar\n",
    "import pickle\n",
    "\n",
    "@dataclass \n",
    "class RAG2:\n",
    "    memory_conversation  = None\n",
    "    def __init__(self,llm,graph,type=\"langchain\",embeddings=None,usememory=False):\n",
    "        self.llm = llm\n",
    "        self.graph = graph\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "        self.usememory = usememory\n",
    "\n",
    "    @staticmethod\n",
    "    def memory(type='langchain',memory_key=\"chat_history\",output_key='result',rmessages=True,readmem=None):\n",
    "        if readmem:\n",
    "            if type.lower()==\"langchain\":\n",
    "                from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "                memory = ConversationBufferMemory(\n",
    "                memory_key=memory_key, return_messages=rmessages)\n",
    "                #output_key=output_key)\n",
    "                \n",
    "                readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "\n",
    "                return memory\n",
    "        else: \n",
    "            return None\n",
    "    \n",
    "\n",
    "    def vector_retriever(self,store=\"chroma\",**kwargs):\n",
    "        texts = kwargs.get(\"texts\")\n",
    "        if store.lower()==\"chroma\":\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "\n",
    "            from langchain.chains import RetrievalQA\n",
    "            persist_directory = kwargs.get(\"persist_directory\",\"filesdb\")\n",
    "            search_kwargs = kwargs.get(\"search_kwargs\",{\"k\": 3})\n",
    "            from langchain.vectorstores import Chroma\n",
    "            \n",
    "            db = Chroma.from_documents(texts,self.embeddings,persist_directory= persist_directory)\n",
    "            vec_retriever = db.as_retriever(search_kwargs=search_kwargs)\n",
    "         \n",
    "            return vec_retriever  \n",
    "\n",
    "            \n",
    "        elif store.lower()==\"faiss\":\n",
    "            #to be implemented\n",
    "            from langchain_community.vectorstores import FAISS\n",
    "            from langchain_core.output_parsers import StrOutputParser\n",
    "            from langchain_core.prompts import ChatPromptTemplate\n",
    "            from langchain_core.runnables import RunnablePassthrough\n",
    "            from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "            if kwargs.get(\"text_embeddings\"):\n",
    "                text_embeddings =kwargs.get(\"text_embeddings\")\n",
    "                vectorstore = FAISS.from_embeddings(text_embeddings=list(zip(text_embeddings[0],text_embeddings[1])), embedding = self.embeddings)\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                return retriever\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                template = \"\"\"Answer the question based only on the following context:{context} \n",
    "                Question: {query}\"\"\"\n",
    "                prompt = ChatPromptTemplate.from_template(template)\n",
    "                retrieval_chain = ({\"context\": retriever, \"query\": RunnablePassthrough()} | prompt\n",
    "                                    | self.llm| StrOutputParser())\n",
    "                return retrieval_chain\n",
    "        \n",
    "    def build_chain(self,**kwargs):\n",
    "        #memory = self.memory() if (readmem := self.memory()) else None\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        \n",
    "        #PROMPT TEMPLATE        \n",
    "        from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate,PromptTemplate\n",
    "        #####\n",
    "\n",
    "        memory = self.usememory\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "\n",
    "        if memory:\n",
    "            prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {question}\")\n",
    "        else:\n",
    "            prompt = PromptTemplate(input_variables=[\"question\"],template =\"{question}\")\n",
    "\n",
    "        self.memory_conversation = self.memory(readmem=memory) if not self.memory_conversation else self.memory_conversation\n",
    "\n",
    "        if kwargs.get(\"falkor\"):\n",
    "            from langchain.chains import FalkorDBQAChain\n",
    "            chain = FalkorDBQAChain.from_llm(llm=self.llm, graph=self.graph, verbose=verbose, memory=self.memory_conversation, **kwargs)\n",
    "        else:\n",
    "            if method.lower()==\"graph\":\n",
    "                #general chain for graphs\n",
    "                print(\"building graph chain\")\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                #Prompt template\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "\n",
    "                chain = GraphCypherQAChain.from_llm(llm=self.llm, prompt=prompt,graph=self.graph,validate_cypher = True, \n",
    "                                                    verbose=verbose,memory=self.memory_conversation, promptTemplate=qa_prompt,**kwargs)\n",
    "                chain.input_key = \"question\"\n",
    "\n",
    "            elif method.lower()==\"vector\":\n",
    "                print(\"Building vector chain \")\n",
    "                retriever = kwargs.get(\"vec_retriever\")\n",
    "                from langchain.chains import RetrievalQA\n",
    "                from langchain.chains import ConversationalRetrievalChain\n",
    "                \n",
    "                # template = (\"Combine the chat history and follow up question into \"\n",
    "                # \"a standalone question. Chat History: {chat_history}\"\n",
    "                # \"Follow up question: {query} \")\n",
    "                template = (\"Every time you respond you should start with This is Copilot your AI assistant\")\n",
    "                prompt = PromptTemplate.from_template(template)\n",
    "                \n",
    "\n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,return_source_documents=True,verbose=verbose)\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "                \n",
    "                chain = ConversationalRetrievalChain.from_llm(self.llm,retriever,memory=self.memory_conversation,get_chat_history=lambda h :h,\n",
    "                                                              verbose=verbose,\n",
    "                                                              combine_docs_chain_kwargs={\"prompt\": qa_prompt}) \n",
    "                \n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,\n",
    "                #                                    return_source_documents=True,verbose=verbose,memory=self.memory_conversation,prompt=prompt)\n",
    "\n",
    "        return chain\n",
    "\n",
    "\n",
    "    def get_retriever(self,**kwargs):\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        complexity = kwargs.get(\"complexity\",\"simple\")\n",
    "        \n",
    "        if method.lower()=='graph':\n",
    "            if self.type.lower()==\"langchain\":\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                if complexity.lower()=='simple':\n",
    "                    print(\"SIMPLE RAG LANGCHAIN\")\n",
    "                    if usememory := kwargs.get(\"usememory\") or self.memory_conversation:\n",
    "                        print(\"With Read memory\")\n",
    "                    \n",
    "                    chain = self.build_chain(**kwargs)\n",
    "                    return chain\n",
    "                \n",
    "                    \n",
    "            elif self.type.lower()==\"llama\":\n",
    "                if complexity.lower()=='simple':\n",
    "                    nl2graph = kwargs.get(\"nl2graph\",False)\n",
    "                    print(nl2graph)\n",
    "                    from llama_index.query_engine import RetrieverQueryEngine\n",
    "                    from llama_index.retrievers import KnowledgeGraphRAGRetriever                    \n",
    "                    graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=kwargs.get(\"storage_context\"),\n",
    "                    service_context=kwargs.get(\"service_context\"),\n",
    "                    llm=self.llm,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=nl2graph)\n",
    "                    query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))\n",
    "                    return query_engine\n",
    "\n",
    "                \n",
    "                elif complexity.lower()==\"custom\":\n",
    "                    if self.type.lower()==\"llama\":\n",
    "                        from llama_index import get_response_synthesizer\n",
    "                        from llama_index.query_engine import RetrieverQueryEngine\n",
    "                        # create custom retriever\n",
    "                        vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "                        kg_retriever = KGTableRetriever(index=kg_index, retriever_mode=\"keyword\", include_text=False)\n",
    "                        custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "                        # create response synthesizer\n",
    "                        response_synthesizer = get_response_synthesizer(service_context=service_context,response_mode=\"tree_summarize\")\n",
    "                        custom_query_engine = RetrieverQueryEngine(retriever=custom_retriever,response_synthesizer=response_synthesizer)\n",
    "                        return custom_query_engine\n",
    "                    elif self.type.lower()=='langchain':\n",
    "                        from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "                        simple = self.get_retriever(\"simple\",**kwargs)\n",
    "                        \n",
    "                        tools = [\n",
    "                            Tool(name= \"GraphRAG\",\n",
    "                                 func= simple.run,\n",
    "                                 description= \"Simple RAG from knowledge graph\"),\n",
    "                            #add more RAG tools\n",
    "                        ]\n",
    "        elif method.lower()==\"vector\":\n",
    "            print(\"Getting vector retriever\")\n",
    "            vec_retriever = self.vector_retriever(**kwargs)\n",
    "\n",
    "            chain = self.build_chain(vec_retriever=vec_retriever,**kwargs)\n",
    "            return chain\n",
    "\n",
    "\n",
    "\n",
    "    def response(self,query,complexity=\"simple\",**kwargs):\n",
    "        method = kwargs.get(\"method\",\"vector\")\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "        retriever = self.get_retriever(**kwargs)\n",
    "        try:\n",
    "            if method.lower()==\"vector\":\n",
    "                print(\"I AM USING VECTOR LETS GO\")\n",
    "                result =  retriever({\"question\": query}).get('answer')\n",
    "                \n",
    "            elif method.lower()==\"graph\":\n",
    "                if self.type.lower()=='langchain':\n",
    "                    print('using retriever({\"query\": query}).get(\"answer\")')\n",
    "                    #result = retriever({\"question\": query}).get('answer')#\n",
    "                    result = retriever.invoke({\"question\": query}).get('result') \n",
    "                    pickled_str = pickle.dumps(self.memory_conversation)\n",
    "                elif self.type.lower()=='llama':\n",
    "                    result = retriever.query(query)\n",
    "                    if 'Empty Response' in result.response:\n",
    "                        raise(\"Error empty result or no information found\")\n",
    "                    else:\n",
    "                        return result.response\n",
    "            \n",
    "            if not result or \"I'm sorry, but I don't have\" in result:\n",
    "                print('Not using RAG')\n",
    "                raise (\"Error empty result or no information found in the KG\")\n",
    "            elif \"I'm sorry, but I don't have\" in result:\n",
    "                raise(\"ERROR 404\")\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if self.type.lower()=='langchain':\n",
    "                from langchain.schema import HumanMessage\n",
    "                #message = HumanMessage(content=query)\n",
    "                from langchain.chains import ConversationChain\n",
    "                from langchain.prompts.prompt import PromptTemplate\n",
    "                if self.usememory:\n",
    "                    print(\"Normal LLM response with same memory\")\n",
    "                    prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {query}\")\n",
    "                    chat = ConversationChain(llm=self.llm,verbose=kwargs.get(\"verbose\",True),prompt=prompt,memory=self.memory_conversation ,input_key=\"question\")\n",
    "                    return chat.invoke({\"question\":query}).get(\"response\")\n",
    "                else:\n",
    "                    from langchain.schema import HumanMessage\n",
    "                    message = HumanMessage(content=query)\n",
    "                    return self.llm.invoke([message])\n",
    "\n",
    "            else:\n",
    "                #return response with llama index self.llm(response)\n",
    "                pass\n",
    "            \n",
    "\n",
    "# COMMAND ----------\n",
    "#start graph\n",
    "url =\"neo4j+s://b75e0bd8.databases.neo4j.io\"\n",
    "username =\"neo4j\"\n",
    "password = \"aziMmumvuVru-rgeKuO0zT7mEtTxu29Jgrmp6lFKv0w\"\n",
    "graph_store = get_graphdb(\"langchain\",graphdb = \"neo4j\",url=url,username=username,password=password)\n",
    "\n",
    "\n",
    "#call RAG class initialize vector class \n",
    "rag2 = RAG2\n",
    "vecrag = rag2(llmrca,graph_store,type=\"langchain\",embeddings = embeddings_model,usememory=True)\n",
    "#vectorchain = vecrag.get_retriever(method=\"vector\",text_embeddings = loaded_embeddings,store=\"faiss\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "\n",
    "    # print(vectorchain({\"question\": \"where and why did the production stoppage occurred\"}).get(\"answer\")) #if use memory we call it like this \n",
    "    # print(vectorchain({\"question\": \"what was my previous question?\"}).get(\"answer\"))\n",
    "    # #\n",
    "    print(vecrag.response(query=\"where and why did the production stoppage occurred\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "    print(vecrag.response(query=\"what was my previous question?\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "# COMMAND ----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "032827ad-4f2f-4c03-aa24-97404d1cda00",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag2 = RAG2\n",
    "vecrag = rag2(llmrca,graph_store,type=\"langchain\",embeddings = embeddings_model,usememory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8f95529-57cb-4674-b450-c501a713a1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 12:17:14 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vector retriever\n",
      "Building vector chain \n",
      "I AM USING VECTOR LETS GO\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  \n",
      "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "                ----\n",
      "                ﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0001198\n",
      "Issue Description: Variations in the quality and accuracy of produced machined parts on Assembly Line 7\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.51 ohms, Voltage fluctuation : 4%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "                ----\n",
      "                \n",
      "Human: Question:```where and why did the production stoppage occurred```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 12:17:18 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a detailed analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, factors such as dust accumulation and lubrication frequency could also impact the overall performance of the assembly line. A thorough investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.\n",
      "Getting vector retriever\n",
      "Building vector chain \n",
      "I AM USING VECTOR LETS GO\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='where and why did the production stoppage occurred'), AIMessage(content='Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a detailed analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, factors such as dust accumulation and lubrication frequency could also impact the overall performance of the assembly line. A thorough investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.')]\n",
      "Follow Up Input: what was my previous question?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 12:17:18 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-20 12:17:18 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  \n",
      "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "                ----\n",
      "                ﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "RCA Reference Guide\n",
      "Please refer the systematic sequence of steps listed below to methodically identify and address root\n",
      "cause issues contributing to various issues in production quality.\n",
      "1)Check the cooling mechanisms: Standard Airflow rate through cooling vents is >50 CFM.\n",
      "2)Evaluate ambient temperature around the motor: Standard Value is 20-25 degrees Celsius.\n",
      "3)Motor surface temperature during operation:  Standard Value is <60 degrees Celsius.\n",
      "If any of the values in steps 1,2 and 3 are found to breach standard values, the likely cause\n",
      "would be Inadequate cooling leading to motor overheating. If not, proceed to the next steps\n",
      "4)Check the rated power of the motor: Standard Value is 1.5 HP\n",
      "5)Assess the load on the motor: Standard Value is <80% of rated current.\n",
      "6)Check user workloads: Standard value is <80% of rated workload.\n",
      "If any of the values in steps 4,5 and 6 are found to breach standard values, the likely cause\n",
      "would be Overloading the motor beyond its capacity. If not, proceed to the next steps\n",
      "7)Voltage stability: standard value is +-5%\n",
      "8)Measure resistance in power wiring: Standard power wiring resistance is <0.1 ohms.\n",
      "9)Monitor fluctuations during operation: Standard Value is <2% variation.\n",
      "If any of the values in steps 7,8 and 9 are found to breach standard values, the likely cause\n",
      "would be Insufficient power supply causing fluctuations. If not, proceed to the next steps\n",
      "10)Review maintenance logs: Frequency standard is Monthly.\n",
      "11)Inspect for dust accumulation on motor surfaces: standard value is <1 mm thickness.\n",
      "12)Measure lubrication frequency : Standard Value is every 50 operating hours.\n",
      "If any of the values in steps 10,11 and 12 are found to breach standard values, the likely\n",
      "cause would be lack of regular maintenance, including\n",
      "cleaning and lubrication. If not, the root cause is a new one which is not listed.\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "                ----\n",
      "                \n",
      "Human: Question:```What was the question I asked before?```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 12:17:19 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I apologize, but I am an AI assistant and do not have the capability to remember previous questions. Is there anything else I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "print(vecrag.response(query=\"where and why did the production stoppage occurred\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "print(vecrag.response(query=\"what was my previous question?\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fce3f3aa-0cbe-4a44-ace6-0f2e12b4a12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vector retriever\n",
      "Building vector chain \n",
      "SIMPLE RAG LANGCHAIN\n",
      "With Read memory\n",
      "building graph chain\n"
     ]
    }
   ],
   "source": [
    "#vector\n",
    "vectorchain= vecrag.get_retriever(method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings)\n",
    "## graoh CHAIN \n",
    "graphchain= vecrag.get_retriever(method=\"graph\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6991a7-573e-4060-b94a-b5312907937b",
   "metadata": {},
   "source": [
    "Testing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d61e05bd-2d29-4d4f-8513-597a3055ae49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='where and why did the production stoppage occurred'), AIMessage(content='Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a detailed analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, factors such as dust accumulation and lubrication frequency could also impact the overall performance of the assembly line. A thorough investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.'), HumanMessage(content='what was my previous question?'), AIMessage(content='I apologize, but I am an AI assistant and do not have the capability to remember previous questions. Is there anything else I can assist you with?'), HumanMessage(content='what did the study concluded'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='lifespan'), AIMessage(content=\"I'm sorry, but I don't have any information on lifespan.\"), HumanMessage(content='what did the study conclude?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what is the capital of usa?'), AIMessage(content='The capital of the United States of America is Washington, D.C.'), HumanMessage(content=\"MATCH (hurricane:Disaster {name: 'Hurricane Ian'})-[:CAUSED]->(damage:Damage) RETURN damage.name\"), AIMessage(content=\"I'm sorry, but I don't have any information about the specific damage caused by Hurricane Ian.\"), HumanMessage(content='what damages caused hurricane ian?'), AIMessage(content=\"I'm sorry, but I don't have any information on the damages caused by Hurricane Ian.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='How many tickets are there?'), AIMessage(content='There are four incident tickets mentioned in the context.')]\n",
      "Follow Up Input: How many tickets are there?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:35:59 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:36:00 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  \n",
      "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "                ----\n",
      "                ﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0001198\n",
      "Issue Description: Variations in the quality and accuracy of produced machined parts on Assembly Line 7\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.51 ohms, Voltage fluctuation : 4%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "                ----\n",
      "                \n",
      "Human: Question:```How many incident tickets are mentioned in the context?```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:36:00 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are four incident tickets mentioned in the context.'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorchain.invoke({\"question\":\"How many tickets are there?\"}).get(\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60c691db-5002-42ee-bd21-02664bd71074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 12:18:49 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (s:Entity)-[:CONCLUDES]->(c:Entity)\n",
      "RETURN c\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'c': {'id': 'only \"about 13% of methane emissions are covered\"'}}, {'c': {'id': 'only \"about 13% of methane emissions are covered by methane mitigation policies\"'}}, {'c': {'id': 'effectiveness of methane mitigation policies \"is far from clear\"'}}]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 12:18:50 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphchain.invoke({\"question\":\"what did the study concluded\"}).get(\"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a69ac14-c683-446d-a8bb-3bead5a61510",
   "metadata": {},
   "source": [
    "## Langchain tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f06435e-7b5b-4161-bea8-08047cfa8b51",
   "metadata": {},
   "source": [
    "To combine vector retriever and Knowledge Graphs we can use langchain tools and an agent executor which combines it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e8c88e66-6bbc-45ba-85a9-19d50fd8ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, create_structured_chat_agent\n",
    "\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "11da30b2-6611-4fbb-829c-2653dc6603ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding vector retriever as tool\n",
    "\n",
    "\n",
    "\n",
    "toolslgc = [\n",
    "    \n",
    "    Tool(\n",
    "        name=\"copilot_retriever\",\n",
    "        func=lambda q:vectorchain.invoke({\"question\":q}).get(\"answer\"),\n",
    "        description=\"useful for when you want to answer questions about RCA documents and tickets using FAISS.\",\n",
    "        return_direct=False,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Copilot_Knowledge\",\n",
    "        func=lambda q:graphchain.invoke({\"question\":q}).get(\"result\"),\n",
    "        description=\"useful for when you want to answer questions about neo4j kg using langchain.\",\n",
    "        return_direct=False,\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "llm_with_tools= llmazure.bind_tools(toolslgc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "42899724-1336-443a-8889-fa8a20be9ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='where and why did the production stoppage occurred'), AIMessage(content='Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a detailed analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, factors such as dust accumulation and lubrication frequency could also impact the overall performance of the assembly line. A thorough investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.'), HumanMessage(content='what was my previous question?'), AIMessage(content='I apologize, but I am an AI assistant and do not have the capability to remember previous questions. Is there anything else I can assist you with?'), HumanMessage(content='what did the study concluded'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='lifespan'), AIMessage(content=\"I'm sorry, but I don't have any information on lifespan.\"), HumanMessage(content='what did the study conclude?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what is the capital of usa?'), AIMessage(content='The capital of the United States of America is Washington, D.C.'), HumanMessage(content=\"MATCH (hurricane:Disaster {name: 'Hurricane Ian'})-[:CAUSED]->(damage:Damage) RETURN damage.name\"), AIMessage(content=\"I'm sorry, but I don't have any information about the specific damage caused by Hurricane Ian.\"), HumanMessage(content='what damages caused hurricane ian?'), AIMessage(content=\"I'm sorry, but I don't have any information on the damages caused by Hurricane Ian.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='How many tickets are there?'), AIMessage(content='There are four incident tickets mentioned in the context.'), HumanMessage(content='How many tickets are there?'), AIMessage(content='There are four incident tickets mentioned in the context.'), HumanMessage(content='what is the first incident ticket about?'), AIMessage(content=\"I'm sorry, but I don't have any information about the first incident ticket.\"), HumanMessage(content='what is the first incident ticket about?'), AIMessage(content='The subject of the first incident ticket is \"Production Line 2 sudden disruption due to unexpected shutdown of Benchmill.\"'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='I have information about three incident tickets related to manufacturing issues on different assembly lines. Each ticket provides a description of the issue, parameter values, and maintenance logs. I can use this information to perform root cause analysis and identify the likely causes of the problems.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, FMEA, maintenance practices, and troubleshooting techniques. I can provide guidance on various topics such as production optimization, quality improvement, equipment maintenance, process efficiency, and problem-solving methodologies. If you have specific questions or need assistance with a particular topic, feel free to ask.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, FMEA, maintenance practices, and troubleshooting techniques. I can provide guidance on various topics such as production optimization, quality improvement, equipment maintenance, process efficiency, and problem-solving methodologies. If you have specific questions or need assistance with a particular topic, feel free to ask.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, failure modes and effects analysis (FMEA), maintenance practices, and more. I can provide guidance on troubleshooting production issues, optimizing manufacturing processes, improving product quality, and implementing preventive maintenance strategies. Additionally, I can assist with data analysis, trend identification, and decision-making based on historical data.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, FMEA, maintenance practices, and troubleshooting techniques. I can provide guidance on various topics such as production optimization, quality improvement, equipment maintenance, process efficiency, and problem-solving methodologies. If you have specific questions or need assistance with a particular topic, feel free to ask.')]\n",
      "Follow Up Input: what is the first incident ticket about?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:45:01 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-20 13:45:01 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  \n",
      "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "                ----\n",
      "                ﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0001198\n",
      "Issue Description: Variations in the quality and accuracy of produced machined parts on Assembly Line 7\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.51 ohms, Voltage fluctuation : 4%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "                ----\n",
      "                \n",
      "Human: Question:```What is the subject of the first incident ticket?```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:45:02 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The subject of the first incident ticket is \"Production Line 2 sudden disruption due to unexpected shutdown of Benchmill.\"\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:45:03 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (:Entity)-[:CONCLUDES]->(conclusion:Entity)\n",
      "RETURN conclusion\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'conclusion': {'id': 'only \"about 13% of methane emissions are covered\"'}}, {'conclusion': {'id': 'only \"about 13% of methane emissions are covered by methane mitigation policies\"'}}, {'conclusion': {'id': 'effectiveness of methane mitigation policies \"is far from clear\"'}}]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:45:03 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.\n"
     ]
    }
   ],
   "source": [
    "print(toolslgc[0].run(\"what is the first incident ticket about?\"))\n",
    "print(toolslgc[1].run(\"what did the study concluded?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c3af6041-20e8-477f-ad71-885d7eef7cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchainhub in c:\\users\\o_hol\\.conda\\envs\\copilotrca\\lib\\site-packages (0.1.14)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\o_hol\\.conda\\envs\\copilotrca\\lib\\site-packages (from langchainhub) (2.31.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in c:\\users\\o_hol\\.conda\\envs\\copilotrca\\lib\\site-packages (from langchainhub) (2.31.0.20240218)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\o_hol\\.conda\\envs\\copilotrca\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\o_hol\\.conda\\envs\\copilotrca\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\o_hol\\.conda\\envs\\copilotrca\\lib\\site-packages (from requests<3,>=2->langchainhub) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\o_hol\\.conda\\envs\\copilotrca\\lib\\site-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing line 1 of C:\\Users\\o_hol\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib-3.6.3-py3.11-nspkg.pth:\n",
      "\n",
      "  Traceback (most recent call last):\n",
      "    File \"<frozen site>\", line 186, in addpackage\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"<frozen importlib._bootstrap>\", line 570, in module_from_spec\n",
      "  AttributeError: 'NoneType' object has no attribute 'loader'\n",
      "\n",
      "Remainder of file ignored\n"
     ]
    }
   ],
   "source": [
    "!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a4a86dc3-f437-4811-aa7c-96f37e5ee284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "agent = create_openai_tools_agent(llmazure, toolslgc, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=toolslgc, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5e45ccc5-e6c0-44e3-9441-6d97aab79dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "querytemplate= \"try using the functions Copilot_Knowledge first to answer the users query then the copilot retriever, this is the query:{question}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "318c3a35-3c2d-4317-9653-5758b6a9858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:45:31 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Copilot_Knowledge` with `tell me everything you know about`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:45:32 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (e1:Entity)-[r]->(e2:Entity)\n",
      "RETURN e1, r, e2\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'e1': {'id': 'pair'}, 'r': ({'id': 'pair'}, 'CHALLENGE', {'id': 'theories of planetary formation'}), 'e2': {'id': 'theories of planetary formation'}}, {'e1': {'id': 'pair'}, 'r': ({'id': 'pair'}, 'BE', {'id': 'lowest-mass star hosting'}), 'e2': {'id': 'lowest-mass star hosting'}}, {'e1': {'id': 'Five employees'}, 'r': ({'id': 'Five employees'}, 'PUBLISH', {'id': 'tropical cyclone report'}), 'e2': {'id': 'tropical cyclone report'}}, {'e1': {'id': 'August'}, 'r': ({'id': 'August'}, 'REACH', {'id': 'new record high temperature'}), 'e2': {'id': 'new record high temperature'}}, {'e1': {'id': \"world's oceans\"}, 'r': ({'id': \"world's oceans\"}, 'REACH', {'id': 'new record high temperature'}), 'e2': {'id': 'new record high temperature'}}, {'e1': {'id': 'Hurricane Ian'}, 'r': ({'id': 'Hurricane Ian'}, 'UPGRADES', {'id': 'Category 4 to Category 5'}), 'e2': {'id': 'Category 4 to Category 5'}}, {'e1': {'id': 'Hurricane Ian'}, 'r': ({'id': 'Hurricane Ian'}, 'CAUSED', {'id': '$112.9 billion worth of damage'}), 'e2': {'id': '$112.9 billion worth of damage'}}, {'e1': {'id': 'Hurricane Ian'}, 'r': ({'id': 'Hurricane Ian'}, 'STRIKES', {'id': 'Florida'}), 'e2': {'id': 'Florida'}}, {'e1': {'id': 'oxygen partial pressure'}, 'r': ({'id': 'oxygen partial pressure'}, 'IS_ABOVE', {'id': '18%'}), 'e2': {'id': '18%'}}, {'e1': {'id': 'emissions'}, 'r': ({'id': 'emissions'}, 'RISE', {'id': 'chlorofluorocarbons'}), 'e2': {'id': 'chlorofluorocarbons'}}]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:45:37 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mI know several pieces of information about different topics. Here are some of them:\n",
      "\n",
      "1. There are theories of planetary formation that have been challenged.\n",
      "2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\n",
      "3. Five employees have published a tropical cyclone report.\n",
      "4. In August, there was a new record high temperature reached.\n",
      "5. The world's oceans have also reached a new record high temperature.\n",
      "6. Hurricane Ian was upgraded from Category 4 to Category 5.\n",
      "7. Hurricane Ian caused $112.9 billion worth of damage.\n",
      "8. Hurricane Ian struck Florida.\n",
      "9. The oxygen partial pressure is above 18%.\n",
      "10. Emissions of chlorofluorocarbons have risen.\n",
      "\n",
      "Please let me know if you would like more information on any specific topic.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:45:38 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `copilot_retriever` with `tell me everything you know about`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='where and why did the production stoppage occurred'), AIMessage(content='Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a detailed analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, factors such as dust accumulation and lubrication frequency could also impact the overall performance of the assembly line. A thorough investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.'), HumanMessage(content='what was my previous question?'), AIMessage(content='I apologize, but I am an AI assistant and do not have the capability to remember previous questions. Is there anything else I can assist you with?'), HumanMessage(content='what did the study concluded'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='lifespan'), AIMessage(content=\"I'm sorry, but I don't have any information on lifespan.\"), HumanMessage(content='what did the study conclude?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what is the capital of usa?'), AIMessage(content='The capital of the United States of America is Washington, D.C.'), HumanMessage(content=\"MATCH (hurricane:Disaster {name: 'Hurricane Ian'})-[:CAUSED]->(damage:Damage) RETURN damage.name\"), AIMessage(content=\"I'm sorry, but I don't have any information about the specific damage caused by Hurricane Ian.\"), HumanMessage(content='what damages caused hurricane ian?'), AIMessage(content=\"I'm sorry, but I don't have any information on the damages caused by Hurricane Ian.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='How many tickets are there?'), AIMessage(content='There are four incident tickets mentioned in the context.'), HumanMessage(content='How many tickets are there?'), AIMessage(content='There are four incident tickets mentioned in the context.'), HumanMessage(content='what is the first incident ticket about?'), AIMessage(content=\"I'm sorry, but I don't have any information about the first incident ticket.\"), HumanMessage(content='what is the first incident ticket about?'), AIMessage(content='The subject of the first incident ticket is \"Production Line 2 sudden disruption due to unexpected shutdown of Benchmill.\"'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='I have information about three incident tickets related to manufacturing issues on different assembly lines. Each ticket provides a description of the issue, parameter values, and maintenance logs. I can use this information to perform root cause analysis and identify the likely causes of the problems.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, FMEA, maintenance practices, and troubleshooting techniques. I can provide guidance on various topics such as production optimization, quality improvement, equipment maintenance, process efficiency, and problem-solving methodologies. If you have specific questions or need assistance with a particular topic, feel free to ask.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, FMEA, maintenance practices, and troubleshooting techniques. I can provide guidance on various topics such as production optimization, quality improvement, equipment maintenance, process efficiency, and problem-solving methodologies. If you have specific questions or need assistance with a particular topic, feel free to ask.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, failure modes and effects analysis (FMEA), maintenance practices, and more. I can provide guidance on troubleshooting production issues, optimizing manufacturing processes, improving product quality, and implementing preventive maintenance strategies. Additionally, I can assist with data analysis, trend identification, and decision-making based on historical data.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, FMEA, maintenance practices, and troubleshooting techniques. I can provide guidance on various topics such as production optimization, quality improvement, equipment maintenance, process efficiency, and problem-solving methodologies. If you have specific questions or need assistance with a particular topic, feel free to ask.'), HumanMessage(content='what is the first incident ticket about?'), AIMessage(content='The subject of the first incident ticket is \"Production Line 2 sudden disruption due to unexpected shutdown of Benchmill.\"'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, failure modes and effects analysis (FMEA), maintenance practices, and more. I can provide guidance on troubleshooting production issues, optimizing manufacturing processes, improving product quality, and implementing preventive maintenance strategies. Additionally, I can assist with data analysis, trend identification, and decision-making based on historical data.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\")]\n",
      "Follow Up Input: tell me everything you know about\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:45:39 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:45:39 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  \n",
      "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "                ----\n",
      "                RCA Reference Guide\n",
      "Please refer the systematic sequence of steps listed below to methodically identify and address root\n",
      "cause issues contributing to various issues in production quality.\n",
      "1)Check the cooling mechanisms: Standard Airflow rate through cooling vents is >50 CFM.\n",
      "2)Evaluate ambient temperature around the motor: Standard Value is 20-25 degrees Celsius.\n",
      "3)Motor surface temperature during operation:  Standard Value is <60 degrees Celsius.\n",
      "If any of the values in steps 1,2 and 3 are found to breach standard values, the likely cause\n",
      "would be Inadequate cooling leading to motor overheating. If not, proceed to the next steps\n",
      "4)Check the rated power of the motor: Standard Value is 1.5 HP\n",
      "5)Assess the load on the motor: Standard Value is <80% of rated current.\n",
      "6)Check user workloads: Standard value is <80% of rated workload.\n",
      "If any of the values in steps 4,5 and 6 are found to breach standard values, the likely cause\n",
      "would be Overloading the motor beyond its capacity. If not, proceed to the next steps\n",
      "7)Voltage stability: standard value is +-5%\n",
      "8)Measure resistance in power wiring: Standard power wiring resistance is <0.1 ohms.\n",
      "9)Monitor fluctuations during operation: Standard Value is <2% variation.\n",
      "If any of the values in steps 7,8 and 9 are found to breach standard values, the likely cause\n",
      "would be Insufficient power supply causing fluctuations. If not, proceed to the next steps\n",
      "10)Review maintenance logs: Frequency standard is Monthly.\n",
      "11)Inspect for dust accumulation on motor surfaces: standard value is <1 mm thickness.\n",
      "12)Measure lubrication frequency : Standard Value is every 50 operating hours.\n",
      "If any of the values in steps 10,11 and 12 are found to breach standard values, the likely\n",
      "cause would be lack of regular maintenance, including\n",
      "cleaning and lubrication. If not, the root cause is a new one which is not listed.\n",
      "\n",
      "﻿Incident Ticket ID: INC0001198\n",
      "Issue Description: Variations in the quality and accuracy of produced machined parts on Assembly Line 7\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.51 ohms, Voltage fluctuation : 4%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "                ----\n",
      "                \n",
      "Human: Question:```What information do you have on different topics?```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:45:41 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mAs a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, failure modes and effects analysis (FMEA), maintenance practices, and more. I can provide guidance on troubleshooting production issues, optimizing manufacturing processes, implementing quality control measures, and improving overall efficiency and productivity in a manufacturing environment. If you have specific questions or need assistance with a particular topic, please let me know and I'll be happy to help.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 13:45:41 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mBased on the available information, it seems that the Copilot_Knowledge function provided more specific details about various topics, including planetary formation, tropical cyclone reports, temperature records, hurricane damage, and emissions of chlorofluorocarbons. On the other hand, the copilot_retriever function provided a general overview of the capabilities and knowledge related to manufacturing processes, quality control, root cause analysis, and maintenance practices.\n",
      "\n",
      "If you have a specific topic or question in mind, please let me know, and I'll do my best to provide more targeted information using the appropriate function.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the available information, it seems that the Copilot_Knowledge function provided more specific details about various topics, including planetary formation, tropical cyclone reports, temperature records, hurricane damage, and emissions of chlorofluorocarbons. On the other hand, the copilot_retriever function provided a general overview of the capabilities and knowledge related to manufacturing processes, quality control, root cause analysis, and maintenance practices.\\n\\nIf you have a specific topic or question in mind, please let me know, and I'll do my best to provide more targeted information using the appropriate function.\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS ANSWER SHOULD COME FROM THE GRAPH\n",
    "agent_executor.invoke({\"input\": \n",
    "                       querytemplate.format(question=\"tell me everything you know about\")}).get(\"output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a12c371e-03fc-4170-954e-544302fb0270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 14:37:40 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Copilot_Knowledge` with `Give me an RCA on ticket 1`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 14:37:42 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 14:37:42 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mI'm sorry, but I don't have any information on ticket 1.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 14:37:43 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `copilot_retriever` with `Give me an RCA on ticket 1`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='where and why did the production stoppage occurred'), AIMessage(content='Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a detailed analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, factors such as dust accumulation and lubrication frequency could also impact the overall performance of the assembly line. A thorough investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.'), HumanMessage(content='what was my previous question?'), AIMessage(content='I apologize, but I am an AI assistant and do not have the capability to remember previous questions. Is there anything else I can assist you with?'), HumanMessage(content='what did the study concluded'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='lifespan'), AIMessage(content=\"I'm sorry, but I don't have any information on lifespan.\"), HumanMessage(content='what did the study conclude?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='what is the capital of usa?'), AIMessage(content='The capital of the United States of America is Washington, D.C.'), HumanMessage(content=\"MATCH (hurricane:Disaster {name: 'Hurricane Ian'})-[:CAUSED]->(damage:Damage) RETURN damage.name\"), AIMessage(content=\"I'm sorry, but I don't have any information about the specific damage caused by Hurricane Ian.\"), HumanMessage(content='what damages caused hurricane ian?'), AIMessage(content=\"I'm sorry, but I don't have any information on the damages caused by Hurricane Ian.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='How many tickets are there?'), AIMessage(content='There are four incident tickets mentioned in the context.'), HumanMessage(content='How many tickets are there?'), AIMessage(content='There are four incident tickets mentioned in the context.'), HumanMessage(content='what is the first incident ticket about?'), AIMessage(content=\"I'm sorry, but I don't have any information about the first incident ticket.\"), HumanMessage(content='what is the first incident ticket about?'), AIMessage(content='The subject of the first incident ticket is \"Production Line 2 sudden disruption due to unexpected shutdown of Benchmill.\"'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='I have information about three incident tickets related to manufacturing issues on different assembly lines. Each ticket provides a description of the issue, parameter values, and maintenance logs. I can use this information to perform root cause analysis and identify the likely causes of the problems.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, FMEA, maintenance practices, and troubleshooting techniques. I can provide guidance on various topics such as production optimization, quality improvement, equipment maintenance, process efficiency, and problem-solving methodologies. If you have specific questions or need assistance with a particular topic, feel free to ask.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, FMEA, maintenance practices, and troubleshooting techniques. I can provide guidance on various topics such as production optimization, quality improvement, equipment maintenance, process efficiency, and problem-solving methodologies. If you have specific questions or need assistance with a particular topic, feel free to ask.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, failure modes and effects analysis (FMEA), maintenance practices, and more. I can provide guidance on troubleshooting production issues, optimizing manufacturing processes, improving product quality, and implementing preventive maintenance strategies. Additionally, I can assist with data analysis, trend identification, and decision-making based on historical data.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, FMEA, maintenance practices, and troubleshooting techniques. I can provide guidance on various topics such as production optimization, quality improvement, equipment maintenance, process efficiency, and problem-solving methodologies. If you have specific questions or need assistance with a particular topic, feel free to ask.'), HumanMessage(content='what is the first incident ticket about?'), AIMessage(content='The subject of the first incident ticket is \"Production Line 2 sudden disruption due to unexpected shutdown of Benchmill.\"'), HumanMessage(content='what did the study concluded?'), AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content='As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, failure modes and effects analysis (FMEA), maintenance practices, and more. I can provide guidance on troubleshooting production issues, optimizing manufacturing processes, improving product quality, and implementing preventive maintenance strategies. Additionally, I can assist with data analysis, trend identification, and decision-making based on historical data.'), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"I know several pieces of information about different topics. Here are some of them:\\n\\n1. There are theories of planetary formation that have been challenged.\\n2. There is a lowest-mass star hosting something, although it is not specified what it is hosting.\\n3. Five employees have published a tropical cyclone report.\\n4. In August, there was a new record high temperature reached.\\n5. The world's oceans have also reached a new record high temperature.\\n6. Hurricane Ian was upgraded from Category 4 to Category 5.\\n7. Hurricane Ian caused $112.9 billion worth of damage.\\n8. Hurricane Ian struck Florida.\\n9. The oxygen partial pressure is above 18%.\\n10. Emissions of chlorofluorocarbons have risen.\\n\\nPlease let me know if you would like more information on any specific topic.\"), HumanMessage(content='tell me everything you know about'), AIMessage(content=\"As a Manufacturing AI Assistant, I have access to a wide range of information related to manufacturing processes, quality control, root cause analysis, failure modes and effects analysis (FMEA), maintenance practices, and more. I can provide guidance on troubleshooting production issues, optimizing manufacturing processes, implementing quality control measures, and improving overall efficiency and productivity in a manufacturing environment. If you have specific questions or need assistance with a particular topic, please let me know and I'll be happy to help.\"), HumanMessage(content='Give me an RCA on ticket 1'), AIMessage(content=\"I'm sorry, but I don't have any information on ticket 1.\")]\n",
      "Follow Up Input: Give me an RCA on ticket 1\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 14:37:44 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 14:37:44 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  \n",
      "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "                ----\n",
      "                ﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0001198\n",
      "Issue Description: Variations in the quality and accuracy of produced machined parts on Assembly Line 7\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.51 ohms, Voltage fluctuation : 4%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "                ----\n",
      "                \n",
      "Human: Question:```What information do you have on ticket 1?```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 14:37:48 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mTicket 1 (INC0007654) is related to a sudden disruption on Production Line 2 during the morning shift. The disruption was caused by the unexpected shutdown of the Benchmill, which impacted the production schedule. The parameter values associated with this incident are as follows: Airflow Rate: 70 CFM, Ambient Temperature: 35 degrees Celsius, Motor surface temperature: 80 degrees Celsius, Motor rated power: 1.5 HP, Load on motor: 70%, User workload: 60%, Voltage stability: 5%, Power wiring resistance: 0.01 ohms, Voltage fluctuation: 1%. The maintenance logs show a monthly frequency, and there is a dust accumulation of 0.5 mm thickness.\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 14:37:49 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mBased on the information I found, ticket 1 (INC0007654) is related to a sudden disruption on Production Line 2 during the morning shift. The disruption was caused by the unexpected shutdown of the Benchmill, which impacted the production schedule. Here are some parameter values associated with this incident:\n",
      "\n",
      "- Airflow Rate: 70 CFM\n",
      "- Ambient Temperature: 35 degrees Celsius\n",
      "- Motor surface temperature: 80 degrees Celsius\n",
      "- Motor rated power: 1.5 HP\n",
      "- Load on motor: 70%\n",
      "- User workload: 60%\n",
      "- Voltage stability: 5%\n",
      "- Power wiring resistance: 0.01 ohms\n",
      "- Voltage fluctuation: 1%\n",
      "\n",
      "The maintenance logs show a monthly frequency, and there is a dust accumulation of 0.5 mm thickness.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based on the information I found, ticket 1 (INC0007654) is related to a sudden disruption on Production Line 2 during the morning shift. The disruption was caused by the unexpected shutdown of the Benchmill, which impacted the production schedule. Here are some parameter values associated with this incident:\\n\\n- Airflow Rate: 70 CFM\\n- Ambient Temperature: 35 degrees Celsius\\n- Motor surface temperature: 80 degrees Celsius\\n- Motor rated power: 1.5 HP\\n- Load on motor: 70%\\n- User workload: 60%\\n- Voltage stability: 5%\\n- Power wiring resistance: 0.01 ohms\\n- Voltage fluctuation: 1%\\n\\nThe maintenance logs show a monthly frequency, and there is a dust accumulation of 0.5 mm thickness.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#THIS ANSWER SHOULD COME FROM THE VECTOR\n",
    "agent_executor.invoke({\"input\": \n",
    "                       querytemplate.format(question=\"Give me an RCA on ticket 1\")}).get(\"output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994bd6f-b814-4eb6-8346-afdb77b59782",
   "metadata": {},
   "source": [
    "### LLama index graph test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e260804f-e0b7-4603-91fb-aacebee1f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass \n",
    "class RAG2:\n",
    "    memory_conversation  = None\n",
    "    def __init__(self,llm,graph,type=\"langchain\",embeddings=None,usememory=False):\n",
    "        self.llm = llm\n",
    "        self.graph = graph\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "        self.usememory = usememory\n",
    "\n",
    "    @staticmethod\n",
    "    def memory(type='langchain',memory_key=\"chat_history\",output_key='result',rmessages=True,readmem=None):\n",
    "        if readmem:\n",
    "            if type.lower()==\"langchain\":\n",
    "                from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "                memory = ConversationBufferMemory(\n",
    "                memory_key=memory_key, return_messages=rmessages)\n",
    "                #output_key=output_key)\n",
    "                \n",
    "                readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "\n",
    "                return memory\n",
    "        else: \n",
    "            return None\n",
    "    \n",
    "\n",
    "    def vector_retriever(self,store=\"chroma\",**kwargs):\n",
    "        texts = kwargs.get(\"texts\")\n",
    "        if store.lower()==\"chroma\":\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "\n",
    "            from langchain.chains import RetrievalQA\n",
    "            persist_directory = kwargs.get(\"persist_directory\",\"filesdb\")\n",
    "            search_kwargs = kwargs.get(\"search_kwargs\",{\"k\": 3})\n",
    "            from langchain.vectorstores import Chroma\n",
    "            \n",
    "            db = Chroma.from_documents(texts,self.embeddings,persist_directory= persist_directory)\n",
    "            vec_retriever = db.as_retriever(search_kwargs=search_kwargs)\n",
    "         \n",
    "            return vec_retriever  \n",
    "\n",
    "            \n",
    "        elif store.lower()==\"faiss\":\n",
    "            #to be implemented\n",
    "            from langchain_community.vectorstores import FAISS\n",
    "            from langchain_core.output_parsers import StrOutputParser\n",
    "            from langchain_core.prompts import ChatPromptTemplate\n",
    "            from langchain_core.runnables import RunnablePassthrough\n",
    "            from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "            if kwargs.get(\"text_embeddings\"):\n",
    "                text_embeddings =kwargs.get(\"text_embeddings\")\n",
    "                vectorstore = FAISS.from_embeddings(text_embeddings=list(zip(text_embeddings[0],text_embeddings[1])), embedding = self.embeddings)\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                return retriever\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                template = \"\"\"Answer the question based only on the following context:{context} \n",
    "                Question: {query}\"\"\"\n",
    "                prompt = ChatPromptTemplate.from_template(template)\n",
    "                retrieval_chain = ({\"context\": retriever, \"query\": RunnablePassthrough()} | prompt\n",
    "                                    | self.llm| StrOutputParser())\n",
    "                return retrieval_chain\n",
    "        \n",
    "    def build_chain(self,**kwargs):\n",
    "        #memory = self.memory() if (readmem := self.memory()) else None\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        \n",
    "        #PROMPT TEMPLATE        \n",
    "        from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate,PromptTemplate\n",
    "\n",
    "        \n",
    "        #####\n",
    "\n",
    "        memory = self.usememory\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "\n",
    "        if memory:\n",
    "            prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {question}\")\n",
    "        else:\n",
    "            prompt = PromptTemplate(input_variables=[\"question\"],template =\"{question}\")\n",
    "\n",
    "        self.memory_conversation = self.memory(readmem=memory) if not self.memory_conversation else self.memory_conversation\n",
    "\n",
    "        if kwargs.get(\"falkor\"):\n",
    "            from langchain.chains import FalkorDBQAChain\n",
    "            chain = FalkorDBQAChain.from_llm(llm=self.llm, graph=self.graph, verbose=verbose, memory=self.memory_conversation, **kwargs)\n",
    "        else:\n",
    "            if method.lower()==\"graph\":\n",
    "                #general chain for graphs\n",
    "                print(\"building graph chain\")\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                #Prompt template\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "\n",
    "                chain = GraphCypherQAChain.from_llm(llm=self.llm, prompt=prompt,graph=self.graph,validate_cypher = True, \n",
    "                                                    verbose=verbose,memory=self.memory_conversation, promptTemplate=qa_prompt,**kwargs)\n",
    "                chain.input_key = \"question\"\n",
    "\n",
    "            elif method.lower()==\"vector\":\n",
    "                print(\"Building vector chain \")\n",
    "                retriever = kwargs.get(\"vec_retriever\")\n",
    "                from langchain.chains import RetrievalQA\n",
    "                from langchain.chains import ConversationalRetrievalChain\n",
    "                \n",
    "                # template = (\"Combine the chat history and follow up question into \"\n",
    "                # \"a standalone question. Chat History: {chat_history}\"\n",
    "                # \"Follow up question: {query} \")\n",
    "                template = (\"Every time you respond you should start with This is Copilot your AI assistant\")\n",
    "                prompt = PromptTemplate.from_template(template)\n",
    "                \n",
    "\n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,return_source_documents=True,verbose=verbose)\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "                \n",
    "                chain = ConversationalRetrievalChain.from_llm(self.llm,retriever,memory=self.memory_conversation,get_chat_history=lambda h :h,\n",
    "                                                              verbose=verbose,\n",
    "                                                              combine_docs_chain_kwargs={\"prompt\": qa_prompt}) \n",
    "                \n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,\n",
    "                #                                    return_source_documents=True,verbose=verbose,memory=self.memory_conversation,prompt=prompt)\n",
    "\n",
    "        return chain\n",
    "\n",
    "\n",
    "    def get_retriever(self,**kwargs):\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        complexity = kwargs.get(\"complexity\",\"simple\")\n",
    "        \n",
    "        if method.lower()=='graph':\n",
    "            if self.type.lower()==\"langchain\":\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                if complexity.lower()=='simple':\n",
    "                    print(\"SIMPLE RAG LANGCHAIN\")\n",
    "                    if usememory := kwargs.get(\"usememory\") or self.memory_conversation:\n",
    "                        print(\"With Read memory\")\n",
    "                    \n",
    "                    chain = self.build_chain(**kwargs)\n",
    "                    return chain\n",
    "                \n",
    "                    \n",
    "            elif self.type.lower()==\"llama\":\n",
    "                if complexity.lower()=='simple':\n",
    "                    nl2graph = kwargs.get(\"nl2graph\",False)\n",
    "                    print(nl2graph)\n",
    "                    from llama_index.query_engine import RetrieverQueryEngine\n",
    "                    from llama_index.retrievers import KnowledgeGraphRAGRetriever                    \n",
    "                    graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=kwargs.get(\"storage_context\"),\n",
    "                    service_context=kwargs.get(\"service_context\"),\n",
    "                    llm=self.llm,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=nl2graph)\n",
    "                    query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))\n",
    "                    return query_engine\n",
    "\n",
    "                \n",
    "                elif complexity.lower()==\"custom\":\n",
    "                    if self.type.lower()==\"llama\":\n",
    "                        from llama_index import get_response_synthesizer\n",
    "                        from llama_index.query_engine import RetrieverQueryEngine\n",
    "                        # create custom retriever\n",
    "                        vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "                        kg_retriever = KGTableRetriever(index=kg_index, retriever_mode=\"keyword\", include_text=False)\n",
    "                        custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "                        # create response synthesizer\n",
    "                        response_synthesizer = get_response_synthesizer(service_context=service_context,response_mode=\"tree_summarize\")\n",
    "                        custom_query_engine = RetrieverQueryEngine(retriever=custom_retriever,response_synthesizer=response_synthesizer)\n",
    "                        return custom_query_engine\n",
    "                    elif self.type.lower()=='langchain':\n",
    "                        from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "                        simple = self.get_retriever(\"simple\",**kwargs)\n",
    "                        \n",
    "                        tools = [\n",
    "                            Tool(name= \"GraphRAG\",\n",
    "                                 func= simple.run,\n",
    "                                 description= \"Simple RAG from knowledge graph\"),\n",
    "                            #add more RAG tools\n",
    "                        ]\n",
    "        elif method.lower()==\"vector\":\n",
    "            print(\"Getting vector retriever\")\n",
    "            vec_retriever = self.vector_retriever(**kwargs)\n",
    "\n",
    "            chain = self.build_chain(vec_retriever=vec_retriever,**kwargs)\n",
    "            return chain\n",
    "\n",
    "\n",
    "\n",
    "    def response(self,query,complexity=\"simple\",**kwargs):\n",
    "        method = kwargs.get(\"method\",\"vector\")\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "        retriever = self.get_retriever(**kwargs)\n",
    "        try:\n",
    "            if method.lower()==\"vector\":\n",
    "                print(\"I AM USING VECTOR LETS GO\")\n",
    "                result =  retriever({\"question\": query}).get('answer')\n",
    "                \n",
    "            elif method.lower()==\"graph\":\n",
    "                if self.type.lower()=='langchain':\n",
    "                    print('using retriever({\"query\": query}).get(\"answer\")')\n",
    "                    #result = retriever({\"question\": query}).get('answer')#\n",
    "                    result = retriever.invoke({\"question\": query}).get('result') \n",
    "                    pickled_str = pickle.dumps(self.memory_conversation)\n",
    "                elif self.type.lower()=='llama':\n",
    "                    result = retriever.query(query)\n",
    "                    if 'Empty Response' in result.response:\n",
    "                        raise(\"Error empty result or no information found\")\n",
    "                    else:\n",
    "                        return result.response\n",
    "            \n",
    "            if not result or \"I'm sorry, but I don't have\" in result:\n",
    "                print('Not using RAG')\n",
    "                raise (\"Error empty result or no information found in the KG\")\n",
    "            elif \"I'm sorry, but I don't have\" in result:\n",
    "                raise(\"ERROR 404\")\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if self.type.lower()=='langchain':\n",
    "                from langchain.schema import HumanMessage\n",
    "                #message = HumanMessage(content=query)\n",
    "                from langchain.chains import ConversationChain\n",
    "                from langchain.prompts.prompt import PromptTemplate\n",
    "                if self.usememory:\n",
    "                    print(\"Normal LLM response with same memory\")\n",
    "                    prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {query}\")\n",
    "                    chat = ConversationChain(llm=self.llm,verbose=kwargs.get(\"verbose\",True),prompt=prompt,memory=self.memory_conversation ,input_key=\"question\")\n",
    "                    return chat.invoke({\"question\":query}).get(\"response\")\n",
    "                else:\n",
    "                    from langchain.schema import HumanMessage\n",
    "                    message = HumanMessage(content=query)\n",
    "                    return self.llm.invoke([message])\n",
    "\n",
    "            else:\n",
    "                #return response with llama index self.llm(response)\n",
    "                pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d65e16c-4c52-4307-abb0-565660ef9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getllm(openai=True, azure=False, mode=\"langchain\",**kwargs):\n",
    "    print(\"Note: to use the LLM use it as msg = HumanMessage(content= your query) \\n llm(messages = [msg]) \")\n",
    "    if openai:\n",
    "        temperature = kwargs.get(\"temperature\",0)\n",
    "        if azure:\n",
    "            print(\"Using Azure OpenAI\")\n",
    "            # from langchain.chat_models import AzureChatOpenAI\n",
    "            from langchain_openai import AzureChatOpenAI\n",
    "            auth = kwargs.get(\"auth\")\n",
    "            if auth.lower()==\"token\":\n",
    "                if mode.lower()==\"langchain\":\n",
    "                    params = {\"model\": kwargs.get(\"model\"),\n",
    "                        \"azure_deployment\": kwargs.get(\"dep_name\"),\n",
    "                        \"openai_api_version\": kwargs.get(\"api_version\"),\n",
    "                        \"temperature\": temperature,\n",
    "                        \"openai_api_key\": kwargs.get(\"api_key\"),\n",
    "                        \"azure_endpoint\": kwargs.get(\"api_base\"),\n",
    "                        \"max_tokens\":kwargs.get(\"max_tokens\",256)}\n",
    "                    llm = AzureChatOpenAI(**params)\n",
    "                elif mode.lower()==\"llama\":\n",
    "                    print(\"Llama's version\")\n",
    "                    #from llama_index.llms import AzureOpenAI\n",
    "                    from llama_index.llms.azure_openai import AzureOpenAI\n",
    "                    import openai\n",
    "                    openai.api_type = \"azure\"\n",
    "                    # openai.api_base =  kwargs.get(\"api_base\") #\"INSERT AZURE API BASE\"\n",
    "                    # openai.api_version = kwargs.get(\"api_version\",\"2023-05-15\")\n",
    "                    \n",
    "                    llm = AzureOpenAI(engine= kwargs.get(\"dep_name\"),temperature=0,model=kwargs.get(\"model\"),\n",
    "                                     azure_deployment =kwargs.get(\"dep_name\"),azure_endpoint = kwargs.get(\"api_base\"),\n",
    "                                     api_version = kwargs.get(\"api_version\",\"2023-05-15\"),api_key= kwargs.get(\"api_key\"))\n",
    "                return llm\n",
    "            elif auth.lower() == \"azure_ad\":\n",
    "                #from azure.identity import DefaultAzureCredential\n",
    "                #credential = DefaultAzureCredential()\n",
    "                #Set the API type to `azure_ad`\n",
    "                #os.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n",
    "               ##Set the API_KEY to the token from the Azure credential\n",
    "                #os.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token            elif auth.lower()==\"azure_ad_provider\":\n",
    "                llm = \"Not implemented\"\n",
    "            elif auth == \"azure ad_provider\":\n",
    "                #managed_identity:\n",
    "                #from azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential   \n",
    "                #credential = ChainedTokenCredential(\n",
    "                #ManagedIdentityCredential(),\n",
    "                #AzureCliCredential())\n",
    "                llm = \"not implemented\"\n",
    "        else:\n",
    "            from langchain_openai import OpenAI\n",
    "            api_base = kwargs.get(\"api_base_url\")\n",
    "            api_key =kwargs.get(\"api_key\",\"NULL\")\n",
    "            llm = OpenAI(\n",
    "                         base_url = api_base,\n",
    "                         openai_api_key = api_key,**kwargs)\n",
    "            return llm \n",
    "    else: #huggingface model\n",
    "        pass    \n",
    "\n",
    "\n",
    "\n",
    "def llm_cost(llm,message) -> \"Total cost in USD\":\n",
    "    from langchain.callbacks import get_openai_callback\n",
    "    with get_openai_callback() as cb:\n",
    "        llm([message])\n",
    "        cost=format(cb.total_cost, '.6f')\n",
    "        print(f\"Total Cost (USD): ${cost}\")\n",
    "    return cost\n",
    "\n",
    "\n",
    "\n",
    "    def get_embeddings(self,documents=None,lib='langchain'):\n",
    "#         if not documents:\n",
    "#             documents = pass\n",
    "        #huggingface or langchain\n",
    "        if lib=='langchain':\n",
    "            from langchain.embeddings import OpenAIEmbeddings\n",
    "            embeddings_model = OpenAIEmbeddings()\n",
    "            embeddings = embeddings_model.embed_documents(documents)\n",
    "            return emebeddings\n",
    "        else:\n",
    "            from langchain.embeddings import HuggingFaceEmbeddings\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=self.huggingembed)\n",
    "            return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a157a285-ee20-4dda-98cc-21029df6c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_graphdb(args,**kwargs):\n",
    "    graphdb = kwargs.get(\"graphdb\")\n",
    "    url,username,password,database = kwargs.get(\"url\"), kwargs.get(\"username\"), kwargs.get(\"password\"), kwargs.get(\"database\")\n",
    "    if args.lower()=='llama':\n",
    "        #from llama_index.storage.storage_context import StorageContext\n",
    "        from llama_index.core import StorageContext\n",
    "        print('Using LLama Index graph class')\n",
    "        if graphdb.lower()=='neo4j' or 'neo4j' in url:\n",
    "            from llama_index.graph_stores.neo4j import Neo4jGraphStore\n",
    "\n",
    "            graph_store = Neo4jGraphStore(\n",
    "                username=username,\n",
    "                password=password,\n",
    "                url=url,\n",
    "                database=database,\n",
    "            )\n",
    "            #build storage context\n",
    "            storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "            return graph_store, storage_context\n",
    "        elif graphdb.lower()=='nebula':\n",
    "            from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "            #TBD get varibales from kwargs\n",
    "            graph_store = NebulaGraphStore(\n",
    "            space_name=kwargs.get(\"space_name\"),\n",
    "            edge_types=kwargs.get(\"edge_types\"),\n",
    "            rel_prop_names=kwargs.get(\"rel_prop_names\"),\n",
    "            tags=kwargs.get(\"tags\"))\n",
    "            storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "            return graph_store, storage_context\n",
    "        \n",
    "        else:\n",
    "            raise(\"Error not implemented for that graph database\")\n",
    "    \n",
    "\n",
    "    elif args.lower()=='langchain':\n",
    "        print('Using langchain graph class')\n",
    "        if graphdb.lower()=='neo4j' or 'neo4j' in url:\n",
    "            from langchain.graphs import Neo4jGraph\n",
    "            graph= Neo4jGraph(\n",
    "               url=url,\n",
    "               username=username,\n",
    "               password=password\n",
    "            )\n",
    "    \n",
    "            return graph\n",
    "        elif graphdb.lower()==\"falkordb\":\n",
    "            print(\"Using falkordb\")\n",
    "            from langchain.graphs import FalkorDBGraph\n",
    "            graph = FalkorDBGraph(database = database,host=url,port=6379, username=username,password = pwd)\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise(\"Error not implemented for that graph database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749eb20-c10e-4c53-ab6b-e8add1cbb6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51c0c388-b1a0-4fb1-ae85-ce9bbce98394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to use the LLM use it as msg = HumanMessage(content= your query) \n",
      " llm(messages = [msg]) \n",
      "Using Azure OpenAI\n",
      "Note: to use the LLM use it as msg = HumanMessage(content= your query) \n",
      " llm(messages = [msg]) \n",
      "Using Azure OpenAI\n",
      "Llama's version\n"
     ]
    }
   ],
   "source": [
    "#loading \n",
    "llmazure =getllm(openai=True,azure =True,mode = \"langchain\", auth=\"token\",model=\"gpt-35-turbo-16k\",dep_name = \"chat-bot-trail\",            \n",
    "                  api_version= \"2023-12-01-preview\", api_key = \"1bbc7a5ceb0e4677888f22d1eb4c8617\",api_base = \"https://chat-bot-iot.openai.azure.com/\")#([message])\n",
    "#LLAMA'S VERSION\n",
    "llmazure2 =getllm(openai=True,azure =True,mode = \"llama\", auth=\"token\",model=\"gpt-35-turbo-16k\",dep_name = \"chat-bot-trail\",            \n",
    "                  api_version= \"2023-05-15\", api_key = \"1bbc7a5ceb0e4677888f22d1eb4c8617\",api_base = \"https://chat-bot-iot.openai.azure.com/\")#([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14ddb8fd-c907-4ea3-987a-eb7b1f293439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using langchain graph class\n",
      "Using LLama Index graph class\n"
     ]
    }
   ],
   "source": [
    "#INITIATE THE GRAPH STORE\n",
    "url = \"neo4j+s://b75e0bd8.databases.neo4j.io\"\n",
    "username= \"neo4j\"\n",
    "password = \"aziMmumvuVru-rgeKuO0zT7mEtTxu29Jgrmp6lFKv0w\"\n",
    "graph_store = get_graphdb(\"langchain\",graphdb=\"neo4j\",url=url,username=username,password=password)\n",
    "\n",
    "#GRAPH STORE LLAMA \n",
    "graph_store_llama, storage_context = get_graphdb(\"llama\",graphdb=\"neo4j\",url=url,username=username,password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eee31228-85f8-4707-86b2-94bde63dba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import KnowledgeGraphQueryEngine\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import KnowledgeGraphRAGRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45d267e1-325b-443f-a9d6-52ae97922a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag2 = RAG2(llmazure2,graph_store_llama,type=\"llama\")\n",
    "service_context= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9630d1e-31c2-4967-abed-1c5fc7761072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=storage_context,\n",
    "                    llm=llmazure2,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=True)\n",
    "#query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83e46cce-9a6f-4951-ad10-8d894982d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=storage_context,\n",
    "                    llm=llmazure2,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84e62cda-4fec-4eba-b306-ebe74ad79771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureOpenAI(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001D8297BA550>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000001D8B7719440>, completion_to_prompt=<function default_completion_to_prompt at 0x000001D8B779C360>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='gpt-35-turbo-16k', temperature=0.0, max_tokens=None, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='1bbc7a5ceb0e4677888f22d1eb4c8617', api_base='https://api.openai.com/v1', api_version='2023-05-15', engine='chat-bot-trail', azure_endpoint='https://chat-bot-iot.openai.azure.com/', azure_deployment='chat-bot-trail', use_azure_ad=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmazure2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e5e3d41d-7a75-409a-9917-80451f5bcbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    graph_rag_retriever,\n",
    "    llm = llmazure2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3a93430-eefd-47e5-8a82-acaa12288645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 12:17:25 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-20 12:17:25 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-20 12:17:26 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The study concluded that a moderate decrease in body temperature extends lifespan.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\n",
    "    \"what did the study concluded\",\n",
    ").response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3d69df8-f956-4e76-9bd9-0873061b10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag_retriever_with_nl2graphquery = KnowledgeGraphRAGRetriever(\n",
    "    storage_context=storage_context,\n",
    "    verbose=True,\n",
    "    with_nl2graphquery=True,\n",
    "    llm = llmazure2\n",
    ")\n",
    "\n",
    "query_engine_with_nl2graphquery = RetrieverQueryEngine.from_args(\n",
    "    graph_rag_retriever_with_nl2graphquery,\n",
    "    llm = llmazure2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e977b64-12bd-43d3-8982-9540894bc499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 12:17:26 [WARNING]: Error in retrieving from nl2graphquery: 'NoneType' object has no attribute 'kwargs'\n",
      "2024-02-20 12:17:26 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-20 12:17:27 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-20 12:17:27 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The study concluded that a moderate decrease in body temperature extends lifespan.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine_with_nl2graphquery.query(\n",
    "    \"what did the study concluded\",\n",
    ").response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "932060df-425a-46ce-abb3-32e7cf0e2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add it to langchain tools\n",
    "from llama_index.core.langchain_helpers.agents import (\n",
    "    IndexToolConfig,\n",
    "    LlamaIndexTool,\n",
    ")\n",
    "\n",
    "tool_config = IndexToolConfig(\n",
    "    query_engine=query_engine_with_nl2graphquery,\n",
    "    name=f\"KG Index\",\n",
    "    description=f\"useful for when you want to answer queries about neo4j\",\n",
    "    tool_kwargs={\"return_direct\": False},\n",
    ")\n",
    "\n",
    "tool = LlamaIndexTool.from_tool_config(tool_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f32b41b-e05d-410f-9c5b-11e686a6c78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 20:31:40 [WARNING]: Error in retrieving from nl2graphquery: 'NoneType' object has no attribute 'kwargs'\n",
      "2024-02-19 20:31:41 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-19 20:31:41 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-19 20:31:43 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The study is related to various subjects such as a moderate decrease in body temperature extending lifespan, neuro-molecular mechanics, disappearance, more than 50% of freshwater lakes, population declines, school curricula including certain topics, carbon taxation approaches, release of methane from Arctic glacial retreat, and the assembly of 43 diverse Y chromosomes.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.run(\"tell me all you know about a study\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa833d45-a18a-4ef1-9570-6c8e384bc66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa5b50-9096-4603-8efa-837e7b08f496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744d16e-332e-489a-97d2-440681c883e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6b1a6-966c-41f9-83b1-4230307c9d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b1105-67e5-4799-8f73-b73da416dbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d298b499-ac0c-4fc7-afac-e540346eef65",
   "metadata": {},
   "source": [
    "## Other test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3ff353c-dfd4-4f1f-8fd3-4b134442c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, create_structured_chat_agent\n",
    "\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "52c780fb-f2f8-4009-842a-0c2e50536173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llmazure.bind_tools(tool)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LlamaIndex\",\n",
    "        func=lambda q: query_engine_with_nl2graphquery.query(q),\n",
    "        description=\"useful for when you want to answer questions about neo4j kg.\",\n",
    "        return_direct=False,\n",
    "    ),\n",
    "]\n",
    "\n",
    "llm_with_tools= llmazure.bind_tools(tools)\n",
    "\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7378a32b-8678-40e3-a233-989f0a604ed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 20:34:50 [WARNING]: Error in retrieving from nl2graphquery: 'NoneType' object has no attribute 'kwargs'\n",
      "2024-02-19 20:34:50 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-19 20:34:51 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-19 20:34:51 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='The study concluded that a moderate decrease in body temperature extends lifespan.', source_nodes=[NodeWithScore(node=TextNode(id_='f884aad2-3f5f-4a25-b071-107af19f2732', embedding=None, metadata={'kg_rel_map': {'study': [['AFFIRMS', 'moderate decrease in body temperature extends lifespan'], ['SHOWS', 'neuro-molecular mechanics'], ['FINDS', 'disappearance'], ['REPORTS', 'more than 50% of freshwater lakes'], ['FOUND', '48% are experiencing population declines'], ['PROPOSES', 'school curricula start including'], ['SUGGESTS', 'carbon taxation approaches would be more effective and fairer'], ['INDICATES', 'release of methane from Arctic glacial retreat'], ['REPORTS_ASSEMBLY_OF', '43 diverse Y chromosomes']]}, 'kg_rel_text': [\"['AFFIRMS', 'moderate decrease in body temperature extends lifespan']\", \"['SHOWS', 'neuro-molecular mechanics']\", \"['FINDS', 'disappearance']\", \"['REPORTS', 'more than 50% of freshwater lakes']\", \"['FOUND', '48% are experiencing population declines']\", \"['PROPOSES', 'school curricula start including']\", \"['SUGGESTS', 'carbon taxation approaches would be more effective and fairer']\", \"['INDICATES', 'release of methane from Arctic glacial retreat']\", \"['REPORTS_ASSEMBLY_OF', '43 diverse Y chromosomes']\"], 'kg_schema': {'schema': \"Node properties are the following:\\nEntity {id: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Entity)-[:CHALLENGE]->(:Entity),(:Entity)-[:BE]->(:Entity),(:Entity)-[:PUBLISH]->(:Entity),(:Entity)-[:REACH]->(:Entity),(:Entity)-[:UPGRADES]->(:Entity),(:Entity)-[:STRIKES]->(:Entity),(:Entity)-[:CAUSED]->(:Entity),(:Entity)-[:IS_ABOVE]->(:Entity),(:Entity)-[:RISE]->(:Entity),(:Entity)-[:BANS]->(:Entity),(:Entity)-[:FOUND_TO_HOST]->(:Entity),(:Entity)-[:AFFIRMS]->(:Entity),(:Entity)-[:FOUND]->(:Entity),(:Entity)-[:SUGGESTS]->(:Entity),(:Entity)-[:REPORTS_ASSEMBLY_OF]->(:Entity),(:Entity)-[:FINDS]->(:Entity),(:Entity)-[:PROPOSES]->(:Entity),(:Entity)-[:SHOWS]->(:Entity),(:Entity)-[:REPORTS]->(:Entity),(:Entity)-[:INDICATES]->(:Entity),(:Entity)-[:OCCURS_AT]->(:Entity),(:Entity)-[:REACHES]->(:Entity),(:Entity)-[:SHOWN_TO_POTENTIALLY_BE]->(:Entity),(:Entity)-[:OF_GREY_HAIR]->(:Entity),(:Entity)-[:BECOMES]->(:Entity),(:Entity)-[:LAND_NEAR]->(:Entity),(:Entity)-[:BELIEVED_TO_EXIST]->(:Entity),(:Entity)-[:LOSING]->(:Entity),(:Entity)-[:MATURE_INTO]->(:Entity),(:Entity)-[:SHOW]->(:Entity),(:Entity)-[:LIST]->(:Entity),(:Entity)-[:RELEASE]->(:Entity),(:Entity)-[:USE]->(:Entity),(:Entity)-[:ANNOUNCED_IN]->(:Entity),(:Entity)-[:DISCOVERY_OF]->(:Entity),(:Entity)-[:ENTER]->(:Entity),(:Entity)-[:PASS]->(:Entity),(:Entity)-[:FOUND_TO_HAVE]->(:Entity),(:Entity)-[:CONTRIBUTING_TO]->(:Entity),(:Entity)-[:CAN_BE_TRANSFERRED_BETWEEN]->(:Entity),(:Entity)-[:TRANSFERRED_TO]->(:Entity),(:Entity)-[:COULD_RESULT_IN]->(:Entity),(:Entity)-[:TAKING]->(:Entity),(:Entity)-[:FILMED_ON]->(:Entity),(:Entity)-[:ARE_REPORTED]->(:Entity),(:Entity)-[:AGAINST]->(:Entity),(:Entity)-[:FOR_COMBINATION_WITH]->(:Entity),(:Entity)-[:PRESENT]->(:Entity),(:Entity)-[:REPORT]->(:Entity),(:Entity)-[:CONFIRM]->(:Entity),(:Entity)-[:MAP]->(:Entity),(:Entity)-[:VIEWED_TOGETHER]->(:Entity),(:Entity)-[:TO_DETECT]->(:Entity),(:Entity)-[:CENTER_OF]->(:Entity),(:Entity)-[:PUBLISHED]->(:Entity),(:Entity)-[:PUBLISHED_BY]->(:Entity),(:Entity)-[:SHOWN_TO_OUTPERFORM]->(:Entity),(:Entity)-[:PREVENT]->(:Entity),(:Entity)-[:OUTPERFORM]->(:Entity),(:Entity)-[:ARE]->(:Entity),(:Entity)-[:ADDRESS]->(:Entity),(:Entity)-[:ARE_INCLUDED_IN]->(:Entity),(:Entity)-[:DETECT]->(:Entity),(:Entity)-[:IN_A_SUSTAINABLE_FOOD_SYSTEM_ACHIEVABLE_WITH_FULL_GLOBAL_DEPLOYMENT_OF_DIVERSE_INTERVENTIONS]->(:Entity),(:Entity)-[:WILL_MAKE]->(:Entity),(:Entity)-[:BEGINS]->(:Entity),(:Entity)-[:DYING_HUMANS]->(:Entity),(:Entity)-[:ACHIEVE]->(:Entity),(:Entity)-[:TESTED]->(:Entity),(:Entity)-[:BOOSTS]->(:Entity),(:Entity)-[:SHARE_NOBEL_PRIZE_IN_PHYSIOLOGY_OR_MEDICINE]->(:Entity),(:Entity)-[:DEMONSTRATED]->(:Entity),(:Entity)-[:SPOTTED_IN]->(:Entity),(:Entity)-[:PROPOSE]->(:Entity),(:Entity)-[:FIND]->(:Entity),(:Entity)-[:DEMONSTRATE]->(:Entity),(:Entity)-[:TERMED]->(:Entity),(:Entity)-[:SHARE_NOBEL_PRIZE_IN_PHYSICS]->(:Entity),(:Entity)-[:ANNOUNCES]->(:Entity),(:Entity)-[:REPORT_STUDIES]->(:Entity),(:Entity)-[:FOUND_IN]->(:Entity),(:Entity)-[:EXTENDED_BY]->(:Entity),(:Entity)-[:DESCRIBED_AS]->(:Entity),(:Entity)-[:DECLARED_EXTINCT_BY]->(:Entity),(:Entity)-[:DECLARED_EXTINCT]->(:Entity),(:Entity)-[:IMPROVE]->(:Entity),(:Entity)-[:IDENTIFY]->(:Entity),(:Entity)-[:RECOMMEND]->(:Entity),(:Entity)-[:CAPTURES]->(:Entity),(:Entity)-[:SPOTTED]->(:Entity),(:Entity)-[:IDENTIFIES]->(:Entity),(:Entity)-[:IN]->(:Entity),(:Entity)-[:KEEPING_TRACK_OF]->(:Entity),(:Entity)-[:VIA]->(:Entity),(:Entity)-[:PRESENTED]->(:Entity),(:Entity)-[:BASED_ON]->(:Entity),(:Entity)-[:62_NEW_MOONS]->(:Entity),(:Entity)-[:REQUIRE]->(:Entity),(:Entity)-[:UNKNOWN_ANCIENT_HUMAN_HOMININ]->(:Entity),(:Entity)-[:FIRE]->(:Entity),(:Entity)-[:OCCULTS]->(:Entity),(:Entity)-[:DISCOVER]->(:Entity),(:Entity)-[:CLOSE]->(:Entity),(:Entity)-[:REPORTED]->(:Entity),(:Entity)-[:may practice]->(:Entity),(:Entity)-[:name]->(:Entity),(:Entity)-[:THEORISED]->(:Entity),(:Entity)-[:monitor]->(:Entity),(:Entity)-[:ensure]->(:Entity),(:Entity)-[:CONCLUDES]->(:Entity),(:Entity)-[:is]->(:Entity),(:Entity)-[:HAVE]->(:Entity),(:Entity)-[:check distribute]->(:Entity),(:Entity)-[:prescriptive authority]->(:Entity),(:Entity)-[:must complete]->(:Entity),(:Entity)-[:advise]->(:Entity),(:Entity)-[:act]->(:Entity),(:Entity)-[:LOST]->(:Entity),(:Entity)-[:produced]->(:Entity),(:Entity)-[:seem driving]->(:Entity),(:Entity)-[:currently taking]->(:Entity),(:Entity)-[:involves taking]->(:Entity),(:Entity)-[:HAS_OVERTAKEN]->(:Entity),(:Entity)-[:WERE]->(:Entity),(:Entity)-[:COSTS]->(:Entity),(:Entity)-[:CREATED]->(:Entity),(:Entity)-[:BURIED]->(:Entity),(:Entity)-[:USED]->(:Entity),(:Entity)-[:DEVELOPED]->(:Entity),(:Entity)-[:WATCH_AND_LEARN_FROM]->(:Entity),(:Entity)-[:RECREATE]->(:Entity),(:Entity)-[:STORE]->(:Entity),(:Entity)-[:is part of]->(:Entity),(:Entity)-[:has]->(:Entity),(:Entity)-[:INCLUDE]->(:Entity),(:Entity)-[:OF]->(:Entity),(:Entity)-[:managed]->(:Entity),(:Entity)-[:covered]->(:Entity),(:Entity)-[:indicated]->(:Entity),(:Entity)-[:have]->(:Entity),(:Entity)-[:comprise]->(:Entity),(:Entity)-[:cover]->(:Entity),(:Entity)-[:enable]->(:Entity),(:Entity)-[:negotiate]->(:Entity),(:Entity)-[:collect]->(:Entity),(:Entity)-[:make]->(:Entity),(:Entity)-[:disclose]->(:Entity),(:Entity)-[:resell]->(:Entity),(:Entity)-[:save]->(:Entity),(:Entity)-[:RESULT_IN]->(:Entity),(:Entity)-[:BOOST]->(:Entity),(:Entity)-[:EXTEND]->(:Entity),(:Entity)-[:PROVIDE]->(:Entity),(:Entity)-[:ENABLE]->(:Entity),(:Entity)-[:CREATE]->(:Entity),(:Entity)-[:IS_LAUNCHED]->(:Entity),(:Entity)-[:are often prohibited]->(:Entity),(:Entity)-[:required to complete]->(:Entity),(:Entity)-[:eligible to sit]->(:Entity),(:Entity)-[:may involve]->(:Entity),(:Entity)-[:IS_CONFIRMED]->(:Entity),(:Entity)-[:composed of]->(:Entity),(:Entity)-[:offer]->(:Entity),(:Entity)-[:involves]->(:Entity),(:Entity)-[:EXCEED]->(:Entity),(:Entity)-[:start]->(:Entity),(:Entity)-[:CONFIRMED_AS]->(:Entity),(:Entity)-[:also complete]->(:Entity),(:Entity)-[:THEORISE]->(:Entity),(:Entity)-[:is a]->(:Entity),(:Entity)-[:CALLED]->(:Entity),(:Entity)-[:HOST]->(:Entity),(:Entity)-[:experienced]->(:Entity),(:Entity)-[:impacting]->(:Entity),(:Entity)-[:Parameter Values]->(:Entity),(:Entity)-[:due to]->(:Entity),(:Entity)-[:IS]->(:Entity),(:Entity)-[:FOUNDED_IN]->(:Entity),(:Entity)-[:experienced sudden disruption]->(:Entity),(:Entity)-[:due unexpected shutdown]->(:Entity),(:Entity)-[:Excessive vibrations]->(:Entity),(:Entity)-[:is a publication by]->(:Entity),(:Entity)-[:is an important tool for measuring]->(:Entity),(:Entity)-[:improving level]->(:Entity),(:Entity)-[:If several QMPs applied]->(:Entity),(:Entity)-[:focuses on]->(:Entity),(:Entity)-[:change]->(:Entity),(:Entity)-[:is within]->(:Entity),(:Entity)-[:et al.]->(:Entity),(:Entity)-[:presents]->(:Entity),(:Entity)-[:terms]->(:Entity),(:Entity)-[:measured]->(:Entity),(:Entity)-[:comparing]->(:Entity),(:Entity)-[:determine]->(:Entity),(:Entity)-[:connection]->(:Entity),(:Entity)-[:pointing]->(:Entity),(:Entity)-[:caused]->(:Entity),(:Entity)-[:scientific studies]->(:Entity),(:Entity)-[:published]->(:Entity),(:Entity)-[:mainly focus]->(:Entity),(:Entity)-[:without]->(:Entity),(:Entity)-[:study]->(:Entity),(:Entity)-[:pointed]->(:Entity),(:Entity)-[:conditions]->(:Entity),(:Entity)-[:dealt]->(:Entity),(:Entity)-[:services sector]->(:Entity),(:Entity)-[:comprehensive current issue]->(:Entity),(:Entity)-[:is in]->(:Entity),(:Entity)-[:is considered]->(:Entity),(:Entity)-[:is studied by]->(:Entity),(:Entity)-[:implement]->(:Entity),(:Entity)-[:The H1 hypothesis]->(:Entity),(:Entity)-[:demonstrable large partly medium-sized enterprises]->(:Entity),(:Entity)-[:aims]->(:Entity),(:Entity)-[:used]->(:Entity),(:Entity)-[:shown]->(:Entity),(:Entity)-[:state]->(:Entity),(:Entity)-[:present]->(:Entity),(:Entity)-[:mention]->(:Entity),(:Entity)-[:reduces]->(:Entity),(:Entity)-[:increases]->(:Entity),(:Entity)-[:leads to]->(:Entity),(:Entity)-[:authored]->(:Entity),(:Entity)-[:dealt using]->(:Entity),(:Entity)-[:aim studies mainly]->(:Entity),(:Entity)-[:presentation conceptual models implementation]->(:Entity),(:Entity)-[:without identifying]->(:Entity),(:Entity)-[:use]->(:Entity),(:Entity)-[:focused improving]->(:Entity),(:Entity)-[:organization]->(:Entity),(:Entity)-[:managing improving performance]->(:Entity),(:Entity)-[:None]->(:Entity),(:Entity)-[:studied]->(:Entity),(:Entity)-[:focused attention importance]->(:Entity),(:Entity)-[:widely used in]->(:Entity),(:Entity)-[:aim studies highlight nature advantages]->(:Entity),(:Entity)-[:applied in]->(:Entity),(:Entity)-[:characterized as]->(:Entity),(:Entity)-[:described as]->(:Entity),(:Entity)-[:use Kaizen in]->(:Entity),(:Entity)-[:was issued by]->(:Entity),(:Entity)-[:was issued in]->(:Entity),(:Entity)-[:mentioned]->(:Entity),(:Entity)-[:The purpose of applying]->(:Entity),(:Entity)-[:create]->(:Entity),(:Entity)-[:implementation]->(:Entity),(:Entity)-[:higher satisfaction]->(:Entity),(:Entity)-[:selected techniques use]->(:Entity),(:Entity)-[:competitiveness]->(:Entity),(:Entity)-[:addressed]->(:Entity),(:Entity)-[:investigated]->(:Entity),(:Entity)-[:opposite]->(:Entity),(:Entity)-[:necessary]->(:Entity),(:Entity)-[:applied]->(:Entity),(:Entity)-[:chosen]->(:Entity),(:Entity)-[:identify]->(:Entity),(:Entity)-[:indicative parameter]->(:Entity),(:Entity)-[:creates]->(:Entity),(:Entity)-[:described]->(:Entity),(:Entity)-[:practical use QMPs]->(:Entity),(:Entity)-[:purpose]->(:Entity),(:Entity)-[:analysis]->(:Entity),(:Entity)-[:𝜒𝜒2]->(:Entity),(:Entity)-[:becomes]->(:Entity),(:Entity)-[:underlying]->(:Entity),(:Entity)-[:is used by]->(:Entity),(:Entity)-[:is used in]->(:Entity),(:Entity)-[:has a reliability of]->(:Entity),(:Entity)-[:has a 95 interval estimate lower limit of]->(:Entity),(:Entity)-[:has a 95 interval estimate upper limit of]->(:Entity),(:Entity)-[:is one of]->(:Entity),(:Entity)-[:is used for]->(:Entity),(:Entity)-[:confirmed]->(:Entity),(:Entity)-[:highest lowest value]->(:Entity),(:Entity)-[:high level technology]->(:Entity),(:Entity)-[:bring together]->(:Entity),(:Entity)-[:setting]->(:Entity),(:Entity)-[:use scale QMPs]->(:Entity),(:Entity)-[:partly technology performance industry]->(:Entity),(:Entity)-[:V alue-added indicator Slovak manufacturing enterprises]->(:Entity),(:Entity)-[:uses two approaches]->(:Entity),(:Entity)-[:divided two groups]->(:Entity),(:Entity)-[:predominance domestic capital]->(:Entity),(:Entity)-[:represented 166 enterprises]->(:Entity),(:Entity)-[:published by]->(:Entity),(:Entity)-[:is implemented in]->(:Entity),(:Entity)-[:likes to drive]->(:Entity),(:Entity)-[:plays]->(:Entity),(:Entity)-[:Practice]->(:Entity),(:Entity)-[:like]->(:Entity),(:Entity)-[:made]->(:Entity),(:Entity)-[:progress]->(:Entity),(:Entity)-[:employing]->(:Entity),(:Entity)-[:less]->(:Entity),(:Entity)-[:originated]->(:Entity),(:Entity)-[:involved]->(:Entity),(:Entity)-[:responsibility]->(:Entity),(:Entity)-[:MBNQA]->(:Entity),(:Entity)-[:used term]->(:Entity),(:Entity)-[:reported]->(:Entity),(:Entity)-[:carry responsibility]->(:Entity),(:Entity)-[:associated]->(:Entity),(:Entity)-[:requires]->(:Entity),(:Entity)-[:defines]->(:Entity),(:Entity)-[:deals]->(:Entity),(:Entity)-[:synonymous]->(:Entity),(:Entity)-[:provides]->(:Entity),(:Entity)-[:imagined]->(:Entity),(:Entity)-[:consumers]->(:Entity),(:Entity)-[:observed]->(:Entity),(:Entity)-[:allocates]->(:Entity),(:Entity)-[:revealed]->(:Entity),(:Entity)-[:amounting]->(:Entity),(:Entity)-[:also include]->(:Entity),(:Entity)-[:surfaces]->(:Entity),(:Entity)-[:Some Factors Consideration Developing Techniques Implementation Guidelines]->(:Entity),(:Entity)-[:identified]->(:Entity),(:Entity)-[:set-up]->(:Entity),(:Entity)-[:recommended]->(:Entity),(:Entity)-[:provided]->(:Entity),(:Entity)-[:achieve]->(:Entity),(:Entity)-[:covers]->(:Entity),(:Entity)-[:includes]->(:Entity),(:Entity)-[:able to compare]->(:Entity),(:Entity)-[:take forms]->(:Entity),(:Entity)-[:limited application]->(:Entity),(:Entity)-[:ranking choices]->(:Entity),(:Entity)-[:makes possible]->(:Entity),(:Entity)-[:relates]->(:Entity),(:Entity)-[:used survey]->(:Entity),(:Entity)-[:developed]->(:Entity),(:Entity)-[:designed]->(:Entity),(:Entity)-[:falls in category]->(:Entity),(:Entity)-[:ask]->(:Entity),(:Entity)-[:cause]->(:Entity),(:Entity)-[:categorized]->(:Entity),(:Entity)-[:impossible to verify]->(:Entity),(:Entity)-[:minimize response errors]->(:Entity),(:Entity)-[:described earlier]->(:Entity),(:Entity)-[:reduce magnitude low response rate]->(:Entity),(:Entity)-[:business performance]->(:Entity),(:Entity)-[:stressed]->(:Entity),(:Entity)-[:implemented]->(:Entity),(:Entity)-[:105 3.10.1]->(:Entity),(:Entity)-[:classified]->(:Entity),(:Entity)-[:similar]->(:Entity),(:Entity)-[:follows]->(:Entity),(:Entity)-[:1980 survey]->(:Entity),(:Entity)-[:generally]->(:Entity),(:Entity)-[:largely]->(:Entity),(:Entity)-[:fiercely competitive]->(:Entity),(:Entity)-[:expected]->(:Entity),(:Entity)-[:sought]->(:Entity),(:Entity)-[:fresh start firms]->(:Entity),(:Entity)-[:is the number 1 producer of]->(:Entity),(:Entity)-[:one]->(:Entity),(:Entity)-[:even]->(:Entity),(:Entity)-[:two]->(:Entity),(:Entity)-[:employs]->(:Entity),(:Entity)-[:192]->(:Entity),(:Entity)-[:number]->(:Entity),(:Entity)-[:would set]->(:Entity),(:Entity)-[:initially tried]->(:Entity),(:Entity)-[:techniques]->(:Entity),(:Entity)-[:five]->(:Entity),(:Entity)-[:followed]->(:Entity),(:Entity)-[:would agreement]->(:Entity),(:Entity)-[:employee suggestion]->(:Entity),(:Entity)-[:exert]->(:Entity),(:Entity)-[:relatively new]->(:Entity),(:Entity)-[:known effects]->(:Entity),(:Entity)-[:installed later]->(:Entity),(:Entity)-[:focuses]->(:Entity),(:Entity)-[:collective efforts]->(:Entity),(:Entity)-[:unsuitable]->(:Entity),(:Entity)-[:among techniques]->(:Entity),(:Entity)-[:sophisticated machines]->(:Entity),(:Entity)-[:least aggressively pursued]->(:Entity),(:Entity)-[:recent years]->(:Entity),(:Entity)-[:seems agree]->(:Entity),(:Entity)-[:reported US industries]->(:Entity),(:Entity)-[:80 Installation Including 1996]->(:Entity),(:Entity)-[:Statistical Method]->(:Entity),(:Entity)-[:Customer Survey]->(:Entity),(:Entity)-[:Waste Elimination]->(:Entity),(:Entity)-[:TPM]->(:Entity),(:Entity)-[:Employee Suggestion]->(:Entity),(:Entity)-[:examined]->(:Entity),(:Entity)-[:ends examining]->(:Entity),(:Entity)-[:improve quality position]->(:Entity),(:Entity)-[:has volume number]->(:Entity),(:Entity)-[:has issue number]->(:Entity),(:Entity)-[:has page number]->(:Entity),(:Entity)-[:has author]->(:Entity),(:Entity)-[:has publication date]->(:Entity),(:Entity)-[:'Total quality management world-class quality']->(:Entity),(:Entity)-[:‘Improving odds f TQM’]->(:Entity),(:Entity)-[:is the publication year of]->(:Entity),(:Entity)-[:is an article in]->(:Entity),(:Entity)-[:is on pages]->(:Entity),(:Entity)-[:is written by]->(:Entity),(:Entity)-[:Proceedings of 4th International Conference Factory 2000- Advanced Factory Automation]->(:Entity),(:Entity)-[:published in]->(:Entity),(:Entity)-[:published on]->(:Entity),(:Entity)-[:is pp 6-15]->(:Entity),(:Entity)-[:is pp 19-31]->(:Entity),(:Entity)-[:is pp 81-85]->(:Entity),(:Entity)-[:is pp 97-100]->(:Entity),(:Entity)-[:titled]->(:Entity),(:Entity)-[:mentioned in]->(:Entity),(:Entity)-[:mentioned on page]->(:Entity),(:Entity)-[:mentioned by]->(:Entity),(:Entity)-[:wrote]->(:Entity),(:Entity)-[:TOM Magazine]->(:Entity),(:Entity)-[:pp]->(:Entity),(:Entity)-[:Does TQM impact]->(:Entity),(:Entity)-[:has page range]->(:Entity),(:Entity)-[:Feb. 1992]->(:Entity),(:Entity)-[:B.G Dale 1991]->(:Entity),(:Entity)-[:D. Nadler 1992]->(:Entity),(:Entity)-[:1995]->(:Entity),(:Entity)-[:D. McCabe 1996]->(:Entity),(:Entity)-[:is published by]->(:Entity),(:Entity)-[:is published in]->(:Entity),(:Entity)-[:The cost quality]->(:Entity),(:Entity)-[:Production Inventory Management Journal]->(:Entity),(:Entity)-[:The new manufacturing challenge]->(:Entity),(:Entity)-[:The 10 commandments f quality]->(:Entity),(:Entity)-[:published for]->(:Entity),(:Entity)-[:published as]->(:Entity),(:Entity)-[:published on pages]->(:Entity),(:Entity)-[:conducted]->(:Entity),(:Entity)-[:discussed]->(:Entity),(:Entity)-[:is the publisher of]->(:Entity),(:Entity)-[:is an]->(:Entity),(:Entity)-[:is about]->(:Entity),(:Entity)-[:is on]->(:Entity),(:Entity)-[:was published in]->(:Entity),(:Entity)-[:has pp]->(:Entity),(:Entity)-[:Need radical innovation continuous improvement]->(:Entity),(:Entity)-[:practices]->(:Entity),(:Entity)-[:Maintenace logs shows]->(:Entity),(:Entity)-[:is the year]->(:Entity),(:Entity)-[:improving level of]->(:Entity),(:Entity)-[:is the author of]->(:Entity),(:Entity)-[:focus]->(:Entity),(:Entity)-[:take account]->(:Entity),(:Entity)-[:manufactured in]->(:Entity),(:Entity)-[:emphasize]->(:Entity),(:Entity)-[:improve]->(:Entity),(:Entity)-[:eliminate]->(:Entity),(:Entity)-[:acquire]->(:Entity),(:Entity)-[:mean]->(:Entity),(:Entity)-[:presentation conceptual models]->(:Entity),(:Entity)-[:gain competitive advantage]->(:Entity),(:Entity)-[:dynamic tool]->(:Entity),(:Entity)-[:Airflow Rate]->(:Entity),(:Entity)-[:Power wiring resistance]->(:Entity),(:Entity)-[:Voltage fluctuation]->(:Entity),(:Entity)-[:Ambient Temperature]->(:Entity),(:Entity)-[:Motor surface temperature]->(:Entity),(:Entity)-[:Motor rated power]->(:Entity),(:Entity)-[:Load motor]->(:Entity),(:Entity)-[:User workload]->(:Entity),(:Entity)-[:Voltage stability]->(:Entity),(:Entity)-[:Issue Description]->(:Entity),(:Entity)-[:lead]->(:Entity),(:Entity)-[:gain competitive advantage in terms of]->(:Entity),(:Entity)-[:The purpose research]->(:Entity),(:Entity)-[:may affect]->(:Entity),(:Entity)-[:achieved performance]->(:Entity),(:Entity)-[:highest level officer]->(:Entity),(:Entity)-[:work]->(:Entity),(:Entity)-[:regarded]->(:Entity),(:Entity)-[:said]->(:Entity),(:Entity)-[:gain]->(:Entity),(:Entity)-[:every effort must made]->(:Entity),(:Entity)-[:requiring references documents files]->(:Entity),(:Entity)-[:fall in four factors]->(:Entity),(:Entity)-[:principles]->(:Entity),(:Entity)-[:response errors]->(:Entity),(:Entity)-[:low response rates]->(:Entity),(:Entity)-[:non-response]->(:Entity),(:Entity)-[:grouped questions together]->(:Entity),(:Entity)-[:still low]->(:Entity),(:Entity)-[:Installation Including]->(:Entity),(:Entity)-[:For Each Quality Initiative]->(:Entity),(:Entity)-[:Number cases]->(:Entity),(:Entity)-[:Cellular Layout]->(:Entity),(:Entity)-[:Improving odds of]->(:Entity),(:Entity)-[:shows monthly frequency]->(:Entity),(:Entity)-[:experienced excessive vibrations]->(:Entity),(:Entity)-[:lead to]->(:Entity),(:Entity)-[:services sector analysed]->(:Entity),(:Entity)-[:comprehensive link]->(:Entity),(:Entity)-[:example study]->(:Entity),(:Entity)-[:total quality]->(:Entity),(:Entity)-[:aim studies highlight]->(:Entity)\"}}, excluded_embed_metadata_keys=['kg_rel_map', 'kg_rel_text', 'kg_schema'], excluded_llm_metadata_keys=['kg_rel_map', 'kg_rel_text', 'kg_schema'], relationships={}, text=\"The following are knowledge sequence in max depth 2 in the form of directed graph like:\\n`subject -[predicate]->, object, <-[predicate_next_hop]-, object_next_hop ...` extracted based on key entities as subject:\\n['AFFIRMS', 'moderate decrease in body temperature extends lifespan']\\n['SHOWS', 'neuro-molecular mechanics']\\n['FINDS', 'disappearance']\\n['REPORTS', 'more than 50% of freshwater lakes']\\n['FOUND', '48% are experiencing population declines']\\n['PROPOSES', 'school curricula start including']\\n['SUGGESTS', 'carbon taxation approaches would be more effective and fairer']\\n['INDICATES', 'release of methane from Arctic glacial retreat']\\n['REPORTS_ASSEMBLY_OF', '43 diverse Y chromosomes']\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None)], metadata={'f884aad2-3f5f-4a25-b071-107af19f2732': {'kg_rel_map': {'study': [['AFFIRMS', 'moderate decrease in body temperature extends lifespan'], ['SHOWS', 'neuro-molecular mechanics'], ['FINDS', 'disappearance'], ['REPORTS', 'more than 50% of freshwater lakes'], ['FOUND', '48% are experiencing population declines'], ['PROPOSES', 'school curricula start including'], ['SUGGESTS', 'carbon taxation approaches would be more effective and fairer'], ['INDICATES', 'release of methane from Arctic glacial retreat'], ['REPORTS_ASSEMBLY_OF', '43 diverse Y chromosomes']]}, 'kg_rel_text': [\"['AFFIRMS', 'moderate decrease in body temperature extends lifespan']\", \"['SHOWS', 'neuro-molecular mechanics']\", \"['FINDS', 'disappearance']\", \"['REPORTS', 'more than 50% of freshwater lakes']\", \"['FOUND', '48% are experiencing population declines']\", \"['PROPOSES', 'school curricula start including']\", \"['SUGGESTS', 'carbon taxation approaches would be more effective and fairer']\", \"['INDICATES', 'release of methane from Arctic glacial retreat']\", \"['REPORTS_ASSEMBLY_OF', '43 diverse Y chromosomes']\"], 'kg_schema': {'schema': \"Node properties are the following:\\nEntity {id: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Entity)-[:CHALLENGE]->(:Entity),(:Entity)-[:BE]->(:Entity),(:Entity)-[:PUBLISH]->(:Entity),(:Entity)-[:REACH]->(:Entity),(:Entity)-[:UPGRADES]->(:Entity),(:Entity)-[:STRIKES]->(:Entity),(:Entity)-[:CAUSED]->(:Entity),(:Entity)-[:IS_ABOVE]->(:Entity),(:Entity)-[:RISE]->(:Entity),(:Entity)-[:BANS]->(:Entity),(:Entity)-[:FOUND_TO_HOST]->(:Entity),(:Entity)-[:AFFIRMS]->(:Entity),(:Entity)-[:FOUND]->(:Entity),(:Entity)-[:SUGGESTS]->(:Entity),(:Entity)-[:REPORTS_ASSEMBLY_OF]->(:Entity),(:Entity)-[:FINDS]->(:Entity),(:Entity)-[:PROPOSES]->(:Entity),(:Entity)-[:SHOWS]->(:Entity),(:Entity)-[:REPORTS]->(:Entity),(:Entity)-[:INDICATES]->(:Entity),(:Entity)-[:OCCURS_AT]->(:Entity),(:Entity)-[:REACHES]->(:Entity),(:Entity)-[:SHOWN_TO_POTENTIALLY_BE]->(:Entity),(:Entity)-[:OF_GREY_HAIR]->(:Entity),(:Entity)-[:BECOMES]->(:Entity),(:Entity)-[:LAND_NEAR]->(:Entity),(:Entity)-[:BELIEVED_TO_EXIST]->(:Entity),(:Entity)-[:LOSING]->(:Entity),(:Entity)-[:MATURE_INTO]->(:Entity),(:Entity)-[:SHOW]->(:Entity),(:Entity)-[:LIST]->(:Entity),(:Entity)-[:RELEASE]->(:Entity),(:Entity)-[:USE]->(:Entity),(:Entity)-[:ANNOUNCED_IN]->(:Entity),(:Entity)-[:DISCOVERY_OF]->(:Entity),(:Entity)-[:ENTER]->(:Entity),(:Entity)-[:PASS]->(:Entity),(:Entity)-[:FOUND_TO_HAVE]->(:Entity),(:Entity)-[:CONTRIBUTING_TO]->(:Entity),(:Entity)-[:CAN_BE_TRANSFERRED_BETWEEN]->(:Entity),(:Entity)-[:TRANSFERRED_TO]->(:Entity),(:Entity)-[:COULD_RESULT_IN]->(:Entity),(:Entity)-[:TAKING]->(:Entity),(:Entity)-[:FILMED_ON]->(:Entity),(:Entity)-[:ARE_REPORTED]->(:Entity),(:Entity)-[:AGAINST]->(:Entity),(:Entity)-[:FOR_COMBINATION_WITH]->(:Entity),(:Entity)-[:PRESENT]->(:Entity),(:Entity)-[:REPORT]->(:Entity),(:Entity)-[:CONFIRM]->(:Entity),(:Entity)-[:MAP]->(:Entity),(:Entity)-[:VIEWED_TOGETHER]->(:Entity),(:Entity)-[:TO_DETECT]->(:Entity),(:Entity)-[:CENTER_OF]->(:Entity),(:Entity)-[:PUBLISHED]->(:Entity),(:Entity)-[:PUBLISHED_BY]->(:Entity),(:Entity)-[:SHOWN_TO_OUTPERFORM]->(:Entity),(:Entity)-[:PREVENT]->(:Entity),(:Entity)-[:OUTPERFORM]->(:Entity),(:Entity)-[:ARE]->(:Entity),(:Entity)-[:ADDRESS]->(:Entity),(:Entity)-[:ARE_INCLUDED_IN]->(:Entity),(:Entity)-[:DETECT]->(:Entity),(:Entity)-[:IN_A_SUSTAINABLE_FOOD_SYSTEM_ACHIEVABLE_WITH_FULL_GLOBAL_DEPLOYMENT_OF_DIVERSE_INTERVENTIONS]->(:Entity),(:Entity)-[:WILL_MAKE]->(:Entity),(:Entity)-[:BEGINS]->(:Entity),(:Entity)-[:DYING_HUMANS]->(:Entity),(:Entity)-[:ACHIEVE]->(:Entity),(:Entity)-[:TESTED]->(:Entity),(:Entity)-[:BOOSTS]->(:Entity),(:Entity)-[:SHARE_NOBEL_PRIZE_IN_PHYSIOLOGY_OR_MEDICINE]->(:Entity),(:Entity)-[:DEMONSTRATED]->(:Entity),(:Entity)-[:SPOTTED_IN]->(:Entity),(:Entity)-[:PROPOSE]->(:Entity),(:Entity)-[:FIND]->(:Entity),(:Entity)-[:DEMONSTRATE]->(:Entity),(:Entity)-[:TERMED]->(:Entity),(:Entity)-[:SHARE_NOBEL_PRIZE_IN_PHYSICS]->(:Entity),(:Entity)-[:ANNOUNCES]->(:Entity),(:Entity)-[:REPORT_STUDIES]->(:Entity),(:Entity)-[:FOUND_IN]->(:Entity),(:Entity)-[:EXTENDED_BY]->(:Entity),(:Entity)-[:DESCRIBED_AS]->(:Entity),(:Entity)-[:DECLARED_EXTINCT_BY]->(:Entity),(:Entity)-[:DECLARED_EXTINCT]->(:Entity),(:Entity)-[:IMPROVE]->(:Entity),(:Entity)-[:IDENTIFY]->(:Entity),(:Entity)-[:RECOMMEND]->(:Entity),(:Entity)-[:CAPTURES]->(:Entity),(:Entity)-[:SPOTTED]->(:Entity),(:Entity)-[:IDENTIFIES]->(:Entity),(:Entity)-[:IN]->(:Entity),(:Entity)-[:KEEPING_TRACK_OF]->(:Entity),(:Entity)-[:VIA]->(:Entity),(:Entity)-[:PRESENTED]->(:Entity),(:Entity)-[:BASED_ON]->(:Entity),(:Entity)-[:62_NEW_MOONS]->(:Entity),(:Entity)-[:REQUIRE]->(:Entity),(:Entity)-[:UNKNOWN_ANCIENT_HUMAN_HOMININ]->(:Entity),(:Entity)-[:FIRE]->(:Entity),(:Entity)-[:OCCULTS]->(:Entity),(:Entity)-[:DISCOVER]->(:Entity),(:Entity)-[:CLOSE]->(:Entity),(:Entity)-[:REPORTED]->(:Entity),(:Entity)-[:may practice]->(:Entity),(:Entity)-[:name]->(:Entity),(:Entity)-[:THEORISED]->(:Entity),(:Entity)-[:monitor]->(:Entity),(:Entity)-[:ensure]->(:Entity),(:Entity)-[:CONCLUDES]->(:Entity),(:Entity)-[:is]->(:Entity),(:Entity)-[:HAVE]->(:Entity),(:Entity)-[:check distribute]->(:Entity),(:Entity)-[:prescriptive authority]->(:Entity),(:Entity)-[:must complete]->(:Entity),(:Entity)-[:advise]->(:Entity),(:Entity)-[:act]->(:Entity),(:Entity)-[:LOST]->(:Entity),(:Entity)-[:produced]->(:Entity),(:Entity)-[:seem driving]->(:Entity),(:Entity)-[:currently taking]->(:Entity),(:Entity)-[:involves taking]->(:Entity),(:Entity)-[:HAS_OVERTAKEN]->(:Entity),(:Entity)-[:WERE]->(:Entity),(:Entity)-[:COSTS]->(:Entity),(:Entity)-[:CREATED]->(:Entity),(:Entity)-[:BURIED]->(:Entity),(:Entity)-[:USED]->(:Entity),(:Entity)-[:DEVELOPED]->(:Entity),(:Entity)-[:WATCH_AND_LEARN_FROM]->(:Entity),(:Entity)-[:RECREATE]->(:Entity),(:Entity)-[:STORE]->(:Entity),(:Entity)-[:is part of]->(:Entity),(:Entity)-[:has]->(:Entity),(:Entity)-[:INCLUDE]->(:Entity),(:Entity)-[:OF]->(:Entity),(:Entity)-[:managed]->(:Entity),(:Entity)-[:covered]->(:Entity),(:Entity)-[:indicated]->(:Entity),(:Entity)-[:have]->(:Entity),(:Entity)-[:comprise]->(:Entity),(:Entity)-[:cover]->(:Entity),(:Entity)-[:enable]->(:Entity),(:Entity)-[:negotiate]->(:Entity),(:Entity)-[:collect]->(:Entity),(:Entity)-[:make]->(:Entity),(:Entity)-[:disclose]->(:Entity),(:Entity)-[:resell]->(:Entity),(:Entity)-[:save]->(:Entity),(:Entity)-[:RESULT_IN]->(:Entity),(:Entity)-[:BOOST]->(:Entity),(:Entity)-[:EXTEND]->(:Entity),(:Entity)-[:PROVIDE]->(:Entity),(:Entity)-[:ENABLE]->(:Entity),(:Entity)-[:CREATE]->(:Entity),(:Entity)-[:IS_LAUNCHED]->(:Entity),(:Entity)-[:are often prohibited]->(:Entity),(:Entity)-[:required to complete]->(:Entity),(:Entity)-[:eligible to sit]->(:Entity),(:Entity)-[:may involve]->(:Entity),(:Entity)-[:IS_CONFIRMED]->(:Entity),(:Entity)-[:composed of]->(:Entity),(:Entity)-[:offer]->(:Entity),(:Entity)-[:involves]->(:Entity),(:Entity)-[:EXCEED]->(:Entity),(:Entity)-[:start]->(:Entity),(:Entity)-[:CONFIRMED_AS]->(:Entity),(:Entity)-[:also complete]->(:Entity),(:Entity)-[:THEORISE]->(:Entity),(:Entity)-[:is a]->(:Entity),(:Entity)-[:CALLED]->(:Entity),(:Entity)-[:HOST]->(:Entity),(:Entity)-[:experienced]->(:Entity),(:Entity)-[:impacting]->(:Entity),(:Entity)-[:Parameter Values]->(:Entity),(:Entity)-[:due to]->(:Entity),(:Entity)-[:IS]->(:Entity),(:Entity)-[:FOUNDED_IN]->(:Entity),(:Entity)-[:experienced sudden disruption]->(:Entity),(:Entity)-[:due unexpected shutdown]->(:Entity),(:Entity)-[:Excessive vibrations]->(:Entity),(:Entity)-[:is a publication by]->(:Entity),(:Entity)-[:is an important tool for measuring]->(:Entity),(:Entity)-[:improving level]->(:Entity),(:Entity)-[:If several QMPs applied]->(:Entity),(:Entity)-[:focuses on]->(:Entity),(:Entity)-[:change]->(:Entity),(:Entity)-[:is within]->(:Entity),(:Entity)-[:et al.]->(:Entity),(:Entity)-[:presents]->(:Entity),(:Entity)-[:terms]->(:Entity),(:Entity)-[:measured]->(:Entity),(:Entity)-[:comparing]->(:Entity),(:Entity)-[:determine]->(:Entity),(:Entity)-[:connection]->(:Entity),(:Entity)-[:pointing]->(:Entity),(:Entity)-[:caused]->(:Entity),(:Entity)-[:scientific studies]->(:Entity),(:Entity)-[:published]->(:Entity),(:Entity)-[:mainly focus]->(:Entity),(:Entity)-[:without]->(:Entity),(:Entity)-[:study]->(:Entity),(:Entity)-[:pointed]->(:Entity),(:Entity)-[:conditions]->(:Entity),(:Entity)-[:dealt]->(:Entity),(:Entity)-[:services sector]->(:Entity),(:Entity)-[:comprehensive current issue]->(:Entity),(:Entity)-[:is in]->(:Entity),(:Entity)-[:is considered]->(:Entity),(:Entity)-[:is studied by]->(:Entity),(:Entity)-[:implement]->(:Entity),(:Entity)-[:The H1 hypothesis]->(:Entity),(:Entity)-[:demonstrable large partly medium-sized enterprises]->(:Entity),(:Entity)-[:aims]->(:Entity),(:Entity)-[:used]->(:Entity),(:Entity)-[:shown]->(:Entity),(:Entity)-[:state]->(:Entity),(:Entity)-[:present]->(:Entity),(:Entity)-[:mention]->(:Entity),(:Entity)-[:reduces]->(:Entity),(:Entity)-[:increases]->(:Entity),(:Entity)-[:leads to]->(:Entity),(:Entity)-[:authored]->(:Entity),(:Entity)-[:dealt using]->(:Entity),(:Entity)-[:aim studies mainly]->(:Entity),(:Entity)-[:presentation conceptual models implementation]->(:Entity),(:Entity)-[:without identifying]->(:Entity),(:Entity)-[:use]->(:Entity),(:Entity)-[:focused improving]->(:Entity),(:Entity)-[:organization]->(:Entity),(:Entity)-[:managing improving performance]->(:Entity),(:Entity)-[:None]->(:Entity),(:Entity)-[:studied]->(:Entity),(:Entity)-[:focused attention importance]->(:Entity),(:Entity)-[:widely used in]->(:Entity),(:Entity)-[:aim studies highlight nature advantages]->(:Entity),(:Entity)-[:applied in]->(:Entity),(:Entity)-[:characterized as]->(:Entity),(:Entity)-[:described as]->(:Entity),(:Entity)-[:use Kaizen in]->(:Entity),(:Entity)-[:was issued by]->(:Entity),(:Entity)-[:was issued in]->(:Entity),(:Entity)-[:mentioned]->(:Entity),(:Entity)-[:The purpose of applying]->(:Entity),(:Entity)-[:create]->(:Entity),(:Entity)-[:implementation]->(:Entity),(:Entity)-[:higher satisfaction]->(:Entity),(:Entity)-[:selected techniques use]->(:Entity),(:Entity)-[:competitiveness]->(:Entity),(:Entity)-[:addressed]->(:Entity),(:Entity)-[:investigated]->(:Entity),(:Entity)-[:opposite]->(:Entity),(:Entity)-[:necessary]->(:Entity),(:Entity)-[:applied]->(:Entity),(:Entity)-[:chosen]->(:Entity),(:Entity)-[:identify]->(:Entity),(:Entity)-[:indicative parameter]->(:Entity),(:Entity)-[:creates]->(:Entity),(:Entity)-[:described]->(:Entity),(:Entity)-[:practical use QMPs]->(:Entity),(:Entity)-[:purpose]->(:Entity),(:Entity)-[:analysis]->(:Entity),(:Entity)-[:𝜒𝜒2]->(:Entity),(:Entity)-[:becomes]->(:Entity),(:Entity)-[:underlying]->(:Entity),(:Entity)-[:is used by]->(:Entity),(:Entity)-[:is used in]->(:Entity),(:Entity)-[:has a reliability of]->(:Entity),(:Entity)-[:has a 95 interval estimate lower limit of]->(:Entity),(:Entity)-[:has a 95 interval estimate upper limit of]->(:Entity),(:Entity)-[:is one of]->(:Entity),(:Entity)-[:is used for]->(:Entity),(:Entity)-[:confirmed]->(:Entity),(:Entity)-[:highest lowest value]->(:Entity),(:Entity)-[:high level technology]->(:Entity),(:Entity)-[:bring together]->(:Entity),(:Entity)-[:setting]->(:Entity),(:Entity)-[:use scale QMPs]->(:Entity),(:Entity)-[:partly technology performance industry]->(:Entity),(:Entity)-[:V alue-added indicator Slovak manufacturing enterprises]->(:Entity),(:Entity)-[:uses two approaches]->(:Entity),(:Entity)-[:divided two groups]->(:Entity),(:Entity)-[:predominance domestic capital]->(:Entity),(:Entity)-[:represented 166 enterprises]->(:Entity),(:Entity)-[:published by]->(:Entity),(:Entity)-[:is implemented in]->(:Entity),(:Entity)-[:likes to drive]->(:Entity),(:Entity)-[:plays]->(:Entity),(:Entity)-[:Practice]->(:Entity),(:Entity)-[:like]->(:Entity),(:Entity)-[:made]->(:Entity),(:Entity)-[:progress]->(:Entity),(:Entity)-[:employing]->(:Entity),(:Entity)-[:less]->(:Entity),(:Entity)-[:originated]->(:Entity),(:Entity)-[:involved]->(:Entity),(:Entity)-[:responsibility]->(:Entity),(:Entity)-[:MBNQA]->(:Entity),(:Entity)-[:used term]->(:Entity),(:Entity)-[:reported]->(:Entity),(:Entity)-[:carry responsibility]->(:Entity),(:Entity)-[:associated]->(:Entity),(:Entity)-[:requires]->(:Entity),(:Entity)-[:defines]->(:Entity),(:Entity)-[:deals]->(:Entity),(:Entity)-[:synonymous]->(:Entity),(:Entity)-[:provides]->(:Entity),(:Entity)-[:imagined]->(:Entity),(:Entity)-[:consumers]->(:Entity),(:Entity)-[:observed]->(:Entity),(:Entity)-[:allocates]->(:Entity),(:Entity)-[:revealed]->(:Entity),(:Entity)-[:amounting]->(:Entity),(:Entity)-[:also include]->(:Entity),(:Entity)-[:surfaces]->(:Entity),(:Entity)-[:Some Factors Consideration Developing Techniques Implementation Guidelines]->(:Entity),(:Entity)-[:identified]->(:Entity),(:Entity)-[:set-up]->(:Entity),(:Entity)-[:recommended]->(:Entity),(:Entity)-[:provided]->(:Entity),(:Entity)-[:achieve]->(:Entity),(:Entity)-[:covers]->(:Entity),(:Entity)-[:includes]->(:Entity),(:Entity)-[:able to compare]->(:Entity),(:Entity)-[:take forms]->(:Entity),(:Entity)-[:limited application]->(:Entity),(:Entity)-[:ranking choices]->(:Entity),(:Entity)-[:makes possible]->(:Entity),(:Entity)-[:relates]->(:Entity),(:Entity)-[:used survey]->(:Entity),(:Entity)-[:developed]->(:Entity),(:Entity)-[:designed]->(:Entity),(:Entity)-[:falls in category]->(:Entity),(:Entity)-[:ask]->(:Entity),(:Entity)-[:cause]->(:Entity),(:Entity)-[:categorized]->(:Entity),(:Entity)-[:impossible to verify]->(:Entity),(:Entity)-[:minimize response errors]->(:Entity),(:Entity)-[:described earlier]->(:Entity),(:Entity)-[:reduce magnitude low response rate]->(:Entity),(:Entity)-[:business performance]->(:Entity),(:Entity)-[:stressed]->(:Entity),(:Entity)-[:implemented]->(:Entity),(:Entity)-[:105 3.10.1]->(:Entity),(:Entity)-[:classified]->(:Entity),(:Entity)-[:similar]->(:Entity),(:Entity)-[:follows]->(:Entity),(:Entity)-[:1980 survey]->(:Entity),(:Entity)-[:generally]->(:Entity),(:Entity)-[:largely]->(:Entity),(:Entity)-[:fiercely competitive]->(:Entity),(:Entity)-[:expected]->(:Entity),(:Entity)-[:sought]->(:Entity),(:Entity)-[:fresh start firms]->(:Entity),(:Entity)-[:is the number 1 producer of]->(:Entity),(:Entity)-[:one]->(:Entity),(:Entity)-[:even]->(:Entity),(:Entity)-[:two]->(:Entity),(:Entity)-[:employs]->(:Entity),(:Entity)-[:192]->(:Entity),(:Entity)-[:number]->(:Entity),(:Entity)-[:would set]->(:Entity),(:Entity)-[:initially tried]->(:Entity),(:Entity)-[:techniques]->(:Entity),(:Entity)-[:five]->(:Entity),(:Entity)-[:followed]->(:Entity),(:Entity)-[:would agreement]->(:Entity),(:Entity)-[:employee suggestion]->(:Entity),(:Entity)-[:exert]->(:Entity),(:Entity)-[:relatively new]->(:Entity),(:Entity)-[:known effects]->(:Entity),(:Entity)-[:installed later]->(:Entity),(:Entity)-[:focuses]->(:Entity),(:Entity)-[:collective efforts]->(:Entity),(:Entity)-[:unsuitable]->(:Entity),(:Entity)-[:among techniques]->(:Entity),(:Entity)-[:sophisticated machines]->(:Entity),(:Entity)-[:least aggressively pursued]->(:Entity),(:Entity)-[:recent years]->(:Entity),(:Entity)-[:seems agree]->(:Entity),(:Entity)-[:reported US industries]->(:Entity),(:Entity)-[:80 Installation Including 1996]->(:Entity),(:Entity)-[:Statistical Method]->(:Entity),(:Entity)-[:Customer Survey]->(:Entity),(:Entity)-[:Waste Elimination]->(:Entity),(:Entity)-[:TPM]->(:Entity),(:Entity)-[:Employee Suggestion]->(:Entity),(:Entity)-[:examined]->(:Entity),(:Entity)-[:ends examining]->(:Entity),(:Entity)-[:improve quality position]->(:Entity),(:Entity)-[:has volume number]->(:Entity),(:Entity)-[:has issue number]->(:Entity),(:Entity)-[:has page number]->(:Entity),(:Entity)-[:has author]->(:Entity),(:Entity)-[:has publication date]->(:Entity),(:Entity)-[:'Total quality management world-class quality']->(:Entity),(:Entity)-[:‘Improving odds f TQM’]->(:Entity),(:Entity)-[:is the publication year of]->(:Entity),(:Entity)-[:is an article in]->(:Entity),(:Entity)-[:is on pages]->(:Entity),(:Entity)-[:is written by]->(:Entity),(:Entity)-[:Proceedings of 4th International Conference Factory 2000- Advanced Factory Automation]->(:Entity),(:Entity)-[:published in]->(:Entity),(:Entity)-[:published on]->(:Entity),(:Entity)-[:is pp 6-15]->(:Entity),(:Entity)-[:is pp 19-31]->(:Entity),(:Entity)-[:is pp 81-85]->(:Entity),(:Entity)-[:is pp 97-100]->(:Entity),(:Entity)-[:titled]->(:Entity),(:Entity)-[:mentioned in]->(:Entity),(:Entity)-[:mentioned on page]->(:Entity),(:Entity)-[:mentioned by]->(:Entity),(:Entity)-[:wrote]->(:Entity),(:Entity)-[:TOM Magazine]->(:Entity),(:Entity)-[:pp]->(:Entity),(:Entity)-[:Does TQM impact]->(:Entity),(:Entity)-[:has page range]->(:Entity),(:Entity)-[:Feb. 1992]->(:Entity),(:Entity)-[:B.G Dale 1991]->(:Entity),(:Entity)-[:D. Nadler 1992]->(:Entity),(:Entity)-[:1995]->(:Entity),(:Entity)-[:D. McCabe 1996]->(:Entity),(:Entity)-[:is published by]->(:Entity),(:Entity)-[:is published in]->(:Entity),(:Entity)-[:The cost quality]->(:Entity),(:Entity)-[:Production Inventory Management Journal]->(:Entity),(:Entity)-[:The new manufacturing challenge]->(:Entity),(:Entity)-[:The 10 commandments f quality]->(:Entity),(:Entity)-[:published for]->(:Entity),(:Entity)-[:published as]->(:Entity),(:Entity)-[:published on pages]->(:Entity),(:Entity)-[:conducted]->(:Entity),(:Entity)-[:discussed]->(:Entity),(:Entity)-[:is the publisher of]->(:Entity),(:Entity)-[:is an]->(:Entity),(:Entity)-[:is about]->(:Entity),(:Entity)-[:is on]->(:Entity),(:Entity)-[:was published in]->(:Entity),(:Entity)-[:has pp]->(:Entity),(:Entity)-[:Need radical innovation continuous improvement]->(:Entity),(:Entity)-[:practices]->(:Entity),(:Entity)-[:Maintenace logs shows]->(:Entity),(:Entity)-[:is the year]->(:Entity),(:Entity)-[:improving level of]->(:Entity),(:Entity)-[:is the author of]->(:Entity),(:Entity)-[:focus]->(:Entity),(:Entity)-[:take account]->(:Entity),(:Entity)-[:manufactured in]->(:Entity),(:Entity)-[:emphasize]->(:Entity),(:Entity)-[:improve]->(:Entity),(:Entity)-[:eliminate]->(:Entity),(:Entity)-[:acquire]->(:Entity),(:Entity)-[:mean]->(:Entity),(:Entity)-[:presentation conceptual models]->(:Entity),(:Entity)-[:gain competitive advantage]->(:Entity),(:Entity)-[:dynamic tool]->(:Entity),(:Entity)-[:Airflow Rate]->(:Entity),(:Entity)-[:Power wiring resistance]->(:Entity),(:Entity)-[:Voltage fluctuation]->(:Entity),(:Entity)-[:Ambient Temperature]->(:Entity),(:Entity)-[:Motor surface temperature]->(:Entity),(:Entity)-[:Motor rated power]->(:Entity),(:Entity)-[:Load motor]->(:Entity),(:Entity)-[:User workload]->(:Entity),(:Entity)-[:Voltage stability]->(:Entity),(:Entity)-[:Issue Description]->(:Entity),(:Entity)-[:lead]->(:Entity),(:Entity)-[:gain competitive advantage in terms of]->(:Entity),(:Entity)-[:The purpose research]->(:Entity),(:Entity)-[:may affect]->(:Entity),(:Entity)-[:achieved performance]->(:Entity),(:Entity)-[:highest level officer]->(:Entity),(:Entity)-[:work]->(:Entity),(:Entity)-[:regarded]->(:Entity),(:Entity)-[:said]->(:Entity),(:Entity)-[:gain]->(:Entity),(:Entity)-[:every effort must made]->(:Entity),(:Entity)-[:requiring references documents files]->(:Entity),(:Entity)-[:fall in four factors]->(:Entity),(:Entity)-[:principles]->(:Entity),(:Entity)-[:response errors]->(:Entity),(:Entity)-[:low response rates]->(:Entity),(:Entity)-[:non-response]->(:Entity),(:Entity)-[:grouped questions together]->(:Entity),(:Entity)-[:still low]->(:Entity),(:Entity)-[:Installation Including]->(:Entity),(:Entity)-[:For Each Quality Initiative]->(:Entity),(:Entity)-[:Number cases]->(:Entity),(:Entity)-[:Cellular Layout]->(:Entity),(:Entity)-[:Improving odds of]->(:Entity),(:Entity)-[:shows monthly frequency]->(:Entity),(:Entity)-[:experienced excessive vibrations]->(:Entity),(:Entity)-[:lead to]->(:Entity),(:Entity)-[:services sector analysed]->(:Entity),(:Entity)-[:comprehensive link]->(:Entity),(:Entity)-[:example study]->(:Entity),(:Entity)-[:total quality]->(:Entity),(:Entity)-[:aim studies highlight]->(:Entity)\"}}})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[0].run(\"what did the study concluded?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "41d37981-5ee0-4b42-baae-b28495943933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 20:37:45 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI'm sorry, but I need more information about the study you are referring to in order to provide an accurate answer. Could you please provide more details or context about the study?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model gpt-35-turbo-16k. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversational_retrieval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_conversational_retrieval_agent\n\u001b[0;32m      3\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m create_conversational_retrieval_agent(llm\u001b[38;5;241m=\u001b[39mllmazure, tools\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      4\u001b[0m     tools[\u001b[38;5;241m0\u001b[39m]], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m agent_executor({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat did the study concluded about the lifespan?\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\chains\\base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    361\u001b[0m }\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    364\u001b[0m     inputs,\n\u001b[0;32m    365\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m    366\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[0;32m    367\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[0;32m    368\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\chains\\base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    163\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m--> 164\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    165\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    166\u001b[0m )\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n\u001b[0;32m    168\u001b[0m     final_outputs[RUN_KEY] \u001b[38;5;241m=\u001b[39m RunInfo(run_id\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mrun_id)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\chains\\base.py:440\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[1;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_outputs(outputs)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msave_context(inputs, outputs)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\agents\\openai_functions_agent\\agent_token_buffer_memory.py:62\u001b[0m, in \u001b[0;36mAgentTokenBufferMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Prune buffer if it exceeds max token limit\u001b[39;00m\n\u001b[0;32m     61\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39mmessages\n\u001b[1;32m---> 62\u001b[0m curr_buffer_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mget_num_tokens_from_messages(buffer)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_limit:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m curr_buffer_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_limit:\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:594\u001b[0m, in \u001b[0;36mChatOpenAI.get_num_tokens_from_messages\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    592\u001b[0m     tokens_per_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    595\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    596\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    597\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    598\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for information on how messages are converted to tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    599\u001b[0m     )\n\u001b[0;32m    600\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    601\u001b[0m messages_dict \u001b[38;5;241m=\u001b[39m [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model gpt-35-turbo-16k. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "from langchain.agents.agent_toolkits.conversational_retrieval.openai_functions import create_conversational_retrieval_agent\n",
    "\n",
    "agent_executor = create_conversational_retrieval_agent(llm=llmazure, tools=[\n",
    "    tools[0]], verbose=True)\n",
    "agent_executor({\"input\": \"what did the study concluded about the lifespan?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5bcdd7ae-e994-4868-8769-8675411bf489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 20:35:00 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have enough information to answer your question. Could you please provide more context or specify which study you are referring to?\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"what did the study concluded?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea4b0dab-5b23-401e-8699-aeb25e2f2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful manufacturing AI Assistant, you specialize in Quality department but have expertise in general manufacturing\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "    \n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b97d154-e2d9-4e90-b991-840cdc6d2cc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A single string input was passed in, but this chain expects multiple inputs (set()). When a chain expects multiple inputs, please call it by passing in a dictionary, eg `chain({'foo': 1, 'bar': 2})`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agent_executor(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\chains\\base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    361\u001b[0m }\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    364\u001b[0m     inputs,\n\u001b[0;32m    365\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m    366\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[0;32m    367\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[0;32m    368\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\chains\\base.py:138\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m include_run_info \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_run_info\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    136\u001b[0m return_only_outputs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_only_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 138\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_inputs(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    139\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m CallbackManager\u001b[38;5;241m.\u001b[39mconfigure(\n\u001b[0;32m    140\u001b[0m     callbacks,\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[0;32m    147\u001b[0m )\n\u001b[0;32m    148\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\chains\\base.py:465\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    463\u001b[0m         _input_keys \u001b[38;5;241m=\u001b[39m _input_keys\u001b[38;5;241m.\u001b[39mdifference(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mmemory_variables)\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_input_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 465\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    466\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA single string input was passed in, but this chain expects \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    467\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple inputs (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_input_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). When a chain expects \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple inputs, please call it by passing in a dictionary, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meg `chain(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfoo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: 1, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: 2})`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    470\u001b[0m         )\n\u001b[0;32m    471\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mlist\u001b[39m(_input_keys)[\u001b[38;5;241m0\u001b[39m]: inputs}\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: A single string input was passed in, but this chain expects multiple inputs (set()). When a chain expects multiple inputs, please call it by passing in a dictionary, eg `chain({'foo': 1, 'bar': 2})`"
     ]
    }
   ],
   "source": [
    "agent_executor(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5dd06-4151-41a3-ab4c-67accf75cec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
