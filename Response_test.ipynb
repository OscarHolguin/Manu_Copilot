{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c3d1ee1-3372-4984-979b-b76705b5928b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (0.10.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting llama_index\n",
      "  Downloading llama_index-0.10.6-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: llama-index-llms-azure-openai in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: llama-index-graph-stores-neo4j in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: llama-index-embeddings-azure-openai in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.10.5)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.2)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.3)\n",
      "Requirement already satisfied: azure-identity<2.0.0,>=1.15.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-llms-azure-openai) (1.15.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-llms-azure-openai) (0.24.1)\n",
      "Requirement already satisfied: neo4j<6.0.0,>=5.16.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-graph-stores-neo4j) (5.16.0)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.23.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.30.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (42.0.2)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.24.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.26.0)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.1.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (0.5.14)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (2023.12.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.5.8)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.26.3)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.11.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (2.1.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (0.5.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (4.12.3)\n",
      "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (0.0.2)\n",
      "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (1.23.22)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (4.0.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from neo4j<6.0.0,>=5.16.0->llama-index-graph-stores-neo4j) (2023.3.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (0.17.3)\n",
      "Requirement already satisfied: idna in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.3.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-core<2.0.0,>=1.23.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (2.5)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.16.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx->llama-index-llms-azure-openai) (0.14.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx->llama-index-llms-azure-openai) (3.5.0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2.0.0,>=1.24.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.8.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (23.2)\n",
      "Requirement already satisfied: portalocker<3,>=1.6 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.8.2)\n",
      "Requirement already satisfied: click in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.5.3)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.22 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (1.23.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama_index) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.0->llama_index) (3.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama_index) (2023.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.21)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from portalocker<3,>=1.6->msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (305.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.14.6)\n",
      "Downloading llama_index-0.10.6-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: llama_index\n",
      "  Attempting uninstall: llama_index\n",
      "    Found existing installation: llama-index 0.10.5\n",
      "    Uninstalling llama-index-0.10.5:\n",
      "      Successfully uninstalled llama-index-0.10.5\n",
      "Successfully installed llama_index-0.10.6\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade llama_index llama-index-llms-azure-openai llama-index-graph-stores-neo4j llama-index-embeddings-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "823ef567-bc8f-4e2b-92c7-9c5b7c389191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilitiesPOC import *\n",
    "\n",
    "from auths import openaisecret, embeddings_secret, adls_secret\n",
    "\n",
    "storage_account =  '{\"storage_account_key\": \"\",\"storage_container\": \"manufacturing-copilot\",\"storage_account\": \"euwdsrg03rsta07dls01\"}'\n",
    "path = \"Documents/Embeddings/\"\n",
    "\n",
    "\n",
    "\n",
    "llmconfig= '{\"mode\": \"langchain\", \"auth\": \"token\", \"model\": \"gpt-35-turbo-16k\", \"deployment_name\": \"mxdrca\",\"api_version\": \"2023-05-15\",\"api_key_secret\":\"\",\"api_endpoint\": \"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "embedconfig= '{\"embedmode\": \"langchain\",\"model\":\"text-embedding-ada-002\",\"deployment_name\":\"azure_embedding\",\"api_key_secret\":\"\",\"embeddingurl\":\"https://usedoai0efaoa03.openai.azure.com/\"}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16c8ade-2780-4e66-ba5d-288c0c652ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to use the LLM use it as msg = HumanMessage(content= your query) \n",
      " llm(messages = [msg]) \n",
      "Using Azure OpenAI\n",
      "Using Azure OpenAI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 19:51:35 [INFO]: Request URL: 'https://euwdsrg03rsta07dls01.dfs.core.windows.net/manufacturing-copilot?resource=REDACTED&directory=REDACTED&recursive=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Accept': 'application/json'\n",
      "    'User-Agent': 'azsdk-python-storage-dfs/12.14.0 Python/3.11.0 (Windows-10-10.0.22621-SP0)'\n",
      "    'x-ms-date': 'REDACTED'\n",
      "    'x-ms-client-request-id': 'f5cf4acd-cf9a-11ee-b52b-141333031892'\n",
      "    'Authorization': 'REDACTED'\n",
      "No body was attached to the request\n",
      "2024-02-19 19:51:36 [INFO]: Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json;charset=utf-8'\n",
      "    'Server': 'Windows-Azure-HDFS/1.0 Microsoft-HTTPAPI/2.0'\n",
      "    'x-ms-request-id': '1c9505ee-c01f-0008-74a7-632b06000000'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'x-ms-client-request-id': 'f5cf4acd-cf9a-11ee-b52b-141333031892'\n",
      "    'Date': 'Tue, 20 Feb 2024 02:51:34 GMT'\n",
      "2024-02-19 19:51:36 [INFO]: Request URL: 'https://euwdsrg03rsta07dls01.blob.core.windows.net/manufacturing-copilot/Documents/Embeddings/embeddings_12022024.pkl'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-range': 'REDACTED'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Accept': 'application/xml'\n",
      "    'User-Agent': 'azsdk-python-storage-dfs/12.14.0 Python/3.11.0 (Windows-10-10.0.22621-SP0)'\n",
      "    'x-ms-date': 'REDACTED'\n",
      "    'x-ms-client-request-id': 'f66494c5-cf9a-11ee-8144-141333031892'\n",
      "    'Authorization': 'REDACTED'\n",
      "No body was attached to the request\n",
      "2024-02-19 19:51:36 [INFO]: Response status: 206\n",
      "Response headers:\n",
      "    'Content-Length': '73944'\n",
      "    'Content-Type': 'application/octet-stream'\n",
      "    'Content-Range': 'REDACTED'\n",
      "    'Last-Modified': 'Mon, 12 Feb 2024 11:04:17 GMT'\n",
      "    'Accept-Ranges': 'REDACTED'\n",
      "    'ETag': '\"0x8DC2BBA5B6060C1\"'\n",
      "    'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'\n",
      "    'x-ms-request-id': '9636b1c0-c01e-0055-80a7-632182000000'\n",
      "    'x-ms-client-request-id': 'f66494c5-cf9a-11ee-8144-141333031892'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'x-ms-resource-type': 'REDACTED'\n",
      "    'x-ms-creation-time': 'REDACTED'\n",
      "    'x-ms-blob-content-md5': 'REDACTED'\n",
      "    'x-ms-lease-status': 'REDACTED'\n",
      "    'x-ms-lease-state': 'REDACTED'\n",
      "    'x-ms-blob-type': 'REDACTED'\n",
      "    'Content-Disposition': 'REDACTED'\n",
      "    'x-ms-server-encrypted': 'REDACTED'\n",
      "    'x-ms-owner': 'REDACTED'\n",
      "    'x-ms-group': 'REDACTED'\n",
      "    'x-ms-permissions': 'REDACTED'\n",
      "    'x-ms-acl': 'REDACTED'\n",
      "    'Date': 'Tue, 20 Feb 2024 02:51:35 GMT'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using langchain graph class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 19:51:38 [INFO]: Loading faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vector retriever\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 19:51:38 [INFO]: Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "2024-02-19 19:51:38 [INFO]: Loading faiss.\n",
      "2024-02-19 19:51:39 [INFO]: Successfully loaded faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vector chain \n",
      "I AM USING VECTOR LETS GO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\o_hol\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "2024-02-19 19:51:42 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  \n",
      "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "                ----\n",
      "                ﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0001198\n",
      "Issue Description: Variations in the quality and accuracy of produced machined parts on Assembly Line 7\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.51 ohms, Voltage fluctuation : 4%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "                ----\n",
      "                \n",
      "Human: Question:```where and why did the production stoppage occurred```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 19:51:46 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a thorough analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, the presence of dust accumulation and the frequency of maintenance logs could also be contributing factors. A detailed investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.\n",
      "Getting vector retriever\n",
      "Building vector chain \n",
      "I AM USING VECTOR LETS GO\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='where and why did the production stoppage occurred'), AIMessage(content='Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a thorough analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, the presence of dust accumulation and the frequency of maintenance logs could also be contributing factors. A detailed investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.')]\n",
      "Follow Up Input: what was my previous question?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 19:51:46 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-19 19:51:46 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  \n",
      "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "                ----\n",
      "                ﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "RCA Reference Guide\n",
      "Please refer the systematic sequence of steps listed below to methodically identify and address root\n",
      "cause issues contributing to various issues in production quality.\n",
      "1)Check the cooling mechanisms: Standard Airflow rate through cooling vents is >50 CFM.\n",
      "2)Evaluate ambient temperature around the motor: Standard Value is 20-25 degrees Celsius.\n",
      "3)Motor surface temperature during operation:  Standard Value is <60 degrees Celsius.\n",
      "If any of the values in steps 1,2 and 3 are found to breach standard values, the likely cause\n",
      "would be Inadequate cooling leading to motor overheating. If not, proceed to the next steps\n",
      "4)Check the rated power of the motor: Standard Value is 1.5 HP\n",
      "5)Assess the load on the motor: Standard Value is <80% of rated current.\n",
      "6)Check user workloads: Standard value is <80% of rated workload.\n",
      "If any of the values in steps 4,5 and 6 are found to breach standard values, the likely cause\n",
      "would be Overloading the motor beyond its capacity. If not, proceed to the next steps\n",
      "7)Voltage stability: standard value is +-5%\n",
      "8)Measure resistance in power wiring: Standard power wiring resistance is <0.1 ohms.\n",
      "9)Monitor fluctuations during operation: Standard Value is <2% variation.\n",
      "If any of the values in steps 7,8 and 9 are found to breach standard values, the likely cause\n",
      "would be Insufficient power supply causing fluctuations. If not, proceed to the next steps\n",
      "10)Review maintenance logs: Frequency standard is Monthly.\n",
      "11)Inspect for dust accumulation on motor surfaces: standard value is <1 mm thickness.\n",
      "12)Measure lubrication frequency : Standard Value is every 50 operating hours.\n",
      "If any of the values in steps 10,11 and 12 are found to breach standard values, the likely\n",
      "cause would be lack of regular maintenance, including\n",
      "cleaning and lubrication. If not, the root cause is a new one which is not listed.\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "                ----\n",
      "                \n",
      "Human: Question:```What was the question I asked before?```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 19:51:47 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I apologize, but I am an AI assistant and do not have the capability to remember previous questions. Is there anything else I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %run ./utilitiesPOC\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from utilitiesPOC import *\n",
    "\n",
    "from auths import openaisecret, embeddings_secret, adls_secret\n",
    "\n",
    "storage_account =  '{\"storage_account_key\": \"\",\"storage_container\": \"manufacturing-copilot\",\"storage_account\": \"euwdsrg03rsta07dls01\"}'\n",
    "path = \"Documents/Embeddings/\"\n",
    "\n",
    "\n",
    "\n",
    "llmconfig= '{\"mode\": \"langchain\", \"auth\": \"token\", \"model\": \"gpt-35-turbo-16k\", \"deployment_name\": \"mxdrca\",\"api_version\": \"2023-05-15\",\"api_key_secret\":\"\",\"api_endpoint\": \"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "embedconfig= '{\"embedmode\": \"langchain\",\"model\":\"text-embedding-ada-002\",\"deployment_name\":\"azure_embedding\",\"api_key_secret\":\"\",\"embeddingurl\":\"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "\n",
    "storage_account = json.loads(storage_account)\n",
    "storage_account_key, container, storage_account = storage_account.get('storage_account_key'),storage_account.get('storage_container'),storage_account.get('storage_account')\n",
    "\n",
    "#read from file\n",
    "storage_account_key = adls_secret\n",
    "\n",
    "llmconfig = json.loads(llmconfig)\n",
    "embedconfig = json.loads(embedconfig)\n",
    "\n",
    "embedmode, embedmodel,embed_deployment,embed_key, embed_url = embedconfig.values()\n",
    "mode, auth, model, deployment_name, api_version, api_key_secret, api_endpoint = llmconfig.values()\n",
    "\n",
    "api_key_secret = openaisecret\n",
    "embed_key = embeddings_secret\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#GETTING THE SECRETS\n",
    "api_key = api_key_secret\n",
    "\n",
    "\n",
    "embed_key = embed_key\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Define LLM \n",
    "#DEFINE LLM USING AZURE OPEN AI\n",
    "llmrca = getllm(openai=True,azure =True,mode = mode, auth=auth,model=model,dep_name = deployment_name,api_version= api_version,\n",
    "                     api_key =api_key,api_base = api_endpoint)\n",
    "\n",
    "\n",
    "embeddings_model  = getembeddings(model=embedmodel,dep_name=embed_deployment,api_key=embed_key,api_base=embed_url)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "mount_point = '/mnt/copilotembedds'\n",
    "\n",
    "\n",
    "#@async_retry\n",
    "def read_embeddings(fromadls=False,dbricks=False,**kwargs):\n",
    "    if fromadls:\n",
    "        if dbricks:\n",
    "            #reads it into a dataframe first from the mount point\n",
    "            storage_account=storage_account,container=container,mount_point=mount_point,path=path,storage_account_key=storage_account_key\n",
    "            for mount in dbutils.fs.mounts():\n",
    "                if mount.mountPoint.startswith(mount_point):\n",
    "                    print(\"Unmounting mnt point\")\n",
    "                    dbutils.fs.unmount(mount.mountPoint)\n",
    "            \n",
    "            #mount to path\n",
    "            dbutils.fs.refreshMounts() #refresh mounts to avoid errors\n",
    "\n",
    "            mntfiles = mount_to_local(storage_account, container, mount_point,path, storage_account_key)\n",
    "            #reads into dataframe\n",
    "            adlsembedspath = [e.path for e in dbutils.fs.ls(mntfiles)][-1] #read last file \n",
    "            adlsembedspath= adlsembedspath.replace(\"dbfs:\",\"/dbfs\")\n",
    "            df = pd.read_pickle(adlsembedspath)\n",
    "\n",
    "        else:\n",
    "            #read from adls but not from databricks\n",
    "            from azure.storage.filedatalake import DataLakeServiceClient\n",
    "            storage_account,container,path,storage_account_key=kwargs.get(\"storage_account\"),kwargs.get(\"container\"),kwargs.get(\"path\"),kwargs.get(\"storage_account_key\")\n",
    "            connstr = f\"DefaultEndpointsProtocol=https;AccountName={storage_account};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "            service = DataLakeServiceClient.from_connection_string(conn_str=connstr)\n",
    "            file_system_client = service.get_file_system_client(container)\n",
    "            paths = file_system_client.get_paths(path=path)\n",
    "            files = [p for p in paths if not p.is_directory]\n",
    "            files.sort(key=lambda x: x.last_modified)\n",
    "            last_file = files[-1].name\n",
    "            file_client = file_system_client.get_file_client(last_file)\n",
    "            \n",
    "            # Download the file content and load it as a Python object\n",
    "            download = file_client.download_file()\n",
    "            downloaded_bytes = download.readall()\n",
    "            df = pickle.loads(downloaded_bytes)\n",
    "            #df = pd.read_pickle(data)\n",
    "            \n",
    "    else:\n",
    "        #read from local FS\n",
    "        adlsembedspath = [n for n in os.listdir(path)][-1]\n",
    "        \n",
    "        df = pd.read_pickle(adlsembedspath)\n",
    "    #returns the texts and embeddings\n",
    "    txtlist = [t for t in df.texts]\n",
    "    embeddings = [e for e in df.embeddings]\n",
    "    return txtlist,embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_embeddings_fromfs(path):\n",
    "    #service = DataLakeServiceClient.from_connection_string(conn_str=\"my_connection_string\")\n",
    "\n",
    "    adlsembedspath = [n for n in os.listdir(path)][-1]        \n",
    "    df = pd.read_pickle(path+\"/\"+adlsembedspath)\n",
    "    #returns the texts and embeddings\n",
    "    txtlist = [t for t in df.texts]\n",
    "    embeddings = [e for e in df.embeddings]\n",
    "    return txtlist,embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "loaded_embeddings = read_embeddings(fromadls=True,dbricks=False,storage_account=storage_account,container=container,path=path,storage_account_key=storage_account_key)#read_embeddings_fromfs(path)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import ClassVar\n",
    "import pickle\n",
    "\n",
    "@dataclass \n",
    "class RAG2:\n",
    "    memory_conversation  = None\n",
    "    def __init__(self,llm,graph,type=\"langchain\",embeddings=None,usememory=False):\n",
    "        self.llm = llm\n",
    "        self.graph = graph\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "        self.usememory = usememory\n",
    "\n",
    "    @staticmethod\n",
    "    def memory(type='langchain',memory_key=\"chat_history\",output_key='result',rmessages=True,readmem=None):\n",
    "        if readmem:\n",
    "            if type.lower()==\"langchain\":\n",
    "                from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "                memory = ConversationBufferMemory(\n",
    "                memory_key=memory_key, return_messages=rmessages)\n",
    "                #output_key=output_key)\n",
    "                \n",
    "                readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "\n",
    "                return memory\n",
    "        else: \n",
    "            return None\n",
    "    \n",
    "\n",
    "    def vector_retriever(self,store=\"chroma\",**kwargs):\n",
    "        texts = kwargs.get(\"texts\")\n",
    "        if store.lower()==\"chroma\":\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "\n",
    "            from langchain.chains import RetrievalQA\n",
    "            persist_directory = kwargs.get(\"persist_directory\",\"filesdb\")\n",
    "            search_kwargs = kwargs.get(\"search_kwargs\",{\"k\": 3})\n",
    "            from langchain.vectorstores import Chroma\n",
    "            \n",
    "            db = Chroma.from_documents(texts,self.embeddings,persist_directory= persist_directory)\n",
    "            vec_retriever = db.as_retriever(search_kwargs=search_kwargs)\n",
    "         \n",
    "            return vec_retriever  \n",
    "\n",
    "            \n",
    "        elif store.lower()==\"faiss\":\n",
    "            #to be implemented\n",
    "            from langchain_community.vectorstores import FAISS\n",
    "            from langchain_core.output_parsers import StrOutputParser\n",
    "            from langchain_core.prompts import ChatPromptTemplate\n",
    "            from langchain_core.runnables import RunnablePassthrough\n",
    "            from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "            if kwargs.get(\"text_embeddings\"):\n",
    "                text_embeddings =kwargs.get(\"text_embeddings\")\n",
    "                vectorstore = FAISS.from_embeddings(text_embeddings=list(zip(text_embeddings[0],text_embeddings[1])), embedding = self.embeddings)\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                return retriever\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                template = \"\"\"Answer the question based only on the following context:{context} \n",
    "                Question: {query}\"\"\"\n",
    "                prompt = ChatPromptTemplate.from_template(template)\n",
    "                retrieval_chain = ({\"context\": retriever, \"query\": RunnablePassthrough()} | prompt\n",
    "                                    | self.llm| StrOutputParser())\n",
    "                return retrieval_chain\n",
    "        \n",
    "    def build_chain(self,**kwargs):\n",
    "        #memory = self.memory() if (readmem := self.memory()) else None\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        \n",
    "        #PROMPT TEMPLATE        \n",
    "        from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate,PromptTemplate\n",
    "        #####\n",
    "\n",
    "        memory = self.usememory\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "\n",
    "        if memory:\n",
    "            prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {question}\")\n",
    "        else:\n",
    "            prompt = PromptTemplate(input_variables=[\"question\"],template =\"{question}\")\n",
    "\n",
    "        self.memory_conversation = self.memory(readmem=memory) if not self.memory_conversation else self.memory_conversation\n",
    "\n",
    "        if kwargs.get(\"falkor\"):\n",
    "            from langchain.chains import FalkorDBQAChain\n",
    "            chain = FalkorDBQAChain.from_llm(llm=self.llm, graph=self.graph, verbose=verbose, memory=self.memory_conversation, **kwargs)\n",
    "        else:\n",
    "            if method.lower()==\"graph\":\n",
    "                #general chain for graphs\n",
    "                print(\"building graph chain\")\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                #Prompt template\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "\n",
    "                chain = GraphCypherQAChain.from_llm(llm=self.llm, prompt=prompt,graph=self.graph,validate_cypher = True, \n",
    "                                                    verbose=verbose,memory=self.memory_conversation, promptTemplate=qa_prompt,**kwargs)\n",
    "                chain.input_key = \"question\"\n",
    "\n",
    "            elif method.lower()==\"vector\":\n",
    "                print(\"Building vector chain \")\n",
    "                retriever = kwargs.get(\"vec_retriever\")\n",
    "                from langchain.chains import RetrievalQA\n",
    "                from langchain.chains import ConversationalRetrievalChain\n",
    "                \n",
    "                # template = (\"Combine the chat history and follow up question into \"\n",
    "                # \"a standalone question. Chat History: {chat_history}\"\n",
    "                # \"Follow up question: {query} \")\n",
    "                template = (\"Every time you respond you should start with This is Copilot your AI assistant\")\n",
    "                prompt = PromptTemplate.from_template(template)\n",
    "                \n",
    "\n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,return_source_documents=True,verbose=verbose)\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "                \n",
    "                chain = ConversationalRetrievalChain.from_llm(self.llm,retriever,memory=self.memory_conversation,get_chat_history=lambda h :h,\n",
    "                                                              verbose=verbose,\n",
    "                                                              combine_docs_chain_kwargs={\"prompt\": qa_prompt}) \n",
    "                \n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,\n",
    "                #                                    return_source_documents=True,verbose=verbose,memory=self.memory_conversation,prompt=prompt)\n",
    "\n",
    "        return chain\n",
    "\n",
    "\n",
    "    def get_retriever(self,**kwargs):\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        complexity = kwargs.get(\"complexity\",\"simple\")\n",
    "        \n",
    "        if method.lower()=='graph':\n",
    "            if self.type.lower()==\"langchain\":\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                if complexity.lower()=='simple':\n",
    "                    print(\"SIMPLE RAG LANGCHAIN\")\n",
    "                    if usememory := kwargs.get(\"usememory\") or self.memory_conversation:\n",
    "                        print(\"With Read memory\")\n",
    "                    \n",
    "                    chain = self.build_chain(**kwargs)\n",
    "                    return chain\n",
    "                \n",
    "                    \n",
    "            elif self.type.lower()==\"llama\":\n",
    "                if complexity.lower()=='simple':\n",
    "                    nl2graph = kwargs.get(\"nl2graph\",False)\n",
    "                    print(nl2graph)\n",
    "                    from llama_index.query_engine import RetrieverQueryEngine\n",
    "                    from llama_index.retrievers import KnowledgeGraphRAGRetriever                    \n",
    "                    graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=kwargs.get(\"storage_context\"),\n",
    "                    service_context=kwargs.get(\"service_context\"),\n",
    "                    llm=self.llm,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=nl2graph)\n",
    "                    query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))\n",
    "                    return query_engine\n",
    "\n",
    "                \n",
    "                elif complexity.lower()==\"custom\":\n",
    "                    if self.type.lower()==\"llama\":\n",
    "                        from llama_index import get_response_synthesizer\n",
    "                        from llama_index.query_engine import RetrieverQueryEngine\n",
    "                        # create custom retriever\n",
    "                        vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "                        kg_retriever = KGTableRetriever(index=kg_index, retriever_mode=\"keyword\", include_text=False)\n",
    "                        custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "                        # create response synthesizer\n",
    "                        response_synthesizer = get_response_synthesizer(service_context=service_context,response_mode=\"tree_summarize\")\n",
    "                        custom_query_engine = RetrieverQueryEngine(retriever=custom_retriever,response_synthesizer=response_synthesizer)\n",
    "                        return custom_query_engine\n",
    "                    elif self.type.lower()=='langchain':\n",
    "                        from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "                        simple = self.get_retriever(\"simple\",**kwargs)\n",
    "                        \n",
    "                        tools = [\n",
    "                            Tool(name= \"GraphRAG\",\n",
    "                                 func= simple.run,\n",
    "                                 description= \"Simple RAG from knowledge graph\"),\n",
    "                            #add more RAG tools\n",
    "                        ]\n",
    "        elif method.lower()==\"vector\":\n",
    "            print(\"Getting vector retriever\")\n",
    "            vec_retriever = self.vector_retriever(**kwargs)\n",
    "\n",
    "            chain = self.build_chain(vec_retriever=vec_retriever,**kwargs)\n",
    "            return chain\n",
    "\n",
    "\n",
    "\n",
    "    def response(self,query,complexity=\"simple\",**kwargs):\n",
    "        method = kwargs.get(\"method\",\"vector\")\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "        retriever = self.get_retriever(**kwargs)\n",
    "        try:\n",
    "            if method.lower()==\"vector\":\n",
    "                print(\"I AM USING VECTOR LETS GO\")\n",
    "                result =  retriever({\"question\": query}).get('answer')\n",
    "                \n",
    "            elif method.lower()==\"graph\":\n",
    "                if self.type.lower()=='langchain':\n",
    "                    print('using retriever({\"query\": query}).get(\"answer\")')\n",
    "                    #result = retriever({\"question\": query}).get('answer')#\n",
    "                    result = retriever.invoke({\"question\": query}).get('result') \n",
    "                    pickled_str = pickle.dumps(self.memory_conversation)\n",
    "                elif self.type.lower()=='llama':\n",
    "                    result = retriever.query(query)\n",
    "                    if 'Empty Response' in result.response:\n",
    "                        raise(\"Error empty result or no information found\")\n",
    "                    else:\n",
    "                        return result.response\n",
    "            \n",
    "            if not result or \"I'm sorry, but I don't have\" in result:\n",
    "                print('Not using RAG')\n",
    "                raise (\"Error empty result or no information found in the KG\")\n",
    "            elif \"I'm sorry, but I don't have\" in result:\n",
    "                raise(\"ERROR 404\")\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if self.type.lower()=='langchain':\n",
    "                from langchain.schema import HumanMessage\n",
    "                #message = HumanMessage(content=query)\n",
    "                from langchain.chains import ConversationChain\n",
    "                from langchain.prompts.prompt import PromptTemplate\n",
    "                if self.usememory:\n",
    "                    print(\"Normal LLM response with same memory\")\n",
    "                    prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {query}\")\n",
    "                    chat = ConversationChain(llm=self.llm,verbose=kwargs.get(\"verbose\",True),prompt=prompt,memory=self.memory_conversation ,input_key=\"question\")\n",
    "                    return chat.invoke({\"question\":query}).get(\"response\")\n",
    "                else:\n",
    "                    from langchain.schema import HumanMessage\n",
    "                    message = HumanMessage(content=query)\n",
    "                    return self.llm.invoke([message])\n",
    "\n",
    "            else:\n",
    "                #return response with llama index self.llm(response)\n",
    "                pass\n",
    "            \n",
    "\n",
    "# COMMAND ----------\n",
    "#start graph\n",
    "url =\"neo4j+s://b75e0bd8.databases.neo4j.io\"\n",
    "username =\"neo4j\"\n",
    "password = \"aziMmumvuVru-rgeKuO0zT7mEtTxu29Jgrmp6lFKv0w\"\n",
    "graph_store = get_graphdb(\"langchain\",graphdb = \"neo4j\",url=url,username=username,password=password)\n",
    "\n",
    "\n",
    "#call RAG class initialize vector class \n",
    "rag2 = RAG2\n",
    "vecrag = rag2(llmrca,graph_store,type=\"langchain\",embeddings = embeddings_model,usememory=True)\n",
    "#vectorchain = vecrag.get_retriever(method=\"vector\",text_embeddings = loaded_embeddings,store=\"faiss\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "\n",
    "    # print(vectorchain({\"question\": \"where and why did the production stoppage occurred\"}).get(\"answer\")) #if use memory we call it like this \n",
    "    # print(vectorchain({\"question\": \"what was my previous question?\"}).get(\"answer\"))\n",
    "    # #\n",
    "    print(vecrag.response(query=\"where and why did the production stoppage occurred\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "    print(vecrag.response(query=\"what was my previous question?\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "# COMMAND ----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "032827ad-4f2f-4c03-aa24-97404d1cda00",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag2 = RAG2\n",
    "vecrag = rag2(llmrca,graph_store,type=\"langchain\",embeddings = embeddings_model,usememory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8f95529-57cb-4674-b450-c501a713a1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vector retriever\n",
      "Building vector chain \n",
      "I AM USING VECTOR LETS GO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 19:52:49 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  \n",
      "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "                ----\n",
      "                ﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0001198\n",
      "Issue Description: Variations in the quality and accuracy of produced machined parts on Assembly Line 7\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.51 ohms, Voltage fluctuation : 4%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "                ----\n",
      "                \n",
      "Human: Question:```where and why did the production stoppage occurred```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 19:52:53 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a detailed analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, factors such as dust accumulation and lubrication frequency could also impact the overall performance of the assembly line. A thorough investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.\n",
      "Getting vector retriever\n",
      "Building vector chain \n",
      "I AM USING VECTOR LETS GO\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='where and why did the production stoppage occurred'), AIMessage(content='Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a detailed analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, factors such as dust accumulation and lubrication frequency could also impact the overall performance of the assembly line. A thorough investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.')]\n",
      "Follow Up Input: what was my previous question?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 19:52:53 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-19 19:52:53 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  \n",
      "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "                ----\n",
      "                ﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "RCA Reference Guide\n",
      "Please refer the systematic sequence of steps listed below to methodically identify and address root\n",
      "cause issues contributing to various issues in production quality.\n",
      "1)Check the cooling mechanisms: Standard Airflow rate through cooling vents is >50 CFM.\n",
      "2)Evaluate ambient temperature around the motor: Standard Value is 20-25 degrees Celsius.\n",
      "3)Motor surface temperature during operation:  Standard Value is <60 degrees Celsius.\n",
      "If any of the values in steps 1,2 and 3 are found to breach standard values, the likely cause\n",
      "would be Inadequate cooling leading to motor overheating. If not, proceed to the next steps\n",
      "4)Check the rated power of the motor: Standard Value is 1.5 HP\n",
      "5)Assess the load on the motor: Standard Value is <80% of rated current.\n",
      "6)Check user workloads: Standard value is <80% of rated workload.\n",
      "If any of the values in steps 4,5 and 6 are found to breach standard values, the likely cause\n",
      "would be Overloading the motor beyond its capacity. If not, proceed to the next steps\n",
      "7)Voltage stability: standard value is +-5%\n",
      "8)Measure resistance in power wiring: Standard power wiring resistance is <0.1 ohms.\n",
      "9)Monitor fluctuations during operation: Standard Value is <2% variation.\n",
      "If any of the values in steps 7,8 and 9 are found to breach standard values, the likely cause\n",
      "would be Insufficient power supply causing fluctuations. If not, proceed to the next steps\n",
      "10)Review maintenance logs: Frequency standard is Monthly.\n",
      "11)Inspect for dust accumulation on motor surfaces: standard value is <1 mm thickness.\n",
      "12)Measure lubrication frequency : Standard Value is every 50 operating hours.\n",
      "If any of the values in steps 10,11 and 12 are found to breach standard values, the likely\n",
      "cause would be lack of regular maintenance, including\n",
      "cleaning and lubrication. If not, the root cause is a new one which is not listed.\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "                ----\n",
      "                \n",
      "Human: Question:```What was the question I asked before?```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 19:52:54 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I apologize, but I am an AI assistant and do not have the capability to remember previous questions. Is there anything else I can assist you with?\n"
     ]
    }
   ],
   "source": [
    "print(vecrag.response(query=\"where and why did the production stoppage occurred\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "print(vecrag.response(query=\"what was my previous question?\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce3f3aa-0cbe-4a44-ace6-0f2e12b4a12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE RAG LANGCHAIN\n",
      "With Read memory\n",
      "building graph chain\n"
     ]
    }
   ],
   "source": [
    "## Vector CHAIN \n",
    "graphchain= vecrag.get_retriever(method=\"graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60c691db-5002-42ee-bd21-02664bd71074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 20:15:11 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mMATCH (s:Entity)-[:CONCLUDES]->(c:Entity)\n",
      "RETURN c\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[{'c': {'id': 'only \"about 13% of methane emissions are covered\"'}}, {'c': {'id': 'only \"about 13% of methane emissions are covered by methane mitigation policies\"'}}, {'c': {'id': 'effectiveness of methane mitigation policies \"is far from clear\"'}}]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 20:15:13 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'what did the study concluded',\n",
       " 'chat_history': [HumanMessage(content='where and why did the production stoppage occurred'),\n",
       "  AIMessage(content='Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a detailed analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, factors such as dust accumulation and lubrication frequency could also impact the overall performance of the assembly line. A thorough investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.'),\n",
       "  HumanMessage(content='what was my previous question?'),\n",
       "  AIMessage(content='I apologize, but I am an AI assistant and do not have the capability to remember previous questions. Is there anything else I can assist you with?'),\n",
       "  HumanMessage(content='what damages did the hurricane made'),\n",
       "  AIMessage(content=\"I'm sorry, but I don't have any information on the damages caused by the hurricane.\"),\n",
       "  HumanMessage(content='what did the hurricane made'),\n",
       "  AIMessage(content=\"I'm sorry, but I don't have any information on what the hurricane made.\"),\n",
       "  HumanMessage(content='what happened in 2018'),\n",
       "  AIMessage(content=\"I'm sorry, but I don't have any information about what happened in 2018.\"),\n",
       "  HumanMessage(content='tell me about hurricane Ian'),\n",
       "  AIMessage(content=\"I'm sorry, but I don't have any information about hurricane Ian.\"),\n",
       "  HumanMessage(content=' hurricane Ian caused what'),\n",
       "  AIMessage(content=\"I'm sorry, but I don't have any information on what hurricane Ian caused.\"),\n",
       "  HumanMessage(content='what did the study concluded'),\n",
       "  AIMessage(content='The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.')],\n",
       " 'result': 'The study concluded that only about 13% of methane emissions are covered by methane mitigation policies. Additionally, the effectiveness of these policies is far from clear.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphchain.invoke({\"question\":\"what did the study concluded\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994bd6f-b814-4eb6-8346-afdb77b59782",
   "metadata": {},
   "source": [
    "### LLama index graph test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e260804f-e0b7-4603-91fb-aacebee1f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass \n",
    "class RAG2:\n",
    "    memory_conversation  = None\n",
    "    def __init__(self,llm,graph,type=\"langchain\",embeddings=None,usememory=False):\n",
    "        self.llm = llm\n",
    "        self.graph = graph\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "        self.usememory = usememory\n",
    "\n",
    "    @staticmethod\n",
    "    def memory(type='langchain',memory_key=\"chat_history\",output_key='result',rmessages=True,readmem=None):\n",
    "        if readmem:\n",
    "            if type.lower()==\"langchain\":\n",
    "                from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "                memory = ConversationBufferMemory(\n",
    "                memory_key=memory_key, return_messages=rmessages)\n",
    "                #output_key=output_key)\n",
    "                \n",
    "                readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "\n",
    "                return memory\n",
    "        else: \n",
    "            return None\n",
    "    \n",
    "\n",
    "    def vector_retriever(self,store=\"chroma\",**kwargs):\n",
    "        texts = kwargs.get(\"texts\")\n",
    "        if store.lower()==\"chroma\":\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "\n",
    "            from langchain.chains import RetrievalQA\n",
    "            persist_directory = kwargs.get(\"persist_directory\",\"filesdb\")\n",
    "            search_kwargs = kwargs.get(\"search_kwargs\",{\"k\": 3})\n",
    "            from langchain.vectorstores import Chroma\n",
    "            \n",
    "            db = Chroma.from_documents(texts,self.embeddings,persist_directory= persist_directory)\n",
    "            vec_retriever = db.as_retriever(search_kwargs=search_kwargs)\n",
    "         \n",
    "            return vec_retriever  \n",
    "\n",
    "            \n",
    "        elif store.lower()==\"faiss\":\n",
    "            #to be implemented\n",
    "            from langchain_community.vectorstores import FAISS\n",
    "            from langchain_core.output_parsers import StrOutputParser\n",
    "            from langchain_core.prompts import ChatPromptTemplate\n",
    "            from langchain_core.runnables import RunnablePassthrough\n",
    "            from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "            if kwargs.get(\"text_embeddings\"):\n",
    "                text_embeddings =kwargs.get(\"text_embeddings\")\n",
    "                vectorstore = FAISS.from_embeddings(text_embeddings=list(zip(text_embeddings[0],text_embeddings[1])), embedding = self.embeddings)\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                return retriever\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                template = \"\"\"Answer the question based only on the following context:{context} \n",
    "                Question: {query}\"\"\"\n",
    "                prompt = ChatPromptTemplate.from_template(template)\n",
    "                retrieval_chain = ({\"context\": retriever, \"query\": RunnablePassthrough()} | prompt\n",
    "                                    | self.llm| StrOutputParser())\n",
    "                return retrieval_chain\n",
    "        \n",
    "    def build_chain(self,**kwargs):\n",
    "        #memory = self.memory() if (readmem := self.memory()) else None\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        \n",
    "        #PROMPT TEMPLATE        \n",
    "        from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate,PromptTemplate\n",
    "\n",
    "        \n",
    "        #####\n",
    "\n",
    "        memory = self.usememory\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "\n",
    "        if memory:\n",
    "            prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {question}\")\n",
    "        else:\n",
    "            prompt = PromptTemplate(input_variables=[\"question\"],template =\"{question}\")\n",
    "\n",
    "        self.memory_conversation = self.memory(readmem=memory) if not self.memory_conversation else self.memory_conversation\n",
    "\n",
    "        if kwargs.get(\"falkor\"):\n",
    "            from langchain.chains import FalkorDBQAChain\n",
    "            chain = FalkorDBQAChain.from_llm(llm=self.llm, graph=self.graph, verbose=verbose, memory=self.memory_conversation, **kwargs)\n",
    "        else:\n",
    "            if method.lower()==\"graph\":\n",
    "                #general chain for graphs\n",
    "                print(\"building graph chain\")\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                #Prompt template\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "\n",
    "                chain = GraphCypherQAChain.from_llm(llm=self.llm, prompt=prompt,graph=self.graph,validate_cypher = True, \n",
    "                                                    verbose=verbose,memory=self.memory_conversation, promptTemplate=qa_prompt,**kwargs)\n",
    "                chain.input_key = \"question\"\n",
    "\n",
    "            elif method.lower()==\"vector\":\n",
    "                print(\"Building vector chain \")\n",
    "                retriever = kwargs.get(\"vec_retriever\")\n",
    "                from langchain.chains import RetrievalQA\n",
    "                from langchain.chains import ConversationalRetrievalChain\n",
    "                \n",
    "                # template = (\"Combine the chat history and follow up question into \"\n",
    "                # \"a standalone question. Chat History: {chat_history}\"\n",
    "                # \"Follow up question: {query} \")\n",
    "                template = (\"Every time you respond you should start with This is Copilot your AI assistant\")\n",
    "                prompt = PromptTemplate.from_template(template)\n",
    "                \n",
    "\n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,return_source_documents=True,verbose=verbose)\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "                \n",
    "                chain = ConversationalRetrievalChain.from_llm(self.llm,retriever,memory=self.memory_conversation,get_chat_history=lambda h :h,\n",
    "                                                              verbose=verbose,\n",
    "                                                              combine_docs_chain_kwargs={\"prompt\": qa_prompt}) \n",
    "                \n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,\n",
    "                #                                    return_source_documents=True,verbose=verbose,memory=self.memory_conversation,prompt=prompt)\n",
    "\n",
    "        return chain\n",
    "\n",
    "\n",
    "    def get_retriever(self,**kwargs):\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        complexity = kwargs.get(\"complexity\",\"simple\")\n",
    "        \n",
    "        if method.lower()=='graph':\n",
    "            if self.type.lower()==\"langchain\":\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                if complexity.lower()=='simple':\n",
    "                    print(\"SIMPLE RAG LANGCHAIN\")\n",
    "                    if usememory := kwargs.get(\"usememory\") or self.memory_conversation:\n",
    "                        print(\"With Read memory\")\n",
    "                    \n",
    "                    chain = self.build_chain(**kwargs)\n",
    "                    return chain\n",
    "                \n",
    "                    \n",
    "            elif self.type.lower()==\"llama\":\n",
    "                if complexity.lower()=='simple':\n",
    "                    nl2graph = kwargs.get(\"nl2graph\",False)\n",
    "                    print(nl2graph)\n",
    "                    from llama_index.query_engine import RetrieverQueryEngine\n",
    "                    from llama_index.retrievers import KnowledgeGraphRAGRetriever                    \n",
    "                    graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=kwargs.get(\"storage_context\"),\n",
    "                    service_context=kwargs.get(\"service_context\"),\n",
    "                    llm=self.llm,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=nl2graph)\n",
    "                    query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))\n",
    "                    return query_engine\n",
    "\n",
    "                \n",
    "                elif complexity.lower()==\"custom\":\n",
    "                    if self.type.lower()==\"llama\":\n",
    "                        from llama_index import get_response_synthesizer\n",
    "                        from llama_index.query_engine import RetrieverQueryEngine\n",
    "                        # create custom retriever\n",
    "                        vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "                        kg_retriever = KGTableRetriever(index=kg_index, retriever_mode=\"keyword\", include_text=False)\n",
    "                        custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "                        # create response synthesizer\n",
    "                        response_synthesizer = get_response_synthesizer(service_context=service_context,response_mode=\"tree_summarize\")\n",
    "                        custom_query_engine = RetrieverQueryEngine(retriever=custom_retriever,response_synthesizer=response_synthesizer)\n",
    "                        return custom_query_engine\n",
    "                    elif self.type.lower()=='langchain':\n",
    "                        from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "                        simple = self.get_retriever(\"simple\",**kwargs)\n",
    "                        \n",
    "                        tools = [\n",
    "                            Tool(name= \"GraphRAG\",\n",
    "                                 func= simple.run,\n",
    "                                 description= \"Simple RAG from knowledge graph\"),\n",
    "                            #add more RAG tools\n",
    "                        ]\n",
    "        elif method.lower()==\"vector\":\n",
    "            print(\"Getting vector retriever\")\n",
    "            vec_retriever = self.vector_retriever(**kwargs)\n",
    "\n",
    "            chain = self.build_chain(vec_retriever=vec_retriever,**kwargs)\n",
    "            return chain\n",
    "\n",
    "\n",
    "\n",
    "    def response(self,query,complexity=\"simple\",**kwargs):\n",
    "        method = kwargs.get(\"method\",\"vector\")\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "        retriever = self.get_retriever(**kwargs)\n",
    "        try:\n",
    "            if method.lower()==\"vector\":\n",
    "                print(\"I AM USING VECTOR LETS GO\")\n",
    "                result =  retriever({\"question\": query}).get('answer')\n",
    "                \n",
    "            elif method.lower()==\"graph\":\n",
    "                if self.type.lower()=='langchain':\n",
    "                    print('using retriever({\"query\": query}).get(\"answer\")')\n",
    "                    #result = retriever({\"question\": query}).get('answer')#\n",
    "                    result = retriever.invoke({\"question\": query}).get('result') \n",
    "                    pickled_str = pickle.dumps(self.memory_conversation)\n",
    "                elif self.type.lower()=='llama':\n",
    "                    result = retriever.query(query)\n",
    "                    if 'Empty Response' in result.response:\n",
    "                        raise(\"Error empty result or no information found\")\n",
    "                    else:\n",
    "                        return result.response\n",
    "            \n",
    "            if not result or \"I'm sorry, but I don't have\" in result:\n",
    "                print('Not using RAG')\n",
    "                raise (\"Error empty result or no information found in the KG\")\n",
    "            elif \"I'm sorry, but I don't have\" in result:\n",
    "                raise(\"ERROR 404\")\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if self.type.lower()=='langchain':\n",
    "                from langchain.schema import HumanMessage\n",
    "                #message = HumanMessage(content=query)\n",
    "                from langchain.chains import ConversationChain\n",
    "                from langchain.prompts.prompt import PromptTemplate\n",
    "                if self.usememory:\n",
    "                    print(\"Normal LLM response with same memory\")\n",
    "                    prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {query}\")\n",
    "                    chat = ConversationChain(llm=self.llm,verbose=kwargs.get(\"verbose\",True),prompt=prompt,memory=self.memory_conversation ,input_key=\"question\")\n",
    "                    return chat.invoke({\"question\":query}).get(\"response\")\n",
    "                else:\n",
    "                    from langchain.schema import HumanMessage\n",
    "                    message = HumanMessage(content=query)\n",
    "                    return self.llm.invoke([message])\n",
    "\n",
    "            else:\n",
    "                #return response with llama index self.llm(response)\n",
    "                pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d65e16c-4c52-4307-abb0-565660ef9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getllm(openai=True, azure=False, mode=\"langchain\",**kwargs):\n",
    "    print(\"Note: to use the LLM use it as msg = HumanMessage(content= your query) \\n llm(messages = [msg]) \")\n",
    "    if openai:\n",
    "        temperature = kwargs.get(\"temperature\",0)\n",
    "        if azure:\n",
    "            print(\"Using Azure OpenAI\")\n",
    "            # from langchain.chat_models import AzureChatOpenAI\n",
    "            from langchain_openai import AzureChatOpenAI\n",
    "            auth = kwargs.get(\"auth\")\n",
    "            if auth.lower()==\"token\":\n",
    "                if mode.lower()==\"langchain\":\n",
    "                    params = {\"model\": kwargs.get(\"model\"),\n",
    "                        \"azure_deployment\": kwargs.get(\"dep_name\"),\n",
    "                        \"openai_api_version\": kwargs.get(\"api_version\"),\n",
    "                        \"temperature\": temperature,\n",
    "                        \"openai_api_key\": kwargs.get(\"api_key\"),\n",
    "                        \"azure_endpoint\": kwargs.get(\"api_base\"),\n",
    "                        \"max_tokens\":kwargs.get(\"max_tokens\",256)}\n",
    "                    llm = AzureChatOpenAI(**params)\n",
    "                elif mode.lower()==\"llama\":\n",
    "                    print(\"Llama's version\")\n",
    "                    #from llama_index.llms import AzureOpenAI\n",
    "                    from llama_index.llms.azure_openai import AzureOpenAI\n",
    "                    import openai\n",
    "                    openai.api_type = \"azure\"\n",
    "                    # openai.api_base =  kwargs.get(\"api_base\") #\"INSERT AZURE API BASE\"\n",
    "                    # openai.api_version = kwargs.get(\"api_version\",\"2023-05-15\")\n",
    "                    \n",
    "                    llm = AzureOpenAI(engine= kwargs.get(\"dep_name\"),temperature=0,model=kwargs.get(\"model\"),\n",
    "                                     azure_deployment =kwargs.get(\"dep_name\"),azure_endpoint = kwargs.get(\"api_base\"),\n",
    "                                     api_version = kwargs.get(\"api_version\",\"2023-05-15\"),api_key= kwargs.get(\"api_key\"))\n",
    "                return llm\n",
    "            elif auth.lower() == \"azure_ad\":\n",
    "                #from azure.identity import DefaultAzureCredential\n",
    "                #credential = DefaultAzureCredential()\n",
    "                #Set the API type to `azure_ad`\n",
    "                #os.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n",
    "               ##Set the API_KEY to the token from the Azure credential\n",
    "                #os.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token            elif auth.lower()==\"azure_ad_provider\":\n",
    "                llm = \"Not implemented\"\n",
    "            elif auth == \"azure ad_provider\":\n",
    "                #managed_identity:\n",
    "                #from azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential   \n",
    "                #credential = ChainedTokenCredential(\n",
    "                #ManagedIdentityCredential(),\n",
    "                #AzureCliCredential())\n",
    "                llm = \"not implemented\"\n",
    "        else:\n",
    "            from langchain_openai import OpenAI\n",
    "            api_base = kwargs.get(\"api_base_url\")\n",
    "            api_key =kwargs.get(\"api_key\",\"NULL\")\n",
    "            llm = OpenAI(\n",
    "                         base_url = api_base,\n",
    "                         openai_api_key = api_key,**kwargs)\n",
    "            return llm \n",
    "    else: #huggingface model\n",
    "        pass    \n",
    "\n",
    "\n",
    "\n",
    "def llm_cost(llm,message) -> \"Total cost in USD\":\n",
    "    from langchain.callbacks import get_openai_callback\n",
    "    with get_openai_callback() as cb:\n",
    "        llm([message])\n",
    "        cost=format(cb.total_cost, '.6f')\n",
    "        print(f\"Total Cost (USD): ${cost}\")\n",
    "    return cost\n",
    "\n",
    "\n",
    "\n",
    "    def get_embeddings(self,documents=None,lib='langchain'):\n",
    "#         if not documents:\n",
    "#             documents = pass\n",
    "        #huggingface or langchain\n",
    "        if lib=='langchain':\n",
    "            from langchain.embeddings import OpenAIEmbeddings\n",
    "            embeddings_model = OpenAIEmbeddings()\n",
    "            embeddings = embeddings_model.embed_documents(documents)\n",
    "            return emebeddings\n",
    "        else:\n",
    "            from langchain.embeddings import HuggingFaceEmbeddings\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=self.huggingembed)\n",
    "            return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a157a285-ee20-4dda-98cc-21029df6c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_graphdb(args,**kwargs):\n",
    "    graphdb = kwargs.get(\"graphdb\")\n",
    "    url,username,password,database = kwargs.get(\"url\"), kwargs.get(\"username\"), kwargs.get(\"password\"), kwargs.get(\"database\")\n",
    "    if args.lower()=='llama':\n",
    "        #from llama_index.storage.storage_context import StorageContext\n",
    "        from llama_index.core import StorageContext\n",
    "        print('Using LLama Index graph class')\n",
    "        if graphdb.lower()=='neo4j' or 'neo4j' in url:\n",
    "            from llama_index.graph_stores.neo4j import Neo4jGraphStore\n",
    "\n",
    "            graph_store = Neo4jGraphStore(\n",
    "                username=username,\n",
    "                password=password,\n",
    "                url=url,\n",
    "                database=database,\n",
    "            )\n",
    "            #build storage context\n",
    "            storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "            return graph_store, storage_context\n",
    "        elif graphdb.lower()=='nebula':\n",
    "            from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "            #TBD get varibales from kwargs\n",
    "            graph_store = NebulaGraphStore(\n",
    "            space_name=kwargs.get(\"space_name\"),\n",
    "            edge_types=kwargs.get(\"edge_types\"),\n",
    "            rel_prop_names=kwargs.get(\"rel_prop_names\"),\n",
    "            tags=kwargs.get(\"tags\"))\n",
    "            storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "            return graph_store, storage_context\n",
    "        \n",
    "        else:\n",
    "            raise(\"Error not implemented for that graph database\")\n",
    "    \n",
    "\n",
    "    elif args.lower()=='langchain':\n",
    "        print('Using langchain graph class')\n",
    "        if graphdb.lower()=='neo4j' or 'neo4j' in url:\n",
    "            from langchain.graphs import Neo4jGraph\n",
    "            graph= Neo4jGraph(\n",
    "               url=url,\n",
    "               username=username,\n",
    "               password=password\n",
    "            )\n",
    "    \n",
    "            return graph\n",
    "        elif graphdb.lower()==\"falkordb\":\n",
    "            print(\"Using falkordb\")\n",
    "            from langchain.graphs import FalkorDBGraph\n",
    "            graph = FalkorDBGraph(database = database,host=url,port=6379, username=username,password = pwd)\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise(\"Error not implemented for that graph database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749eb20-c10e-4c53-ab6b-e8add1cbb6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c0c388-b1a0-4fb1-ae85-ce9bbce98394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to use the LLM use it as msg = HumanMessage(content= your query) \n",
      " llm(messages = [msg]) \n",
      "Using Azure OpenAI\n",
      "Note: to use the LLM use it as msg = HumanMessage(content= your query) \n",
      " llm(messages = [msg]) \n",
      "Using Azure OpenAI\n",
      "Llama's version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\o_hol\\AppData\\Roaming\\Python\\Python311\\site-packages\\scipy\\__init__.py:169: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#loading \n",
    "llmazure =getllm(openai=True,azure =True,mode = \"langchain\", auth=\"token\",model=\"gpt-35-turbo-16k\",dep_name = \"chat-bot-trail\",            \n",
    "                  api_version= \"2023-12-01-preview\", api_key = \"1bbc7a5ceb0e4677888f22d1eb4c8617\",api_base = \"https://chat-bot-iot.openai.azure.com/\")#([message])\n",
    "#LLAMA'S VERSION\n",
    "llmazure2 =getllm(openai=True,azure =True,mode = \"llama\", auth=\"token\",model=\"gpt-35-turbo-16k\",dep_name = \"chat-bot-trail\",            \n",
    "                  api_version= \"2023-05-15\", api_key = \"1bbc7a5ceb0e4677888f22d1eb4c8617\",api_base = \"https://chat-bot-iot.openai.azure.com/\")#([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14ddb8fd-c907-4ea3-987a-eb7b1f293439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using langchain graph class\n",
      "Using LLama Index graph class\n"
     ]
    }
   ],
   "source": [
    "#INITIATE THE GRAPH STORE\n",
    "url = \"neo4j+s://b75e0bd8.databases.neo4j.io\"\n",
    "username= \"neo4j\"\n",
    "password = \"aziMmumvuVru-rgeKuO0zT7mEtTxu29Jgrmp6lFKv0w\"\n",
    "graph_store = get_graphdb(\"langchain\",graphdb=\"neo4j\",url=url,username=username,password=password)\n",
    "\n",
    "#GRAPH STORE LLAMA \n",
    "graph_store_llama, storage_context = get_graphdb(\"llama\",graphdb=\"neo4j\",url=url,username=username,password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eee31228-85f8-4707-86b2-94bde63dba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import KnowledgeGraphQueryEngine\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import KnowledgeGraphRAGRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45d267e1-325b-443f-a9d6-52ae97922a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag2 = RAG2(llmazure2,graph_store_llama,type=\"llama\")\n",
    "service_context= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9630d1e-31c2-4967-abed-1c5fc7761072",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=storage_context,\n",
    "                    llm=llmazure2,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=True)\n",
    "#query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83e46cce-9a6f-4951-ad10-8d894982d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=storage_context,\n",
    "                    llm=llmazure2,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84e62cda-4fec-4eba-b306-ebe74ad79771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureOpenAI(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x00000292B5AA8D90>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x00000292B54E7920>, completion_to_prompt=<function default_completion_to_prompt at 0x00000292B556A840>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='gpt-35-turbo-16k', temperature=0.0, max_tokens=None, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='1bbc7a5ceb0e4677888f22d1eb4c8617', api_base='https://api.openai.com/v1', api_version='2023-05-15', engine='chat-bot-trail', azure_endpoint='https://chat-bot-iot.openai.azure.com/', azure_deployment='chat-bot-trail', use_azure_ad=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmazure2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5e3d41d-7a75-409a-9917-80451f5bcbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    graph_rag_retriever,\n",
    "    llm = llmazure2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3a93430-eefd-47e5-8a82-acaa12288645",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 20:16:43 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-19 20:16:43 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-19 20:16:44 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The study concluded that a moderate decrease in body temperature extends lifespan.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\n",
    "    \"what did the study concluded\",\n",
    ").response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3d69df8-f956-4e76-9bd9-0873061b10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag_retriever_with_nl2graphquery = KnowledgeGraphRAGRetriever(\n",
    "    storage_context=storage_context,\n",
    "    verbose=True,\n",
    "    with_nl2graphquery=True,\n",
    "    llm = llmazure2\n",
    ")\n",
    "\n",
    "query_engine_with_nl2graphquery = RetrieverQueryEngine.from_args(\n",
    "    graph_rag_retriever_with_nl2graphquery,\n",
    "    llm = llmazure2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7e977b64-12bd-43d3-8982-9540894bc499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 20:34:20 [WARNING]: Error in retrieving from nl2graphquery: 'NoneType' object has no attribute 'kwargs'\n",
      "2024-02-19 20:34:21 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-19 20:34:21 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-19 20:34:22 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The study concluded that a moderate decrease in body temperature extends lifespan.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine_with_nl2graphquery.query(\n",
    "    \"what did the study concluded\",\n",
    ").response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "932060df-425a-46ce-abb3-32e7cf0e2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add it to langchain tools\n",
    "from llama_index.core.langchain_helpers.agents import (\n",
    "    IndexToolConfig,\n",
    "    LlamaIndexTool,\n",
    ")\n",
    "\n",
    "tool_config = IndexToolConfig(\n",
    "    query_engine=query_engine_with_nl2graphquery,\n",
    "    name=f\"KG Index\",\n",
    "    description=f\"useful for when you want to answer queries about neo4j\",\n",
    "    tool_kwargs={\"return_direct\": False},\n",
    ")\n",
    "\n",
    "tool = LlamaIndexTool.from_tool_config(tool_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f32b41b-e05d-410f-9c5b-11e686a6c78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 20:31:40 [WARNING]: Error in retrieving from nl2graphquery: 'NoneType' object has no attribute 'kwargs'\n",
      "2024-02-19 20:31:41 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-19 20:31:41 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-19 20:31:43 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The study is related to various subjects such as a moderate decrease in body temperature extending lifespan, neuro-molecular mechanics, disappearance, more than 50% of freshwater lakes, population declines, school curricula including certain topics, carbon taxation approaches, release of methane from Arctic glacial retreat, and the assembly of 43 diverse Y chromosomes.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.run(\"tell me all you know about a study\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa833d45-a18a-4ef1-9570-6c8e384bc66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa5b50-9096-4603-8efa-837e7b08f496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744d16e-332e-489a-97d2-440681c883e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6b1a6-966c-41f9-83b1-4230307c9d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b1105-67e5-4799-8f73-b73da416dbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d298b499-ac0c-4fc7-afac-e540346eef65",
   "metadata": {},
   "source": [
    "## Other test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d097ed0-40b4-40ee-be88-5722ecd0e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, create_structured_chat_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3ff353c-dfd4-4f1f-8fd3-4b134442c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "52c780fb-f2f8-4009-842a-0c2e50536173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llmazure.bind_tools(tool)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LlamaIndex\",\n",
    "        func=lambda q: query_engine_with_nl2graphquery.query(q),\n",
    "        description=\"useful for when you want to answer questions about neo4j kg.\",\n",
    "        return_direct=False,\n",
    "    ),\n",
    "]\n",
    "\n",
    "llm_with_tools= llmazure.bind_tools(tools)\n",
    "\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7378a32b-8678-40e3-a233-989f0a604ed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 20:34:50 [WARNING]: Error in retrieving from nl2graphquery: 'NoneType' object has no attribute 'kwargs'\n",
      "2024-02-19 20:34:50 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-19 20:34:51 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "2024-02-19 20:34:51 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='The study concluded that a moderate decrease in body temperature extends lifespan.', source_nodes=[NodeWithScore(node=TextNode(id_='f884aad2-3f5f-4a25-b071-107af19f2732', embedding=None, metadata={'kg_rel_map': {'study': [['AFFIRMS', 'moderate decrease in body temperature extends lifespan'], ['SHOWS', 'neuro-molecular mechanics'], ['FINDS', 'disappearance'], ['REPORTS', 'more than 50% of freshwater lakes'], ['FOUND', '48% are experiencing population declines'], ['PROPOSES', 'school curricula start including'], ['SUGGESTS', 'carbon taxation approaches would be more effective and fairer'], ['INDICATES', 'release of methane from Arctic glacial retreat'], ['REPORTS_ASSEMBLY_OF', '43 diverse Y chromosomes']]}, 'kg_rel_text': [\"['AFFIRMS', 'moderate decrease in body temperature extends lifespan']\", \"['SHOWS', 'neuro-molecular mechanics']\", \"['FINDS', 'disappearance']\", \"['REPORTS', 'more than 50% of freshwater lakes']\", \"['FOUND', '48% are experiencing population declines']\", \"['PROPOSES', 'school curricula start including']\", \"['SUGGESTS', 'carbon taxation approaches would be more effective and fairer']\", \"['INDICATES', 'release of methane from Arctic glacial retreat']\", \"['REPORTS_ASSEMBLY_OF', '43 diverse Y chromosomes']\"], 'kg_schema': {'schema': \"Node properties are the following:\\nEntity {id: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Entity)-[:CHALLENGE]->(:Entity),(:Entity)-[:BE]->(:Entity),(:Entity)-[:PUBLISH]->(:Entity),(:Entity)-[:REACH]->(:Entity),(:Entity)-[:UPGRADES]->(:Entity),(:Entity)-[:STRIKES]->(:Entity),(:Entity)-[:CAUSED]->(:Entity),(:Entity)-[:IS_ABOVE]->(:Entity),(:Entity)-[:RISE]->(:Entity),(:Entity)-[:BANS]->(:Entity),(:Entity)-[:FOUND_TO_HOST]->(:Entity),(:Entity)-[:AFFIRMS]->(:Entity),(:Entity)-[:FOUND]->(:Entity),(:Entity)-[:SUGGESTS]->(:Entity),(:Entity)-[:REPORTS_ASSEMBLY_OF]->(:Entity),(:Entity)-[:FINDS]->(:Entity),(:Entity)-[:PROPOSES]->(:Entity),(:Entity)-[:SHOWS]->(:Entity),(:Entity)-[:REPORTS]->(:Entity),(:Entity)-[:INDICATES]->(:Entity),(:Entity)-[:OCCURS_AT]->(:Entity),(:Entity)-[:REACHES]->(:Entity),(:Entity)-[:SHOWN_TO_POTENTIALLY_BE]->(:Entity),(:Entity)-[:OF_GREY_HAIR]->(:Entity),(:Entity)-[:BECOMES]->(:Entity),(:Entity)-[:LAND_NEAR]->(:Entity),(:Entity)-[:BELIEVED_TO_EXIST]->(:Entity),(:Entity)-[:LOSING]->(:Entity),(:Entity)-[:MATURE_INTO]->(:Entity),(:Entity)-[:SHOW]->(:Entity),(:Entity)-[:LIST]->(:Entity),(:Entity)-[:RELEASE]->(:Entity),(:Entity)-[:USE]->(:Entity),(:Entity)-[:ANNOUNCED_IN]->(:Entity),(:Entity)-[:DISCOVERY_OF]->(:Entity),(:Entity)-[:ENTER]->(:Entity),(:Entity)-[:PASS]->(:Entity),(:Entity)-[:FOUND_TO_HAVE]->(:Entity),(:Entity)-[:CONTRIBUTING_TO]->(:Entity),(:Entity)-[:CAN_BE_TRANSFERRED_BETWEEN]->(:Entity),(:Entity)-[:TRANSFERRED_TO]->(:Entity),(:Entity)-[:COULD_RESULT_IN]->(:Entity),(:Entity)-[:TAKING]->(:Entity),(:Entity)-[:FILMED_ON]->(:Entity),(:Entity)-[:ARE_REPORTED]->(:Entity),(:Entity)-[:AGAINST]->(:Entity),(:Entity)-[:FOR_COMBINATION_WITH]->(:Entity),(:Entity)-[:PRESENT]->(:Entity),(:Entity)-[:REPORT]->(:Entity),(:Entity)-[:CONFIRM]->(:Entity),(:Entity)-[:MAP]->(:Entity),(:Entity)-[:VIEWED_TOGETHER]->(:Entity),(:Entity)-[:TO_DETECT]->(:Entity),(:Entity)-[:CENTER_OF]->(:Entity),(:Entity)-[:PUBLISHED]->(:Entity),(:Entity)-[:PUBLISHED_BY]->(:Entity),(:Entity)-[:SHOWN_TO_OUTPERFORM]->(:Entity),(:Entity)-[:PREVENT]->(:Entity),(:Entity)-[:OUTPERFORM]->(:Entity),(:Entity)-[:ARE]->(:Entity),(:Entity)-[:ADDRESS]->(:Entity),(:Entity)-[:ARE_INCLUDED_IN]->(:Entity),(:Entity)-[:DETECT]->(:Entity),(:Entity)-[:IN_A_SUSTAINABLE_FOOD_SYSTEM_ACHIEVABLE_WITH_FULL_GLOBAL_DEPLOYMENT_OF_DIVERSE_INTERVENTIONS]->(:Entity),(:Entity)-[:WILL_MAKE]->(:Entity),(:Entity)-[:BEGINS]->(:Entity),(:Entity)-[:DYING_HUMANS]->(:Entity),(:Entity)-[:ACHIEVE]->(:Entity),(:Entity)-[:TESTED]->(:Entity),(:Entity)-[:BOOSTS]->(:Entity),(:Entity)-[:SHARE_NOBEL_PRIZE_IN_PHYSIOLOGY_OR_MEDICINE]->(:Entity),(:Entity)-[:DEMONSTRATED]->(:Entity),(:Entity)-[:SPOTTED_IN]->(:Entity),(:Entity)-[:PROPOSE]->(:Entity),(:Entity)-[:FIND]->(:Entity),(:Entity)-[:DEMONSTRATE]->(:Entity),(:Entity)-[:TERMED]->(:Entity),(:Entity)-[:SHARE_NOBEL_PRIZE_IN_PHYSICS]->(:Entity),(:Entity)-[:ANNOUNCES]->(:Entity),(:Entity)-[:REPORT_STUDIES]->(:Entity),(:Entity)-[:FOUND_IN]->(:Entity),(:Entity)-[:EXTENDED_BY]->(:Entity),(:Entity)-[:DESCRIBED_AS]->(:Entity),(:Entity)-[:DECLARED_EXTINCT_BY]->(:Entity),(:Entity)-[:DECLARED_EXTINCT]->(:Entity),(:Entity)-[:IMPROVE]->(:Entity),(:Entity)-[:IDENTIFY]->(:Entity),(:Entity)-[:RECOMMEND]->(:Entity),(:Entity)-[:CAPTURES]->(:Entity),(:Entity)-[:SPOTTED]->(:Entity),(:Entity)-[:IDENTIFIES]->(:Entity),(:Entity)-[:IN]->(:Entity),(:Entity)-[:KEEPING_TRACK_OF]->(:Entity),(:Entity)-[:VIA]->(:Entity),(:Entity)-[:PRESENTED]->(:Entity),(:Entity)-[:BASED_ON]->(:Entity),(:Entity)-[:62_NEW_MOONS]->(:Entity),(:Entity)-[:REQUIRE]->(:Entity),(:Entity)-[:UNKNOWN_ANCIENT_HUMAN_HOMININ]->(:Entity),(:Entity)-[:FIRE]->(:Entity),(:Entity)-[:OCCULTS]->(:Entity),(:Entity)-[:DISCOVER]->(:Entity),(:Entity)-[:CLOSE]->(:Entity),(:Entity)-[:REPORTED]->(:Entity),(:Entity)-[:may practice]->(:Entity),(:Entity)-[:name]->(:Entity),(:Entity)-[:THEORISED]->(:Entity),(:Entity)-[:monitor]->(:Entity),(:Entity)-[:ensure]->(:Entity),(:Entity)-[:CONCLUDES]->(:Entity),(:Entity)-[:is]->(:Entity),(:Entity)-[:HAVE]->(:Entity),(:Entity)-[:check distribute]->(:Entity),(:Entity)-[:prescriptive authority]->(:Entity),(:Entity)-[:must complete]->(:Entity),(:Entity)-[:advise]->(:Entity),(:Entity)-[:act]->(:Entity),(:Entity)-[:LOST]->(:Entity),(:Entity)-[:produced]->(:Entity),(:Entity)-[:seem driving]->(:Entity),(:Entity)-[:currently taking]->(:Entity),(:Entity)-[:involves taking]->(:Entity),(:Entity)-[:HAS_OVERTAKEN]->(:Entity),(:Entity)-[:WERE]->(:Entity),(:Entity)-[:COSTS]->(:Entity),(:Entity)-[:CREATED]->(:Entity),(:Entity)-[:BURIED]->(:Entity),(:Entity)-[:USED]->(:Entity),(:Entity)-[:DEVELOPED]->(:Entity),(:Entity)-[:WATCH_AND_LEARN_FROM]->(:Entity),(:Entity)-[:RECREATE]->(:Entity),(:Entity)-[:STORE]->(:Entity),(:Entity)-[:is part of]->(:Entity),(:Entity)-[:has]->(:Entity),(:Entity)-[:INCLUDE]->(:Entity),(:Entity)-[:OF]->(:Entity),(:Entity)-[:managed]->(:Entity),(:Entity)-[:covered]->(:Entity),(:Entity)-[:indicated]->(:Entity),(:Entity)-[:have]->(:Entity),(:Entity)-[:comprise]->(:Entity),(:Entity)-[:cover]->(:Entity),(:Entity)-[:enable]->(:Entity),(:Entity)-[:negotiate]->(:Entity),(:Entity)-[:collect]->(:Entity),(:Entity)-[:make]->(:Entity),(:Entity)-[:disclose]->(:Entity),(:Entity)-[:resell]->(:Entity),(:Entity)-[:save]->(:Entity),(:Entity)-[:RESULT_IN]->(:Entity),(:Entity)-[:BOOST]->(:Entity),(:Entity)-[:EXTEND]->(:Entity),(:Entity)-[:PROVIDE]->(:Entity),(:Entity)-[:ENABLE]->(:Entity),(:Entity)-[:CREATE]->(:Entity),(:Entity)-[:IS_LAUNCHED]->(:Entity),(:Entity)-[:are often prohibited]->(:Entity),(:Entity)-[:required to complete]->(:Entity),(:Entity)-[:eligible to sit]->(:Entity),(:Entity)-[:may involve]->(:Entity),(:Entity)-[:IS_CONFIRMED]->(:Entity),(:Entity)-[:composed of]->(:Entity),(:Entity)-[:offer]->(:Entity),(:Entity)-[:involves]->(:Entity),(:Entity)-[:EXCEED]->(:Entity),(:Entity)-[:start]->(:Entity),(:Entity)-[:CONFIRMED_AS]->(:Entity),(:Entity)-[:also complete]->(:Entity),(:Entity)-[:THEORISE]->(:Entity),(:Entity)-[:is a]->(:Entity),(:Entity)-[:CALLED]->(:Entity),(:Entity)-[:HOST]->(:Entity),(:Entity)-[:experienced]->(:Entity),(:Entity)-[:impacting]->(:Entity),(:Entity)-[:Parameter Values]->(:Entity),(:Entity)-[:due to]->(:Entity),(:Entity)-[:IS]->(:Entity),(:Entity)-[:FOUNDED_IN]->(:Entity),(:Entity)-[:experienced sudden disruption]->(:Entity),(:Entity)-[:due unexpected shutdown]->(:Entity),(:Entity)-[:Excessive vibrations]->(:Entity),(:Entity)-[:is a publication by]->(:Entity),(:Entity)-[:is an important tool for measuring]->(:Entity),(:Entity)-[:improving level]->(:Entity),(:Entity)-[:If several QMPs applied]->(:Entity),(:Entity)-[:focuses on]->(:Entity),(:Entity)-[:change]->(:Entity),(:Entity)-[:is within]->(:Entity),(:Entity)-[:et al.]->(:Entity),(:Entity)-[:presents]->(:Entity),(:Entity)-[:terms]->(:Entity),(:Entity)-[:measured]->(:Entity),(:Entity)-[:comparing]->(:Entity),(:Entity)-[:determine]->(:Entity),(:Entity)-[:connection]->(:Entity),(:Entity)-[:pointing]->(:Entity),(:Entity)-[:caused]->(:Entity),(:Entity)-[:scientific studies]->(:Entity),(:Entity)-[:published]->(:Entity),(:Entity)-[:mainly focus]->(:Entity),(:Entity)-[:without]->(:Entity),(:Entity)-[:study]->(:Entity),(:Entity)-[:pointed]->(:Entity),(:Entity)-[:conditions]->(:Entity),(:Entity)-[:dealt]->(:Entity),(:Entity)-[:services sector]->(:Entity),(:Entity)-[:comprehensive current issue]->(:Entity),(:Entity)-[:is in]->(:Entity),(:Entity)-[:is considered]->(:Entity),(:Entity)-[:is studied by]->(:Entity),(:Entity)-[:implement]->(:Entity),(:Entity)-[:The H1 hypothesis]->(:Entity),(:Entity)-[:demonstrable large partly medium-sized enterprises]->(:Entity),(:Entity)-[:aims]->(:Entity),(:Entity)-[:used]->(:Entity),(:Entity)-[:shown]->(:Entity),(:Entity)-[:state]->(:Entity),(:Entity)-[:present]->(:Entity),(:Entity)-[:mention]->(:Entity),(:Entity)-[:reduces]->(:Entity),(:Entity)-[:increases]->(:Entity),(:Entity)-[:leads to]->(:Entity),(:Entity)-[:authored]->(:Entity),(:Entity)-[:dealt using]->(:Entity),(:Entity)-[:aim studies mainly]->(:Entity),(:Entity)-[:presentation conceptual models implementation]->(:Entity),(:Entity)-[:without identifying]->(:Entity),(:Entity)-[:use]->(:Entity),(:Entity)-[:focused improving]->(:Entity),(:Entity)-[:organization]->(:Entity),(:Entity)-[:managing improving performance]->(:Entity),(:Entity)-[:None]->(:Entity),(:Entity)-[:studied]->(:Entity),(:Entity)-[:focused attention importance]->(:Entity),(:Entity)-[:widely used in]->(:Entity),(:Entity)-[:aim studies highlight nature advantages]->(:Entity),(:Entity)-[:applied in]->(:Entity),(:Entity)-[:characterized as]->(:Entity),(:Entity)-[:described as]->(:Entity),(:Entity)-[:use Kaizen in]->(:Entity),(:Entity)-[:was issued by]->(:Entity),(:Entity)-[:was issued in]->(:Entity),(:Entity)-[:mentioned]->(:Entity),(:Entity)-[:The purpose of applying]->(:Entity),(:Entity)-[:create]->(:Entity),(:Entity)-[:implementation]->(:Entity),(:Entity)-[:higher satisfaction]->(:Entity),(:Entity)-[:selected techniques use]->(:Entity),(:Entity)-[:competitiveness]->(:Entity),(:Entity)-[:addressed]->(:Entity),(:Entity)-[:investigated]->(:Entity),(:Entity)-[:opposite]->(:Entity),(:Entity)-[:necessary]->(:Entity),(:Entity)-[:applied]->(:Entity),(:Entity)-[:chosen]->(:Entity),(:Entity)-[:identify]->(:Entity),(:Entity)-[:indicative parameter]->(:Entity),(:Entity)-[:creates]->(:Entity),(:Entity)-[:described]->(:Entity),(:Entity)-[:practical use QMPs]->(:Entity),(:Entity)-[:purpose]->(:Entity),(:Entity)-[:analysis]->(:Entity),(:Entity)-[:𝜒𝜒2]->(:Entity),(:Entity)-[:becomes]->(:Entity),(:Entity)-[:underlying]->(:Entity),(:Entity)-[:is used by]->(:Entity),(:Entity)-[:is used in]->(:Entity),(:Entity)-[:has a reliability of]->(:Entity),(:Entity)-[:has a 95 interval estimate lower limit of]->(:Entity),(:Entity)-[:has a 95 interval estimate upper limit of]->(:Entity),(:Entity)-[:is one of]->(:Entity),(:Entity)-[:is used for]->(:Entity),(:Entity)-[:confirmed]->(:Entity),(:Entity)-[:highest lowest value]->(:Entity),(:Entity)-[:high level technology]->(:Entity),(:Entity)-[:bring together]->(:Entity),(:Entity)-[:setting]->(:Entity),(:Entity)-[:use scale QMPs]->(:Entity),(:Entity)-[:partly technology performance industry]->(:Entity),(:Entity)-[:V alue-added indicator Slovak manufacturing enterprises]->(:Entity),(:Entity)-[:uses two approaches]->(:Entity),(:Entity)-[:divided two groups]->(:Entity),(:Entity)-[:predominance domestic capital]->(:Entity),(:Entity)-[:represented 166 enterprises]->(:Entity),(:Entity)-[:published by]->(:Entity),(:Entity)-[:is implemented in]->(:Entity),(:Entity)-[:likes to drive]->(:Entity),(:Entity)-[:plays]->(:Entity),(:Entity)-[:Practice]->(:Entity),(:Entity)-[:like]->(:Entity),(:Entity)-[:made]->(:Entity),(:Entity)-[:progress]->(:Entity),(:Entity)-[:employing]->(:Entity),(:Entity)-[:less]->(:Entity),(:Entity)-[:originated]->(:Entity),(:Entity)-[:involved]->(:Entity),(:Entity)-[:responsibility]->(:Entity),(:Entity)-[:MBNQA]->(:Entity),(:Entity)-[:used term]->(:Entity),(:Entity)-[:reported]->(:Entity),(:Entity)-[:carry responsibility]->(:Entity),(:Entity)-[:associated]->(:Entity),(:Entity)-[:requires]->(:Entity),(:Entity)-[:defines]->(:Entity),(:Entity)-[:deals]->(:Entity),(:Entity)-[:synonymous]->(:Entity),(:Entity)-[:provides]->(:Entity),(:Entity)-[:imagined]->(:Entity),(:Entity)-[:consumers]->(:Entity),(:Entity)-[:observed]->(:Entity),(:Entity)-[:allocates]->(:Entity),(:Entity)-[:revealed]->(:Entity),(:Entity)-[:amounting]->(:Entity),(:Entity)-[:also include]->(:Entity),(:Entity)-[:surfaces]->(:Entity),(:Entity)-[:Some Factors Consideration Developing Techniques Implementation Guidelines]->(:Entity),(:Entity)-[:identified]->(:Entity),(:Entity)-[:set-up]->(:Entity),(:Entity)-[:recommended]->(:Entity),(:Entity)-[:provided]->(:Entity),(:Entity)-[:achieve]->(:Entity),(:Entity)-[:covers]->(:Entity),(:Entity)-[:includes]->(:Entity),(:Entity)-[:able to compare]->(:Entity),(:Entity)-[:take forms]->(:Entity),(:Entity)-[:limited application]->(:Entity),(:Entity)-[:ranking choices]->(:Entity),(:Entity)-[:makes possible]->(:Entity),(:Entity)-[:relates]->(:Entity),(:Entity)-[:used survey]->(:Entity),(:Entity)-[:developed]->(:Entity),(:Entity)-[:designed]->(:Entity),(:Entity)-[:falls in category]->(:Entity),(:Entity)-[:ask]->(:Entity),(:Entity)-[:cause]->(:Entity),(:Entity)-[:categorized]->(:Entity),(:Entity)-[:impossible to verify]->(:Entity),(:Entity)-[:minimize response errors]->(:Entity),(:Entity)-[:described earlier]->(:Entity),(:Entity)-[:reduce magnitude low response rate]->(:Entity),(:Entity)-[:business performance]->(:Entity),(:Entity)-[:stressed]->(:Entity),(:Entity)-[:implemented]->(:Entity),(:Entity)-[:105 3.10.1]->(:Entity),(:Entity)-[:classified]->(:Entity),(:Entity)-[:similar]->(:Entity),(:Entity)-[:follows]->(:Entity),(:Entity)-[:1980 survey]->(:Entity),(:Entity)-[:generally]->(:Entity),(:Entity)-[:largely]->(:Entity),(:Entity)-[:fiercely competitive]->(:Entity),(:Entity)-[:expected]->(:Entity),(:Entity)-[:sought]->(:Entity),(:Entity)-[:fresh start firms]->(:Entity),(:Entity)-[:is the number 1 producer of]->(:Entity),(:Entity)-[:one]->(:Entity),(:Entity)-[:even]->(:Entity),(:Entity)-[:two]->(:Entity),(:Entity)-[:employs]->(:Entity),(:Entity)-[:192]->(:Entity),(:Entity)-[:number]->(:Entity),(:Entity)-[:would set]->(:Entity),(:Entity)-[:initially tried]->(:Entity),(:Entity)-[:techniques]->(:Entity),(:Entity)-[:five]->(:Entity),(:Entity)-[:followed]->(:Entity),(:Entity)-[:would agreement]->(:Entity),(:Entity)-[:employee suggestion]->(:Entity),(:Entity)-[:exert]->(:Entity),(:Entity)-[:relatively new]->(:Entity),(:Entity)-[:known effects]->(:Entity),(:Entity)-[:installed later]->(:Entity),(:Entity)-[:focuses]->(:Entity),(:Entity)-[:collective efforts]->(:Entity),(:Entity)-[:unsuitable]->(:Entity),(:Entity)-[:among techniques]->(:Entity),(:Entity)-[:sophisticated machines]->(:Entity),(:Entity)-[:least aggressively pursued]->(:Entity),(:Entity)-[:recent years]->(:Entity),(:Entity)-[:seems agree]->(:Entity),(:Entity)-[:reported US industries]->(:Entity),(:Entity)-[:80 Installation Including 1996]->(:Entity),(:Entity)-[:Statistical Method]->(:Entity),(:Entity)-[:Customer Survey]->(:Entity),(:Entity)-[:Waste Elimination]->(:Entity),(:Entity)-[:TPM]->(:Entity),(:Entity)-[:Employee Suggestion]->(:Entity),(:Entity)-[:examined]->(:Entity),(:Entity)-[:ends examining]->(:Entity),(:Entity)-[:improve quality position]->(:Entity),(:Entity)-[:has volume number]->(:Entity),(:Entity)-[:has issue number]->(:Entity),(:Entity)-[:has page number]->(:Entity),(:Entity)-[:has author]->(:Entity),(:Entity)-[:has publication date]->(:Entity),(:Entity)-[:'Total quality management world-class quality']->(:Entity),(:Entity)-[:‘Improving odds f TQM’]->(:Entity),(:Entity)-[:is the publication year of]->(:Entity),(:Entity)-[:is an article in]->(:Entity),(:Entity)-[:is on pages]->(:Entity),(:Entity)-[:is written by]->(:Entity),(:Entity)-[:Proceedings of 4th International Conference Factory 2000- Advanced Factory Automation]->(:Entity),(:Entity)-[:published in]->(:Entity),(:Entity)-[:published on]->(:Entity),(:Entity)-[:is pp 6-15]->(:Entity),(:Entity)-[:is pp 19-31]->(:Entity),(:Entity)-[:is pp 81-85]->(:Entity),(:Entity)-[:is pp 97-100]->(:Entity),(:Entity)-[:titled]->(:Entity),(:Entity)-[:mentioned in]->(:Entity),(:Entity)-[:mentioned on page]->(:Entity),(:Entity)-[:mentioned by]->(:Entity),(:Entity)-[:wrote]->(:Entity),(:Entity)-[:TOM Magazine]->(:Entity),(:Entity)-[:pp]->(:Entity),(:Entity)-[:Does TQM impact]->(:Entity),(:Entity)-[:has page range]->(:Entity),(:Entity)-[:Feb. 1992]->(:Entity),(:Entity)-[:B.G Dale 1991]->(:Entity),(:Entity)-[:D. Nadler 1992]->(:Entity),(:Entity)-[:1995]->(:Entity),(:Entity)-[:D. McCabe 1996]->(:Entity),(:Entity)-[:is published by]->(:Entity),(:Entity)-[:is published in]->(:Entity),(:Entity)-[:The cost quality]->(:Entity),(:Entity)-[:Production Inventory Management Journal]->(:Entity),(:Entity)-[:The new manufacturing challenge]->(:Entity),(:Entity)-[:The 10 commandments f quality]->(:Entity),(:Entity)-[:published for]->(:Entity),(:Entity)-[:published as]->(:Entity),(:Entity)-[:published on pages]->(:Entity),(:Entity)-[:conducted]->(:Entity),(:Entity)-[:discussed]->(:Entity),(:Entity)-[:is the publisher of]->(:Entity),(:Entity)-[:is an]->(:Entity),(:Entity)-[:is about]->(:Entity),(:Entity)-[:is on]->(:Entity),(:Entity)-[:was published in]->(:Entity),(:Entity)-[:has pp]->(:Entity),(:Entity)-[:Need radical innovation continuous improvement]->(:Entity),(:Entity)-[:practices]->(:Entity),(:Entity)-[:Maintenace logs shows]->(:Entity),(:Entity)-[:is the year]->(:Entity),(:Entity)-[:improving level of]->(:Entity),(:Entity)-[:is the author of]->(:Entity),(:Entity)-[:focus]->(:Entity),(:Entity)-[:take account]->(:Entity),(:Entity)-[:manufactured in]->(:Entity),(:Entity)-[:emphasize]->(:Entity),(:Entity)-[:improve]->(:Entity),(:Entity)-[:eliminate]->(:Entity),(:Entity)-[:acquire]->(:Entity),(:Entity)-[:mean]->(:Entity),(:Entity)-[:presentation conceptual models]->(:Entity),(:Entity)-[:gain competitive advantage]->(:Entity),(:Entity)-[:dynamic tool]->(:Entity),(:Entity)-[:Airflow Rate]->(:Entity),(:Entity)-[:Power wiring resistance]->(:Entity),(:Entity)-[:Voltage fluctuation]->(:Entity),(:Entity)-[:Ambient Temperature]->(:Entity),(:Entity)-[:Motor surface temperature]->(:Entity),(:Entity)-[:Motor rated power]->(:Entity),(:Entity)-[:Load motor]->(:Entity),(:Entity)-[:User workload]->(:Entity),(:Entity)-[:Voltage stability]->(:Entity),(:Entity)-[:Issue Description]->(:Entity),(:Entity)-[:lead]->(:Entity),(:Entity)-[:gain competitive advantage in terms of]->(:Entity),(:Entity)-[:The purpose research]->(:Entity),(:Entity)-[:may affect]->(:Entity),(:Entity)-[:achieved performance]->(:Entity),(:Entity)-[:highest level officer]->(:Entity),(:Entity)-[:work]->(:Entity),(:Entity)-[:regarded]->(:Entity),(:Entity)-[:said]->(:Entity),(:Entity)-[:gain]->(:Entity),(:Entity)-[:every effort must made]->(:Entity),(:Entity)-[:requiring references documents files]->(:Entity),(:Entity)-[:fall in four factors]->(:Entity),(:Entity)-[:principles]->(:Entity),(:Entity)-[:response errors]->(:Entity),(:Entity)-[:low response rates]->(:Entity),(:Entity)-[:non-response]->(:Entity),(:Entity)-[:grouped questions together]->(:Entity),(:Entity)-[:still low]->(:Entity),(:Entity)-[:Installation Including]->(:Entity),(:Entity)-[:For Each Quality Initiative]->(:Entity),(:Entity)-[:Number cases]->(:Entity),(:Entity)-[:Cellular Layout]->(:Entity),(:Entity)-[:Improving odds of]->(:Entity),(:Entity)-[:shows monthly frequency]->(:Entity),(:Entity)-[:experienced excessive vibrations]->(:Entity),(:Entity)-[:lead to]->(:Entity),(:Entity)-[:services sector analysed]->(:Entity),(:Entity)-[:comprehensive link]->(:Entity),(:Entity)-[:example study]->(:Entity),(:Entity)-[:total quality]->(:Entity),(:Entity)-[:aim studies highlight]->(:Entity)\"}}, excluded_embed_metadata_keys=['kg_rel_map', 'kg_rel_text', 'kg_schema'], excluded_llm_metadata_keys=['kg_rel_map', 'kg_rel_text', 'kg_schema'], relationships={}, text=\"The following are knowledge sequence in max depth 2 in the form of directed graph like:\\n`subject -[predicate]->, object, <-[predicate_next_hop]-, object_next_hop ...` extracted based on key entities as subject:\\n['AFFIRMS', 'moderate decrease in body temperature extends lifespan']\\n['SHOWS', 'neuro-molecular mechanics']\\n['FINDS', 'disappearance']\\n['REPORTS', 'more than 50% of freshwater lakes']\\n['FOUND', '48% are experiencing population declines']\\n['PROPOSES', 'school curricula start including']\\n['SUGGESTS', 'carbon taxation approaches would be more effective and fairer']\\n['INDICATES', 'release of methane from Arctic glacial retreat']\\n['REPORTS_ASSEMBLY_OF', '43 diverse Y chromosomes']\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=None)], metadata={'f884aad2-3f5f-4a25-b071-107af19f2732': {'kg_rel_map': {'study': [['AFFIRMS', 'moderate decrease in body temperature extends lifespan'], ['SHOWS', 'neuro-molecular mechanics'], ['FINDS', 'disappearance'], ['REPORTS', 'more than 50% of freshwater lakes'], ['FOUND', '48% are experiencing population declines'], ['PROPOSES', 'school curricula start including'], ['SUGGESTS', 'carbon taxation approaches would be more effective and fairer'], ['INDICATES', 'release of methane from Arctic glacial retreat'], ['REPORTS_ASSEMBLY_OF', '43 diverse Y chromosomes']]}, 'kg_rel_text': [\"['AFFIRMS', 'moderate decrease in body temperature extends lifespan']\", \"['SHOWS', 'neuro-molecular mechanics']\", \"['FINDS', 'disappearance']\", \"['REPORTS', 'more than 50% of freshwater lakes']\", \"['FOUND', '48% are experiencing population declines']\", \"['PROPOSES', 'school curricula start including']\", \"['SUGGESTS', 'carbon taxation approaches would be more effective and fairer']\", \"['INDICATES', 'release of methane from Arctic glacial retreat']\", \"['REPORTS_ASSEMBLY_OF', '43 diverse Y chromosomes']\"], 'kg_schema': {'schema': \"Node properties are the following:\\nEntity {id: STRING}\\nRelationship properties are the following:\\n\\nThe relationships are the following:\\n(:Entity)-[:CHALLENGE]->(:Entity),(:Entity)-[:BE]->(:Entity),(:Entity)-[:PUBLISH]->(:Entity),(:Entity)-[:REACH]->(:Entity),(:Entity)-[:UPGRADES]->(:Entity),(:Entity)-[:STRIKES]->(:Entity),(:Entity)-[:CAUSED]->(:Entity),(:Entity)-[:IS_ABOVE]->(:Entity),(:Entity)-[:RISE]->(:Entity),(:Entity)-[:BANS]->(:Entity),(:Entity)-[:FOUND_TO_HOST]->(:Entity),(:Entity)-[:AFFIRMS]->(:Entity),(:Entity)-[:FOUND]->(:Entity),(:Entity)-[:SUGGESTS]->(:Entity),(:Entity)-[:REPORTS_ASSEMBLY_OF]->(:Entity),(:Entity)-[:FINDS]->(:Entity),(:Entity)-[:PROPOSES]->(:Entity),(:Entity)-[:SHOWS]->(:Entity),(:Entity)-[:REPORTS]->(:Entity),(:Entity)-[:INDICATES]->(:Entity),(:Entity)-[:OCCURS_AT]->(:Entity),(:Entity)-[:REACHES]->(:Entity),(:Entity)-[:SHOWN_TO_POTENTIALLY_BE]->(:Entity),(:Entity)-[:OF_GREY_HAIR]->(:Entity),(:Entity)-[:BECOMES]->(:Entity),(:Entity)-[:LAND_NEAR]->(:Entity),(:Entity)-[:BELIEVED_TO_EXIST]->(:Entity),(:Entity)-[:LOSING]->(:Entity),(:Entity)-[:MATURE_INTO]->(:Entity),(:Entity)-[:SHOW]->(:Entity),(:Entity)-[:LIST]->(:Entity),(:Entity)-[:RELEASE]->(:Entity),(:Entity)-[:USE]->(:Entity),(:Entity)-[:ANNOUNCED_IN]->(:Entity),(:Entity)-[:DISCOVERY_OF]->(:Entity),(:Entity)-[:ENTER]->(:Entity),(:Entity)-[:PASS]->(:Entity),(:Entity)-[:FOUND_TO_HAVE]->(:Entity),(:Entity)-[:CONTRIBUTING_TO]->(:Entity),(:Entity)-[:CAN_BE_TRANSFERRED_BETWEEN]->(:Entity),(:Entity)-[:TRANSFERRED_TO]->(:Entity),(:Entity)-[:COULD_RESULT_IN]->(:Entity),(:Entity)-[:TAKING]->(:Entity),(:Entity)-[:FILMED_ON]->(:Entity),(:Entity)-[:ARE_REPORTED]->(:Entity),(:Entity)-[:AGAINST]->(:Entity),(:Entity)-[:FOR_COMBINATION_WITH]->(:Entity),(:Entity)-[:PRESENT]->(:Entity),(:Entity)-[:REPORT]->(:Entity),(:Entity)-[:CONFIRM]->(:Entity),(:Entity)-[:MAP]->(:Entity),(:Entity)-[:VIEWED_TOGETHER]->(:Entity),(:Entity)-[:TO_DETECT]->(:Entity),(:Entity)-[:CENTER_OF]->(:Entity),(:Entity)-[:PUBLISHED]->(:Entity),(:Entity)-[:PUBLISHED_BY]->(:Entity),(:Entity)-[:SHOWN_TO_OUTPERFORM]->(:Entity),(:Entity)-[:PREVENT]->(:Entity),(:Entity)-[:OUTPERFORM]->(:Entity),(:Entity)-[:ARE]->(:Entity),(:Entity)-[:ADDRESS]->(:Entity),(:Entity)-[:ARE_INCLUDED_IN]->(:Entity),(:Entity)-[:DETECT]->(:Entity),(:Entity)-[:IN_A_SUSTAINABLE_FOOD_SYSTEM_ACHIEVABLE_WITH_FULL_GLOBAL_DEPLOYMENT_OF_DIVERSE_INTERVENTIONS]->(:Entity),(:Entity)-[:WILL_MAKE]->(:Entity),(:Entity)-[:BEGINS]->(:Entity),(:Entity)-[:DYING_HUMANS]->(:Entity),(:Entity)-[:ACHIEVE]->(:Entity),(:Entity)-[:TESTED]->(:Entity),(:Entity)-[:BOOSTS]->(:Entity),(:Entity)-[:SHARE_NOBEL_PRIZE_IN_PHYSIOLOGY_OR_MEDICINE]->(:Entity),(:Entity)-[:DEMONSTRATED]->(:Entity),(:Entity)-[:SPOTTED_IN]->(:Entity),(:Entity)-[:PROPOSE]->(:Entity),(:Entity)-[:FIND]->(:Entity),(:Entity)-[:DEMONSTRATE]->(:Entity),(:Entity)-[:TERMED]->(:Entity),(:Entity)-[:SHARE_NOBEL_PRIZE_IN_PHYSICS]->(:Entity),(:Entity)-[:ANNOUNCES]->(:Entity),(:Entity)-[:REPORT_STUDIES]->(:Entity),(:Entity)-[:FOUND_IN]->(:Entity),(:Entity)-[:EXTENDED_BY]->(:Entity),(:Entity)-[:DESCRIBED_AS]->(:Entity),(:Entity)-[:DECLARED_EXTINCT_BY]->(:Entity),(:Entity)-[:DECLARED_EXTINCT]->(:Entity),(:Entity)-[:IMPROVE]->(:Entity),(:Entity)-[:IDENTIFY]->(:Entity),(:Entity)-[:RECOMMEND]->(:Entity),(:Entity)-[:CAPTURES]->(:Entity),(:Entity)-[:SPOTTED]->(:Entity),(:Entity)-[:IDENTIFIES]->(:Entity),(:Entity)-[:IN]->(:Entity),(:Entity)-[:KEEPING_TRACK_OF]->(:Entity),(:Entity)-[:VIA]->(:Entity),(:Entity)-[:PRESENTED]->(:Entity),(:Entity)-[:BASED_ON]->(:Entity),(:Entity)-[:62_NEW_MOONS]->(:Entity),(:Entity)-[:REQUIRE]->(:Entity),(:Entity)-[:UNKNOWN_ANCIENT_HUMAN_HOMININ]->(:Entity),(:Entity)-[:FIRE]->(:Entity),(:Entity)-[:OCCULTS]->(:Entity),(:Entity)-[:DISCOVER]->(:Entity),(:Entity)-[:CLOSE]->(:Entity),(:Entity)-[:REPORTED]->(:Entity),(:Entity)-[:may practice]->(:Entity),(:Entity)-[:name]->(:Entity),(:Entity)-[:THEORISED]->(:Entity),(:Entity)-[:monitor]->(:Entity),(:Entity)-[:ensure]->(:Entity),(:Entity)-[:CONCLUDES]->(:Entity),(:Entity)-[:is]->(:Entity),(:Entity)-[:HAVE]->(:Entity),(:Entity)-[:check distribute]->(:Entity),(:Entity)-[:prescriptive authority]->(:Entity),(:Entity)-[:must complete]->(:Entity),(:Entity)-[:advise]->(:Entity),(:Entity)-[:act]->(:Entity),(:Entity)-[:LOST]->(:Entity),(:Entity)-[:produced]->(:Entity),(:Entity)-[:seem driving]->(:Entity),(:Entity)-[:currently taking]->(:Entity),(:Entity)-[:involves taking]->(:Entity),(:Entity)-[:HAS_OVERTAKEN]->(:Entity),(:Entity)-[:WERE]->(:Entity),(:Entity)-[:COSTS]->(:Entity),(:Entity)-[:CREATED]->(:Entity),(:Entity)-[:BURIED]->(:Entity),(:Entity)-[:USED]->(:Entity),(:Entity)-[:DEVELOPED]->(:Entity),(:Entity)-[:WATCH_AND_LEARN_FROM]->(:Entity),(:Entity)-[:RECREATE]->(:Entity),(:Entity)-[:STORE]->(:Entity),(:Entity)-[:is part of]->(:Entity),(:Entity)-[:has]->(:Entity),(:Entity)-[:INCLUDE]->(:Entity),(:Entity)-[:OF]->(:Entity),(:Entity)-[:managed]->(:Entity),(:Entity)-[:covered]->(:Entity),(:Entity)-[:indicated]->(:Entity),(:Entity)-[:have]->(:Entity),(:Entity)-[:comprise]->(:Entity),(:Entity)-[:cover]->(:Entity),(:Entity)-[:enable]->(:Entity),(:Entity)-[:negotiate]->(:Entity),(:Entity)-[:collect]->(:Entity),(:Entity)-[:make]->(:Entity),(:Entity)-[:disclose]->(:Entity),(:Entity)-[:resell]->(:Entity),(:Entity)-[:save]->(:Entity),(:Entity)-[:RESULT_IN]->(:Entity),(:Entity)-[:BOOST]->(:Entity),(:Entity)-[:EXTEND]->(:Entity),(:Entity)-[:PROVIDE]->(:Entity),(:Entity)-[:ENABLE]->(:Entity),(:Entity)-[:CREATE]->(:Entity),(:Entity)-[:IS_LAUNCHED]->(:Entity),(:Entity)-[:are often prohibited]->(:Entity),(:Entity)-[:required to complete]->(:Entity),(:Entity)-[:eligible to sit]->(:Entity),(:Entity)-[:may involve]->(:Entity),(:Entity)-[:IS_CONFIRMED]->(:Entity),(:Entity)-[:composed of]->(:Entity),(:Entity)-[:offer]->(:Entity),(:Entity)-[:involves]->(:Entity),(:Entity)-[:EXCEED]->(:Entity),(:Entity)-[:start]->(:Entity),(:Entity)-[:CONFIRMED_AS]->(:Entity),(:Entity)-[:also complete]->(:Entity),(:Entity)-[:THEORISE]->(:Entity),(:Entity)-[:is a]->(:Entity),(:Entity)-[:CALLED]->(:Entity),(:Entity)-[:HOST]->(:Entity),(:Entity)-[:experienced]->(:Entity),(:Entity)-[:impacting]->(:Entity),(:Entity)-[:Parameter Values]->(:Entity),(:Entity)-[:due to]->(:Entity),(:Entity)-[:IS]->(:Entity),(:Entity)-[:FOUNDED_IN]->(:Entity),(:Entity)-[:experienced sudden disruption]->(:Entity),(:Entity)-[:due unexpected shutdown]->(:Entity),(:Entity)-[:Excessive vibrations]->(:Entity),(:Entity)-[:is a publication by]->(:Entity),(:Entity)-[:is an important tool for measuring]->(:Entity),(:Entity)-[:improving level]->(:Entity),(:Entity)-[:If several QMPs applied]->(:Entity),(:Entity)-[:focuses on]->(:Entity),(:Entity)-[:change]->(:Entity),(:Entity)-[:is within]->(:Entity),(:Entity)-[:et al.]->(:Entity),(:Entity)-[:presents]->(:Entity),(:Entity)-[:terms]->(:Entity),(:Entity)-[:measured]->(:Entity),(:Entity)-[:comparing]->(:Entity),(:Entity)-[:determine]->(:Entity),(:Entity)-[:connection]->(:Entity),(:Entity)-[:pointing]->(:Entity),(:Entity)-[:caused]->(:Entity),(:Entity)-[:scientific studies]->(:Entity),(:Entity)-[:published]->(:Entity),(:Entity)-[:mainly focus]->(:Entity),(:Entity)-[:without]->(:Entity),(:Entity)-[:study]->(:Entity),(:Entity)-[:pointed]->(:Entity),(:Entity)-[:conditions]->(:Entity),(:Entity)-[:dealt]->(:Entity),(:Entity)-[:services sector]->(:Entity),(:Entity)-[:comprehensive current issue]->(:Entity),(:Entity)-[:is in]->(:Entity),(:Entity)-[:is considered]->(:Entity),(:Entity)-[:is studied by]->(:Entity),(:Entity)-[:implement]->(:Entity),(:Entity)-[:The H1 hypothesis]->(:Entity),(:Entity)-[:demonstrable large partly medium-sized enterprises]->(:Entity),(:Entity)-[:aims]->(:Entity),(:Entity)-[:used]->(:Entity),(:Entity)-[:shown]->(:Entity),(:Entity)-[:state]->(:Entity),(:Entity)-[:present]->(:Entity),(:Entity)-[:mention]->(:Entity),(:Entity)-[:reduces]->(:Entity),(:Entity)-[:increases]->(:Entity),(:Entity)-[:leads to]->(:Entity),(:Entity)-[:authored]->(:Entity),(:Entity)-[:dealt using]->(:Entity),(:Entity)-[:aim studies mainly]->(:Entity),(:Entity)-[:presentation conceptual models implementation]->(:Entity),(:Entity)-[:without identifying]->(:Entity),(:Entity)-[:use]->(:Entity),(:Entity)-[:focused improving]->(:Entity),(:Entity)-[:organization]->(:Entity),(:Entity)-[:managing improving performance]->(:Entity),(:Entity)-[:None]->(:Entity),(:Entity)-[:studied]->(:Entity),(:Entity)-[:focused attention importance]->(:Entity),(:Entity)-[:widely used in]->(:Entity),(:Entity)-[:aim studies highlight nature advantages]->(:Entity),(:Entity)-[:applied in]->(:Entity),(:Entity)-[:characterized as]->(:Entity),(:Entity)-[:described as]->(:Entity),(:Entity)-[:use Kaizen in]->(:Entity),(:Entity)-[:was issued by]->(:Entity),(:Entity)-[:was issued in]->(:Entity),(:Entity)-[:mentioned]->(:Entity),(:Entity)-[:The purpose of applying]->(:Entity),(:Entity)-[:create]->(:Entity),(:Entity)-[:implementation]->(:Entity),(:Entity)-[:higher satisfaction]->(:Entity),(:Entity)-[:selected techniques use]->(:Entity),(:Entity)-[:competitiveness]->(:Entity),(:Entity)-[:addressed]->(:Entity),(:Entity)-[:investigated]->(:Entity),(:Entity)-[:opposite]->(:Entity),(:Entity)-[:necessary]->(:Entity),(:Entity)-[:applied]->(:Entity),(:Entity)-[:chosen]->(:Entity),(:Entity)-[:identify]->(:Entity),(:Entity)-[:indicative parameter]->(:Entity),(:Entity)-[:creates]->(:Entity),(:Entity)-[:described]->(:Entity),(:Entity)-[:practical use QMPs]->(:Entity),(:Entity)-[:purpose]->(:Entity),(:Entity)-[:analysis]->(:Entity),(:Entity)-[:𝜒𝜒2]->(:Entity),(:Entity)-[:becomes]->(:Entity),(:Entity)-[:underlying]->(:Entity),(:Entity)-[:is used by]->(:Entity),(:Entity)-[:is used in]->(:Entity),(:Entity)-[:has a reliability of]->(:Entity),(:Entity)-[:has a 95 interval estimate lower limit of]->(:Entity),(:Entity)-[:has a 95 interval estimate upper limit of]->(:Entity),(:Entity)-[:is one of]->(:Entity),(:Entity)-[:is used for]->(:Entity),(:Entity)-[:confirmed]->(:Entity),(:Entity)-[:highest lowest value]->(:Entity),(:Entity)-[:high level technology]->(:Entity),(:Entity)-[:bring together]->(:Entity),(:Entity)-[:setting]->(:Entity),(:Entity)-[:use scale QMPs]->(:Entity),(:Entity)-[:partly technology performance industry]->(:Entity),(:Entity)-[:V alue-added indicator Slovak manufacturing enterprises]->(:Entity),(:Entity)-[:uses two approaches]->(:Entity),(:Entity)-[:divided two groups]->(:Entity),(:Entity)-[:predominance domestic capital]->(:Entity),(:Entity)-[:represented 166 enterprises]->(:Entity),(:Entity)-[:published by]->(:Entity),(:Entity)-[:is implemented in]->(:Entity),(:Entity)-[:likes to drive]->(:Entity),(:Entity)-[:plays]->(:Entity),(:Entity)-[:Practice]->(:Entity),(:Entity)-[:like]->(:Entity),(:Entity)-[:made]->(:Entity),(:Entity)-[:progress]->(:Entity),(:Entity)-[:employing]->(:Entity),(:Entity)-[:less]->(:Entity),(:Entity)-[:originated]->(:Entity),(:Entity)-[:involved]->(:Entity),(:Entity)-[:responsibility]->(:Entity),(:Entity)-[:MBNQA]->(:Entity),(:Entity)-[:used term]->(:Entity),(:Entity)-[:reported]->(:Entity),(:Entity)-[:carry responsibility]->(:Entity),(:Entity)-[:associated]->(:Entity),(:Entity)-[:requires]->(:Entity),(:Entity)-[:defines]->(:Entity),(:Entity)-[:deals]->(:Entity),(:Entity)-[:synonymous]->(:Entity),(:Entity)-[:provides]->(:Entity),(:Entity)-[:imagined]->(:Entity),(:Entity)-[:consumers]->(:Entity),(:Entity)-[:observed]->(:Entity),(:Entity)-[:allocates]->(:Entity),(:Entity)-[:revealed]->(:Entity),(:Entity)-[:amounting]->(:Entity),(:Entity)-[:also include]->(:Entity),(:Entity)-[:surfaces]->(:Entity),(:Entity)-[:Some Factors Consideration Developing Techniques Implementation Guidelines]->(:Entity),(:Entity)-[:identified]->(:Entity),(:Entity)-[:set-up]->(:Entity),(:Entity)-[:recommended]->(:Entity),(:Entity)-[:provided]->(:Entity),(:Entity)-[:achieve]->(:Entity),(:Entity)-[:covers]->(:Entity),(:Entity)-[:includes]->(:Entity),(:Entity)-[:able to compare]->(:Entity),(:Entity)-[:take forms]->(:Entity),(:Entity)-[:limited application]->(:Entity),(:Entity)-[:ranking choices]->(:Entity),(:Entity)-[:makes possible]->(:Entity),(:Entity)-[:relates]->(:Entity),(:Entity)-[:used survey]->(:Entity),(:Entity)-[:developed]->(:Entity),(:Entity)-[:designed]->(:Entity),(:Entity)-[:falls in category]->(:Entity),(:Entity)-[:ask]->(:Entity),(:Entity)-[:cause]->(:Entity),(:Entity)-[:categorized]->(:Entity),(:Entity)-[:impossible to verify]->(:Entity),(:Entity)-[:minimize response errors]->(:Entity),(:Entity)-[:described earlier]->(:Entity),(:Entity)-[:reduce magnitude low response rate]->(:Entity),(:Entity)-[:business performance]->(:Entity),(:Entity)-[:stressed]->(:Entity),(:Entity)-[:implemented]->(:Entity),(:Entity)-[:105 3.10.1]->(:Entity),(:Entity)-[:classified]->(:Entity),(:Entity)-[:similar]->(:Entity),(:Entity)-[:follows]->(:Entity),(:Entity)-[:1980 survey]->(:Entity),(:Entity)-[:generally]->(:Entity),(:Entity)-[:largely]->(:Entity),(:Entity)-[:fiercely competitive]->(:Entity),(:Entity)-[:expected]->(:Entity),(:Entity)-[:sought]->(:Entity),(:Entity)-[:fresh start firms]->(:Entity),(:Entity)-[:is the number 1 producer of]->(:Entity),(:Entity)-[:one]->(:Entity),(:Entity)-[:even]->(:Entity),(:Entity)-[:two]->(:Entity),(:Entity)-[:employs]->(:Entity),(:Entity)-[:192]->(:Entity),(:Entity)-[:number]->(:Entity),(:Entity)-[:would set]->(:Entity),(:Entity)-[:initially tried]->(:Entity),(:Entity)-[:techniques]->(:Entity),(:Entity)-[:five]->(:Entity),(:Entity)-[:followed]->(:Entity),(:Entity)-[:would agreement]->(:Entity),(:Entity)-[:employee suggestion]->(:Entity),(:Entity)-[:exert]->(:Entity),(:Entity)-[:relatively new]->(:Entity),(:Entity)-[:known effects]->(:Entity),(:Entity)-[:installed later]->(:Entity),(:Entity)-[:focuses]->(:Entity),(:Entity)-[:collective efforts]->(:Entity),(:Entity)-[:unsuitable]->(:Entity),(:Entity)-[:among techniques]->(:Entity),(:Entity)-[:sophisticated machines]->(:Entity),(:Entity)-[:least aggressively pursued]->(:Entity),(:Entity)-[:recent years]->(:Entity),(:Entity)-[:seems agree]->(:Entity),(:Entity)-[:reported US industries]->(:Entity),(:Entity)-[:80 Installation Including 1996]->(:Entity),(:Entity)-[:Statistical Method]->(:Entity),(:Entity)-[:Customer Survey]->(:Entity),(:Entity)-[:Waste Elimination]->(:Entity),(:Entity)-[:TPM]->(:Entity),(:Entity)-[:Employee Suggestion]->(:Entity),(:Entity)-[:examined]->(:Entity),(:Entity)-[:ends examining]->(:Entity),(:Entity)-[:improve quality position]->(:Entity),(:Entity)-[:has volume number]->(:Entity),(:Entity)-[:has issue number]->(:Entity),(:Entity)-[:has page number]->(:Entity),(:Entity)-[:has author]->(:Entity),(:Entity)-[:has publication date]->(:Entity),(:Entity)-[:'Total quality management world-class quality']->(:Entity),(:Entity)-[:‘Improving odds f TQM’]->(:Entity),(:Entity)-[:is the publication year of]->(:Entity),(:Entity)-[:is an article in]->(:Entity),(:Entity)-[:is on pages]->(:Entity),(:Entity)-[:is written by]->(:Entity),(:Entity)-[:Proceedings of 4th International Conference Factory 2000- Advanced Factory Automation]->(:Entity),(:Entity)-[:published in]->(:Entity),(:Entity)-[:published on]->(:Entity),(:Entity)-[:is pp 6-15]->(:Entity),(:Entity)-[:is pp 19-31]->(:Entity),(:Entity)-[:is pp 81-85]->(:Entity),(:Entity)-[:is pp 97-100]->(:Entity),(:Entity)-[:titled]->(:Entity),(:Entity)-[:mentioned in]->(:Entity),(:Entity)-[:mentioned on page]->(:Entity),(:Entity)-[:mentioned by]->(:Entity),(:Entity)-[:wrote]->(:Entity),(:Entity)-[:TOM Magazine]->(:Entity),(:Entity)-[:pp]->(:Entity),(:Entity)-[:Does TQM impact]->(:Entity),(:Entity)-[:has page range]->(:Entity),(:Entity)-[:Feb. 1992]->(:Entity),(:Entity)-[:B.G Dale 1991]->(:Entity),(:Entity)-[:D. Nadler 1992]->(:Entity),(:Entity)-[:1995]->(:Entity),(:Entity)-[:D. McCabe 1996]->(:Entity),(:Entity)-[:is published by]->(:Entity),(:Entity)-[:is published in]->(:Entity),(:Entity)-[:The cost quality]->(:Entity),(:Entity)-[:Production Inventory Management Journal]->(:Entity),(:Entity)-[:The new manufacturing challenge]->(:Entity),(:Entity)-[:The 10 commandments f quality]->(:Entity),(:Entity)-[:published for]->(:Entity),(:Entity)-[:published as]->(:Entity),(:Entity)-[:published on pages]->(:Entity),(:Entity)-[:conducted]->(:Entity),(:Entity)-[:discussed]->(:Entity),(:Entity)-[:is the publisher of]->(:Entity),(:Entity)-[:is an]->(:Entity),(:Entity)-[:is about]->(:Entity),(:Entity)-[:is on]->(:Entity),(:Entity)-[:was published in]->(:Entity),(:Entity)-[:has pp]->(:Entity),(:Entity)-[:Need radical innovation continuous improvement]->(:Entity),(:Entity)-[:practices]->(:Entity),(:Entity)-[:Maintenace logs shows]->(:Entity),(:Entity)-[:is the year]->(:Entity),(:Entity)-[:improving level of]->(:Entity),(:Entity)-[:is the author of]->(:Entity),(:Entity)-[:focus]->(:Entity),(:Entity)-[:take account]->(:Entity),(:Entity)-[:manufactured in]->(:Entity),(:Entity)-[:emphasize]->(:Entity),(:Entity)-[:improve]->(:Entity),(:Entity)-[:eliminate]->(:Entity),(:Entity)-[:acquire]->(:Entity),(:Entity)-[:mean]->(:Entity),(:Entity)-[:presentation conceptual models]->(:Entity),(:Entity)-[:gain competitive advantage]->(:Entity),(:Entity)-[:dynamic tool]->(:Entity),(:Entity)-[:Airflow Rate]->(:Entity),(:Entity)-[:Power wiring resistance]->(:Entity),(:Entity)-[:Voltage fluctuation]->(:Entity),(:Entity)-[:Ambient Temperature]->(:Entity),(:Entity)-[:Motor surface temperature]->(:Entity),(:Entity)-[:Motor rated power]->(:Entity),(:Entity)-[:Load motor]->(:Entity),(:Entity)-[:User workload]->(:Entity),(:Entity)-[:Voltage stability]->(:Entity),(:Entity)-[:Issue Description]->(:Entity),(:Entity)-[:lead]->(:Entity),(:Entity)-[:gain competitive advantage in terms of]->(:Entity),(:Entity)-[:The purpose research]->(:Entity),(:Entity)-[:may affect]->(:Entity),(:Entity)-[:achieved performance]->(:Entity),(:Entity)-[:highest level officer]->(:Entity),(:Entity)-[:work]->(:Entity),(:Entity)-[:regarded]->(:Entity),(:Entity)-[:said]->(:Entity),(:Entity)-[:gain]->(:Entity),(:Entity)-[:every effort must made]->(:Entity),(:Entity)-[:requiring references documents files]->(:Entity),(:Entity)-[:fall in four factors]->(:Entity),(:Entity)-[:principles]->(:Entity),(:Entity)-[:response errors]->(:Entity),(:Entity)-[:low response rates]->(:Entity),(:Entity)-[:non-response]->(:Entity),(:Entity)-[:grouped questions together]->(:Entity),(:Entity)-[:still low]->(:Entity),(:Entity)-[:Installation Including]->(:Entity),(:Entity)-[:For Each Quality Initiative]->(:Entity),(:Entity)-[:Number cases]->(:Entity),(:Entity)-[:Cellular Layout]->(:Entity),(:Entity)-[:Improving odds of]->(:Entity),(:Entity)-[:shows monthly frequency]->(:Entity),(:Entity)-[:experienced excessive vibrations]->(:Entity),(:Entity)-[:lead to]->(:Entity),(:Entity)-[:services sector analysed]->(:Entity),(:Entity)-[:comprehensive link]->(:Entity),(:Entity)-[:example study]->(:Entity),(:Entity)-[:total quality]->(:Entity),(:Entity)-[:aim studies highlight]->(:Entity)\"}}})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools[0].run(\"what did the study concluded?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "41d37981-5ee0-4b42-baae-b28495943933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 20:37:45 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI'm sorry, but I need more information about the study you are referring to in order to provide an accurate answer. Could you please provide more details or context about the study?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model gpt-35-turbo-16k. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_toolkits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconversational_retrieval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_conversational_retrieval_agent\n\u001b[0;32m      3\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m create_conversational_retrieval_agent(llm\u001b[38;5;241m=\u001b[39mllmazure, tools\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      4\u001b[0m     tools[\u001b[38;5;241m0\u001b[39m]], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m agent_executor({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat did the study concluded about the lifespan?\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\chains\\base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    361\u001b[0m }\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    364\u001b[0m     inputs,\n\u001b[0;32m    365\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m    366\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[0;32m    367\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[0;32m    368\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\chains\\base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    163\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m--> 164\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    165\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    166\u001b[0m )\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n\u001b[0;32m    168\u001b[0m     final_outputs[RUN_KEY] \u001b[38;5;241m=\u001b[39m RunInfo(run_id\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mrun_id)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\chains\\base.py:440\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[1;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_outputs(outputs)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msave_context(inputs, outputs)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\agents\\openai_functions_agent\\agent_token_buffer_memory.py:62\u001b[0m, in \u001b[0;36mAgentTokenBufferMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Prune buffer if it exceeds max token limit\u001b[39;00m\n\u001b[0;32m     61\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39mmessages\n\u001b[1;32m---> 62\u001b[0m curr_buffer_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mget_num_tokens_from_messages(buffer)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_limit:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m curr_buffer_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_limit:\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:594\u001b[0m, in \u001b[0;36mChatOpenAI.get_num_tokens_from_messages\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    592\u001b[0m     tokens_per_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    595\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    596\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    597\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    598\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for information on how messages are converted to tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    599\u001b[0m     )\n\u001b[0;32m    600\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    601\u001b[0m messages_dict \u001b[38;5;241m=\u001b[39m [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model gpt-35-turbo-16k. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "from langchain.agents.agent_toolkits.conversational_retrieval.openai_functions import create_conversational_retrieval_agent\n",
    "\n",
    "agent_executor = create_conversational_retrieval_agent(llm=llmazure, tools=[\n",
    "    tools[0]], verbose=True)\n",
    "agent_executor({\"input\": \"what did the study concluded about the lifespan?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5bcdd7ae-e994-4868-8769-8675411bf489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-19 20:35:00 [INFO]: HTTP Request: POST https://chat-bot-iot.openai.azure.com//openai/deployments/chat-bot-trail/chat/completions?api-version=2023-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have enough information to answer your question. Could you please provide more context or specify which study you are referring to?\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"what did the study concluded?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea4b0dab-5b23-401e-8699-aeb25e2f2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful manufacturing AI Assistant, you specialize in Quality department but have expertise in general manufacturing\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "    \n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b97d154-e2d9-4e90-b991-840cdc6d2cc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A single string input was passed in, but this chain expects multiple inputs (set()). When a chain expects multiple inputs, please call it by passing in a dictionary, eg `chain({'foo': 1, 'bar': 2})`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agent_executor(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     emit_warning()\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\chains\\base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \n\u001b[0;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    361\u001b[0m }\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    364\u001b[0m     inputs,\n\u001b[0;32m    365\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m    366\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[0;32m    367\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[0;32m    368\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\chains\\base.py:138\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m include_run_info \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_run_info\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    136\u001b[0m return_only_outputs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_only_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 138\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_inputs(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    139\u001b[0m callback_manager \u001b[38;5;241m=\u001b[39m CallbackManager\u001b[38;5;241m.\u001b[39mconfigure(\n\u001b[0;32m    140\u001b[0m     callbacks,\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[0;32m    147\u001b[0m )\n\u001b[0;32m    148\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\chains\\base.py:465\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    463\u001b[0m         _input_keys \u001b[38;5;241m=\u001b[39m _input_keys\u001b[38;5;241m.\u001b[39mdifference(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mmemory_variables)\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_input_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 465\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    466\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA single string input was passed in, but this chain expects \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    467\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple inputs (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_input_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). When a chain expects \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    468\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple inputs, please call it by passing in a dictionary, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    469\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meg `chain(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfoo\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: 1, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: 2})`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    470\u001b[0m         )\n\u001b[0;32m    471\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mlist\u001b[39m(_input_keys)[\u001b[38;5;241m0\u001b[39m]: inputs}\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: A single string input was passed in, but this chain expects multiple inputs (set()). When a chain expects multiple inputs, please call it by passing in a dictionary, eg `chain({'foo': 1, 'bar': 2})`"
     ]
    }
   ],
   "source": [
    "agent_executor(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5dd06-4151-41a3-ab4c-67accf75cec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
