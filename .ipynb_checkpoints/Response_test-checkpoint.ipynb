{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c3d1ee1-3372-4984-979b-b76705b5928b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama_index in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (0.10.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting llama_index\n",
      "  Downloading llama_index-0.10.6-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: llama-index-llms-azure-openai in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: llama-index-graph-stores-neo4j in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: llama-index-embeddings-azure-openai in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.10.5)\n",
      "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.2)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.1)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama_index) (0.1.3)\n",
      "Requirement already satisfied: azure-identity<2.0.0,>=1.15.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-llms-azure-openai) (1.15.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-llms-azure-openai) (0.24.1)\n",
      "Requirement already satisfied: neo4j<6.0.0,>=5.16.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-graph-stores-neo4j) (5.16.0)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.23.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.30.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (42.0.2)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.24.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.26.0)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.1.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (0.5.14)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (2023.12.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.5.8)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.26.3)\n",
      "Requirement already satisfied: openai>=1.1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (1.11.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (2.1.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (0.5.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-core<0.11.0,>=0.10.0->llama_index) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (4.12.3)\n",
      "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (0.0.2)\n",
      "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (1.23.22)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (4.0.1)\n",
      "Requirement already satisfied: pytz in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from neo4j<6.0.0,>=5.16.0->llama-index-graph-stores-neo4j) (2023.3.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (0.17.3)\n",
      "Requirement already satisfied: idna in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpx->llama-index-llms-azure-openai) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.3.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from azure-core<2.0.0,>=1.23.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (2.5)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (1.16.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.16.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx->llama-index-llms-azure-openai) (0.14.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx->llama-index-llms-azure-openai) (3.5.0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2.0.0,>=1.24.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.8.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (23.2)\n",
      "Requirement already satisfied: portalocker<3,>=1.6 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.8.2)\n",
      "Requirement already satisfied: click in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.5.3)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.22 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file<0.2.0,>=0.1.0->llama_index) (1.23.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.26.18)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.0->llama_index) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from tqdm<5.0.0,>=4.66.1->llama-index-core<0.11.0,>=0.10.0->llama_index) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.0->llama_index) (3.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pandas->llama-index-core<0.11.0,>=0.10.0->llama_index) (2023.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (2.21)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from portalocker<3,>=1.6->msal-extensions<2.0.0,>=0.3.0->azure-identity<2.0.0,>=1.15.0->llama-index-llms-azure-openai) (305.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\zz936bx\\anaconda3\\envs\\llm\\lib\\site-packages (from pydantic<3,>=1.9.0->openai>=1.1.0->llama-index-core<0.11.0,>=0.10.0->llama_index) (2.14.6)\n",
      "Downloading llama_index-0.10.6-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: llama_index\n",
      "  Attempting uninstall: llama_index\n",
      "    Found existing installation: llama-index 0.10.5\n",
      "    Uninstalling llama-index-0.10.5:\n",
      "      Successfully uninstalled llama-index-0.10.5\n",
      "Successfully installed llama_index-0.10.6\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade llama_index llama-index-llms-azure-openai llama-index-graph-stores-neo4j llama-index-embeddings-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "823ef567-bc8f-4e2b-92c7-9c5b7c389191",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pwd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutilitiesPOC\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mauths\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m openaisecret, embeddings_secret, adls_secret\n\u001b[0;32m      5\u001b[0m storage_account \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_account_key\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_container\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanufacturing-copilot\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_account\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuwdsrg03rsta07dls01\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive\\Documentos\\Respaldo\\PredatorCODECH\\DataScience\\AI\\Corporate\\Manufacturing-Copilot\\Manu_Copilot\\utilitiesPOC.py:683\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[38;5;66;03m# COMMAND ----------\u001b[39;00m\n\u001b[0;32m    680\u001b[0m \n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# DBTITLE 1,MANUFACTURING COPILOT\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m--> 683\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PyPDFLoader, UnstructuredPDFLoader, OnlinePDFLoader\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter, TextSplitter\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain\\document_loaders\\__init__.py:32\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m document_loaders\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# If not in interactive env, raise warning.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interactive_env():\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_community\\document_loaders\\__init__.py:163\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morg_mode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnstructuredOrgModeLoader\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m    151\u001b[0m     AmazonTextractPDFLoader,\n\u001b[0;32m    152\u001b[0m     MathpixPDFLoader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     UnstructuredPDFLoader,\n\u001b[0;32m    162\u001b[0m )\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpebblo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PebbloSafeLoader\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolars_dataframe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolarsDataFrameLoader\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpowerpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UnstructuredPowerPointLoader\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_community\\document_loaders\\pebblo.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpwd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhttp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPStatus\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pwd'"
     ]
    }
   ],
   "source": [
    "from utilitiesPOC import *\n",
    "\n",
    "from auths import openaisecret, embeddings_secret, adls_secret\n",
    "\n",
    "storage_account =  '{\"storage_account_key\": \"\",\"storage_container\": \"manufacturing-copilot\",\"storage_account\": \"euwdsrg03rsta07dls01\"}'\n",
    "path = \"Documents/Embeddings/\"\n",
    "\n",
    "\n",
    "\n",
    "llmconfig= '{\"mode\": \"langchain\", \"auth\": \"token\", \"model\": \"gpt-35-turbo-16k\", \"deployment_name\": \"mxdrca\",\"api_version\": \"2023-05-15\",\"api_key_secret\":\"\",\"api_endpoint\": \"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "embedconfig= '{\"embedmode\": \"langchain\",\"model\":\"text-embedding-ada-002\",\"deployment_name\":\"azure_embedding\",\"api_key_secret\":\"\",\"embeddingurl\":\"https://usedoai0efaoa03.openai.azure.com/\"}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16c8ade-2780-4e66-ba5d-288c0c652ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %run ./utilitiesPOC\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from utilitiesPOC import *\n",
    "\n",
    "from auths import openaisecret, embeddings_secret, adls_secret\n",
    "\n",
    "storage_account =  '{\"storage_account_key\": \"\",\"storage_container\": \"manufacturing-copilot\",\"storage_account\": \"euwdsrg03rsta07dls01\"}'\n",
    "path = \"Documents/Embeddings/\"\n",
    "\n",
    "\n",
    "\n",
    "llmconfig= '{\"mode\": \"langchain\", \"auth\": \"token\", \"model\": \"gpt-35-turbo-16k\", \"deployment_name\": \"mxdrca\",\"api_version\": \"2023-05-15\",\"api_key_secret\":\"\",\"api_endpoint\": \"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "embedconfig= '{\"embedmode\": \"langchain\",\"model\":\"text-embedding-ada-002\",\"deployment_name\":\"azure_embedding\",\"api_key_secret\":\"\",\"embeddingurl\":\"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "\n",
    "storage_account = json.loads(storage_account)\n",
    "storage_account_key, container, storage_account = storage_account.get('storage_account_key'),storage_account.get('storage_container'),storage_account.get('storage_account')\n",
    "\n",
    "#read from file\n",
    "storage_account_key = adls_secret\n",
    "\n",
    "llmconfig = json.loads(llmconfig)\n",
    "embedconfig = json.loads(embedconfig)\n",
    "\n",
    "embedmode, embedmodel,embed_deployment,embed_key, embed_url = embedconfig.values()\n",
    "mode, auth, model, deployment_name, api_version, api_key_secret, api_endpoint = llmconfig.values()\n",
    "\n",
    "api_key_secret = openaisecret\n",
    "embed_key = embeddings_secret\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#GETTING THE SECRETS\n",
    "api_key = api_key_secret\n",
    "\n",
    "\n",
    "embed_key = embed_key\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Define LLM \n",
    "#DEFINE LLM USING AZURE OPEN AI\n",
    "llmrca = getllm(openai=True,azure =True,mode = mode, auth=auth,model=model,dep_name = deployment_name,api_version= api_version,\n",
    "                     api_key =api_key,api_base = api_endpoint)\n",
    "\n",
    "\n",
    "embeddings_model  = getembeddings(model=embedmodel,dep_name=embed_deployment,api_key=embed_key,api_base=embed_url)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "mount_point = '/mnt/copilotembedds'\n",
    "\n",
    "\n",
    "#@async_retry\n",
    "def read_embeddings(fromadls=False,dbricks=False,**kwargs):\n",
    "    if fromadls:\n",
    "        if dbricks:\n",
    "            #reads it into a dataframe first from the mount point\n",
    "            storage_account=storage_account,container=container,mount_point=mount_point,path=path,storage_account_key=storage_account_key\n",
    "            for mount in dbutils.fs.mounts():\n",
    "                if mount.mountPoint.startswith(mount_point):\n",
    "                    print(\"Unmounting mnt point\")\n",
    "                    dbutils.fs.unmount(mount.mountPoint)\n",
    "            \n",
    "            #mount to path\n",
    "            dbutils.fs.refreshMounts() #refresh mounts to avoid errors\n",
    "\n",
    "            mntfiles = mount_to_local(storage_account, container, mount_point,path, storage_account_key)\n",
    "            #reads into dataframe\n",
    "            adlsembedspath = [e.path for e in dbutils.fs.ls(mntfiles)][-1] #read last file \n",
    "            adlsembedspath= adlsembedspath.replace(\"dbfs:\",\"/dbfs\")\n",
    "            df = pd.read_pickle(adlsembedspath)\n",
    "\n",
    "        else:\n",
    "            #read from adls but not from databricks\n",
    "            from azure.storage.filedatalake import DataLakeServiceClient\n",
    "            storage_account,container,path,storage_account_key=kwargs.get(\"storage_account\"),kwargs.get(\"container\"),kwargs.get(\"path\"),kwargs.get(\"storage_account_key\")\n",
    "            connstr = f\"DefaultEndpointsProtocol=https;AccountName={storage_account};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "            service = DataLakeServiceClient.from_connection_string(conn_str=connstr)\n",
    "            file_system_client = service.get_file_system_client(container)\n",
    "            paths = file_system_client.get_paths(path=path)\n",
    "            files = [p for p in paths if not p.is_directory]\n",
    "            files.sort(key=lambda x: x.last_modified)\n",
    "            last_file = files[-1].name\n",
    "            file_client = file_system_client.get_file_client(last_file)\n",
    "            \n",
    "            # Download the file content and load it as a Python object\n",
    "            download = file_client.download_file()\n",
    "            downloaded_bytes = download.readall()\n",
    "            df = pickle.loads(downloaded_bytes)\n",
    "            #df = pd.read_pickle(data)\n",
    "            \n",
    "    else:\n",
    "        #read from local FS\n",
    "        adlsembedspath = [n for n in os.listdir(path)][-1]\n",
    "        \n",
    "        df = pd.read_pickle(adlsembedspath)\n",
    "    #returns the texts and embeddings\n",
    "    txtlist = [t for t in df.texts]\n",
    "    embeddings = [e for e in df.embeddings]\n",
    "    return txtlist,embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_embeddings_fromfs(path):\n",
    "    #service = DataLakeServiceClient.from_connection_string(conn_str=\"my_connection_string\")\n",
    "\n",
    "    adlsembedspath = [n for n in os.listdir(path)][-1]        \n",
    "    df = pd.read_pickle(path+\"/\"+adlsembedspath)\n",
    "    #returns the texts and embeddings\n",
    "    txtlist = [t for t in df.texts]\n",
    "    embeddings = [e for e in df.embeddings]\n",
    "    return txtlist,embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "loaded_embeddings = read_embeddings(fromadls=True,dbricks=False,storage_account=storage_account,container=container,path=path,storage_account_key=storage_account_key)#read_embeddings_fromfs(path)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import ClassVar\n",
    "import pickle\n",
    "\n",
    "@dataclass \n",
    "class RAG2:\n",
    "    memory_conversation  = None\n",
    "    def __init__(self,llm,graph,type=\"langchain\",embeddings=None,usememory=False):\n",
    "        self.llm = llm\n",
    "        self.graph = graph\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "        self.usememory = usememory\n",
    "\n",
    "    @staticmethod\n",
    "    def memory(type='langchain',memory_key=\"chat_history\",output_key='result',rmessages=True,readmem=None):\n",
    "        if readmem:\n",
    "            if type.lower()==\"langchain\":\n",
    "                from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "                memory = ConversationBufferMemory(\n",
    "                memory_key=memory_key, return_messages=rmessages)\n",
    "                #output_key=output_key)\n",
    "                \n",
    "                readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "\n",
    "                return memory\n",
    "        else: \n",
    "            return None\n",
    "    \n",
    "\n",
    "    def vector_retriever(self,store=\"chroma\",**kwargs):\n",
    "        texts = kwargs.get(\"texts\")\n",
    "        if store.lower()==\"chroma\":\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "\n",
    "            from langchain.chains import RetrievalQA\n",
    "            persist_directory = kwargs.get(\"persist_directory\",\"filesdb\")\n",
    "            search_kwargs = kwargs.get(\"search_kwargs\",{\"k\": 3})\n",
    "            from langchain.vectorstores import Chroma\n",
    "            \n",
    "            db = Chroma.from_documents(texts,self.embeddings,persist_directory= persist_directory)\n",
    "            vec_retriever = db.as_retriever(search_kwargs=search_kwargs)\n",
    "         \n",
    "            return vec_retriever  \n",
    "\n",
    "            \n",
    "        elif store.lower()==\"faiss\":\n",
    "            #to be implemented\n",
    "            from langchain_community.vectorstores import FAISS\n",
    "            from langchain_core.output_parsers import StrOutputParser\n",
    "            from langchain_core.prompts import ChatPromptTemplate\n",
    "            from langchain_core.runnables import RunnablePassthrough\n",
    "            from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "            if kwargs.get(\"text_embeddings\"):\n",
    "                text_embeddings =kwargs.get(\"text_embeddings\")\n",
    "                vectorstore = FAISS.from_embeddings(text_embeddings=list(zip(text_embeddings[0],text_embeddings[1])), embedding = self.embeddings)\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                return retriever\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                template = \"\"\"Answer the question based only on the following context:{context} \n",
    "                Question: {query}\"\"\"\n",
    "                prompt = ChatPromptTemplate.from_template(template)\n",
    "                retrieval_chain = ({\"context\": retriever, \"query\": RunnablePassthrough()} | prompt\n",
    "                                    | self.llm| StrOutputParser())\n",
    "                return retrieval_chain\n",
    "        \n",
    "    def build_chain(self,**kwargs):\n",
    "        #memory = self.memory() if (readmem := self.memory()) else None\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        \n",
    "        #PROMPT TEMPLATE        \n",
    "        from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate,PromptTemplate\n",
    "        #####\n",
    "\n",
    "        memory = self.usememory\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "\n",
    "        if memory:\n",
    "            prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {question}\")\n",
    "        else:\n",
    "            prompt = PromptTemplate(input_variables=[\"question\"],template =\"{question}\")\n",
    "\n",
    "        self.memory_conversation = self.memory(readmem=memory) if not self.memory_conversation else self.memory_conversation\n",
    "\n",
    "        if kwargs.get(\"falkor\"):\n",
    "            from langchain.chains import FalkorDBQAChain\n",
    "            chain = FalkorDBQAChain.from_llm(llm=self.llm, graph=self.graph, verbose=verbose, memory=self.memory_conversation, **kwargs)\n",
    "        else:\n",
    "            if method.lower()==\"graph\":\n",
    "                #general chain for graphs\n",
    "                print(\"building graph chain\")\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                #Prompt template\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "\n",
    "                chain = GraphCypherQAChain.from_llm(llm=self.llm, prompt=prompt,graph=self.graph,validate_cypher = True, \n",
    "                                                    verbose=verbose,memory=self.memory_conversation, promptTemplate=qa_prompt,**kwargs)\n",
    "                chain.input_key = \"question\"\n",
    "\n",
    "            elif method.lower()==\"vector\":\n",
    "                print(\"Building vector chain \")\n",
    "                retriever = kwargs.get(\"vec_retriever\")\n",
    "                from langchain.chains import RetrievalQA\n",
    "                from langchain.chains import ConversationalRetrievalChain\n",
    "                \n",
    "                # template = (\"Combine the chat history and follow up question into \"\n",
    "                # \"a standalone question. Chat History: {chat_history}\"\n",
    "                # \"Follow up question: {query} \")\n",
    "                template = (\"Every time you respond you should start with This is Copilot your AI assistant\")\n",
    "                prompt = PromptTemplate.from_template(template)\n",
    "                \n",
    "\n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,return_source_documents=True,verbose=verbose)\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "                \n",
    "                chain = ConversationalRetrievalChain.from_llm(self.llm,retriever,memory=self.memory_conversation,get_chat_history=lambda h :h,\n",
    "                                                              verbose=verbose,\n",
    "                                                              combine_docs_chain_kwargs={\"prompt\": qa_prompt}) \n",
    "                \n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,\n",
    "                #                                    return_source_documents=True,verbose=verbose,memory=self.memory_conversation,prompt=prompt)\n",
    "\n",
    "        return chain\n",
    "\n",
    "\n",
    "    def get_retriever(self,**kwargs):\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        complexity = kwargs.get(\"complexity\",\"simple\")\n",
    "        \n",
    "        if method.lower()=='graph':\n",
    "            if self.type.lower()==\"langchain\":\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                if complexity.lower()=='simple':\n",
    "                    print(\"SIMPLE RAG LANGCHAIN\")\n",
    "                    if usememory := kwargs.get(\"usememory\") or self.memory_conversation:\n",
    "                        print(\"With Read memory\")\n",
    "                    \n",
    "                    chain = self.build_chain(**kwargs)\n",
    "                    return chain\n",
    "                \n",
    "                    \n",
    "            elif self.type.lower()==\"llama\":\n",
    "                if complexity.lower()=='simple':\n",
    "                    nl2graph = kwargs.get(\"nl2graph\",False)\n",
    "                    print(nl2graph)\n",
    "                    from llama_index.query_engine import RetrieverQueryEngine\n",
    "                    from llama_index.retrievers import KnowledgeGraphRAGRetriever                    \n",
    "                    graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=kwargs.get(\"storage_context\"),\n",
    "                    service_context=kwargs.get(\"service_context\"),\n",
    "                    llm=self.llm,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=nl2graph)\n",
    "                    query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))\n",
    "                    return query_engine\n",
    "\n",
    "                \n",
    "                elif complexity.lower()==\"custom\":\n",
    "                    if self.type.lower()==\"llama\":\n",
    "                        from llama_index import get_response_synthesizer\n",
    "                        from llama_index.query_engine import RetrieverQueryEngine\n",
    "                        # create custom retriever\n",
    "                        vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "                        kg_retriever = KGTableRetriever(index=kg_index, retriever_mode=\"keyword\", include_text=False)\n",
    "                        custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "                        # create response synthesizer\n",
    "                        response_synthesizer = get_response_synthesizer(service_context=service_context,response_mode=\"tree_summarize\")\n",
    "                        custom_query_engine = RetrieverQueryEngine(retriever=custom_retriever,response_synthesizer=response_synthesizer)\n",
    "                        return custom_query_engine\n",
    "                    elif self.type.lower()=='langchain':\n",
    "                        from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "                        simple = self.get_retriever(\"simple\",**kwargs)\n",
    "                        \n",
    "                        tools = [\n",
    "                            Tool(name= \"GraphRAG\",\n",
    "                                 func= simple.run,\n",
    "                                 description= \"Simple RAG from knowledge graph\"),\n",
    "                            #add more RAG tools\n",
    "                        ]\n",
    "        elif method.lower()==\"vector\":\n",
    "            print(\"Getting vector retriever\")\n",
    "            vec_retriever = self.vector_retriever(**kwargs)\n",
    "\n",
    "            chain = self.build_chain(vec_retriever=vec_retriever,**kwargs)\n",
    "            return chain\n",
    "\n",
    "\n",
    "\n",
    "    def response(self,query,complexity=\"simple\",**kwargs):\n",
    "        method = kwargs.get(\"method\",\"vector\")\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "        retriever = self.get_retriever(**kwargs)\n",
    "        try:\n",
    "            if method.lower()==\"vector\":\n",
    "                print(\"I AM USING VECTOR LETS GO\")\n",
    "                result =  retriever({\"question\": query}).get('answer')\n",
    "                \n",
    "            elif method.lower()==\"graph\":\n",
    "                if self.type.lower()=='langchain':\n",
    "                    print('using retriever({\"query\": query}).get(\"answer\")')\n",
    "                    #result = retriever({\"question\": query}).get('answer')#\n",
    "                    result = retriever.invoke({\"question\": query}).get('result') \n",
    "                    pickled_str = pickle.dumps(self.memory_conversation)\n",
    "                elif self.type.lower()=='llama':\n",
    "                    result = retriever.query(query)\n",
    "                    if 'Empty Response' in result.response:\n",
    "                        raise(\"Error empty result or no information found\")\n",
    "                    else:\n",
    "                        return result.response\n",
    "            \n",
    "            if not result or \"I'm sorry, but I don't have\" in result:\n",
    "                print('Not using RAG')\n",
    "                raise (\"Error empty result or no information found in the KG\")\n",
    "            elif \"I'm sorry, but I don't have\" in result:\n",
    "                raise(\"ERROR 404\")\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if self.type.lower()=='langchain':\n",
    "                from langchain.schema import HumanMessage\n",
    "                #message = HumanMessage(content=query)\n",
    "                from langchain.chains import ConversationChain\n",
    "                from langchain.prompts.prompt import PromptTemplate\n",
    "                if self.usememory:\n",
    "                    print(\"Normal LLM response with same memory\")\n",
    "                    prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {query}\")\n",
    "                    chat = ConversationChain(llm=self.llm,verbose=kwargs.get(\"verbose\",True),prompt=prompt,memory=self.memory_conversation ,input_key=\"question\")\n",
    "                    return chat.invoke({\"question\":query}).get(\"response\")\n",
    "                else:\n",
    "                    from langchain.schema import HumanMessage\n",
    "                    message = HumanMessage(content=query)\n",
    "                    return self.llm.invoke([message])\n",
    "\n",
    "            else:\n",
    "                #return response with llama index self.llm(response)\n",
    "                pass\n",
    "            \n",
    "\n",
    "# COMMAND ----------\n",
    "#start graph\n",
    "url =\"neo4j+s://b75e0bd8.databases.neo4j.io\"\n",
    "username =\"neo4j\"\n",
    "password = \"aziMmumvuVru-rgeKuO0zT7mEtTxu29Jgrmp6lFKv0w\"\n",
    "graph_store = get_graphdb(\"langchain\",graphdb = \"neo4j\",url=url,username=username,password=password)\n",
    "\n",
    "\n",
    "#call RAG class initialize vector class \n",
    "rag2 = RAG2\n",
    "vecrag = rag2(llmrca,graph_store,type=\"langchain\",embeddings = embeddings_model,usememory=True)\n",
    "#vectorchain = vecrag.get_retriever(method=\"vector\",text_embeddings = loaded_embeddings,store=\"faiss\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "\n",
    "    # print(vectorchain({\"question\": \"where and why did the production stoppage occurred\"}).get(\"answer\")) #if use memory we call it like this \n",
    "    # print(vectorchain({\"question\": \"what was my previous question?\"}).get(\"answer\"))\n",
    "    # #\n",
    "    print(vecrag.response(query=\"where and why did the production stoppage occurred\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "    print(vecrag.response(query=\"what was my previous question?\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "# COMMAND ----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032827ad-4f2f-4c03-aa24-97404d1cda00",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag2 = RAG2\n",
    "vecrag = rag2(llmrca,graph_store,type=\"langchain\",embeddings = embeddings_model,usememory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f95529-57cb-4674-b450-c501a713a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecrag.response(query=\"where and why did the production stoppage occurred\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "print(vecrag.response(query=\"what was my previous question?\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994bd6f-b814-4eb6-8346-afdb77b59782",
   "metadata": {},
   "source": [
    "### LLama index graph test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e260804f-e0b7-4603-91fb-aacebee1f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass \n",
    "class RAG2:\n",
    "    memory_conversation  = None\n",
    "    def __init__(self,llm,graph,type=\"langchain\",embeddings=None,usememory=False):\n",
    "        self.llm = llm\n",
    "        self.graph = graph\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "        self.usememory = usememory\n",
    "\n",
    "    @staticmethod\n",
    "    def memory(type='langchain',memory_key=\"chat_history\",output_key='result',rmessages=True,readmem=None):\n",
    "        if readmem:\n",
    "            if type.lower()==\"langchain\":\n",
    "                from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "                memory = ConversationBufferMemory(\n",
    "                memory_key=memory_key, return_messages=rmessages)\n",
    "                #output_key=output_key)\n",
    "                \n",
    "                readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "\n",
    "                return memory\n",
    "        else: \n",
    "            return None\n",
    "    \n",
    "\n",
    "    def vector_retriever(self,store=\"chroma\",**kwargs):\n",
    "        texts = kwargs.get(\"texts\")\n",
    "        if store.lower()==\"chroma\":\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "\n",
    "            from langchain.chains import RetrievalQA\n",
    "            persist_directory = kwargs.get(\"persist_directory\",\"filesdb\")\n",
    "            search_kwargs = kwargs.get(\"search_kwargs\",{\"k\": 3})\n",
    "            from langchain.vectorstores import Chroma\n",
    "            \n",
    "            db = Chroma.from_documents(texts,self.embeddings,persist_directory= persist_directory)\n",
    "            vec_retriever = db.as_retriever(search_kwargs=search_kwargs)\n",
    "         \n",
    "            return vec_retriever  \n",
    "\n",
    "            \n",
    "        elif store.lower()==\"faiss\":\n",
    "            #to be implemented\n",
    "            from langchain_community.vectorstores import FAISS\n",
    "            from langchain_core.output_parsers import StrOutputParser\n",
    "            from langchain_core.prompts import ChatPromptTemplate\n",
    "            from langchain_core.runnables import RunnablePassthrough\n",
    "            from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "            if kwargs.get(\"text_embeddings\"):\n",
    "                text_embeddings =kwargs.get(\"text_embeddings\")\n",
    "                vectorstore = FAISS.from_embeddings(text_embeddings=list(zip(text_embeddings[0],text_embeddings[1])), embedding = self.embeddings)\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                return retriever\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                template = \"\"\"Answer the question based only on the following context:{context} \n",
    "                Question: {query}\"\"\"\n",
    "                prompt = ChatPromptTemplate.from_template(template)\n",
    "                retrieval_chain = ({\"context\": retriever, \"query\": RunnablePassthrough()} | prompt\n",
    "                                    | self.llm| StrOutputParser())\n",
    "                return retrieval_chain\n",
    "        \n",
    "    def build_chain(self,**kwargs):\n",
    "        #memory = self.memory() if (readmem := self.memory()) else None\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        \n",
    "        #PROMPT TEMPLATE        \n",
    "        from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate,PromptTemplate\n",
    "\n",
    "        \n",
    "        #####\n",
    "\n",
    "        memory = self.usememory\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "\n",
    "        if memory:\n",
    "            prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {question}\")\n",
    "        else:\n",
    "            prompt = PromptTemplate(input_variables=[\"question\"],template =\"{question}\")\n",
    "\n",
    "        self.memory_conversation = self.memory(readmem=memory) if not self.memory_conversation else self.memory_conversation\n",
    "\n",
    "        if kwargs.get(\"falkor\"):\n",
    "            from langchain.chains import FalkorDBQAChain\n",
    "            chain = FalkorDBQAChain.from_llm(llm=self.llm, graph=self.graph, verbose=verbose, memory=self.memory_conversation, **kwargs)\n",
    "        else:\n",
    "            if method.lower()==\"graph\":\n",
    "                #general chain for graphs\n",
    "                print(\"building graph chain\")\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                #Prompt template\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "\n",
    "                chain = GraphCypherQAChain.from_llm(llm=self.llm, prompt=prompt,graph=self.graph,validate_cypher = True, \n",
    "                                                    verbose=verbose,memory=self.memory_conversation, promptTemplate=qa_prompt,**kwargs)\n",
    "                chain.input_key = \"question\"\n",
    "\n",
    "            elif method.lower()==\"vector\":\n",
    "                print(\"Building vector chain \")\n",
    "                retriever = kwargs.get(\"vec_retriever\")\n",
    "                from langchain.chains import RetrievalQA\n",
    "                from langchain.chains import ConversationalRetrievalChain\n",
    "                \n",
    "                # template = (\"Combine the chat history and follow up question into \"\n",
    "                # \"a standalone question. Chat History: {chat_history}\"\n",
    "                # \"Follow up question: {query} \")\n",
    "                template = (\"Every time you respond you should start with This is Copilot your AI assistant\")\n",
    "                prompt = PromptTemplate.from_template(template)\n",
    "                \n",
    "\n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,return_source_documents=True,verbose=verbose)\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "                \n",
    "                chain = ConversationalRetrievalChain.from_llm(self.llm,retriever,memory=self.memory_conversation,get_chat_history=lambda h :h,\n",
    "                                                              verbose=verbose,\n",
    "                                                              combine_docs_chain_kwargs={\"prompt\": qa_prompt}) \n",
    "                \n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,\n",
    "                #                                    return_source_documents=True,verbose=verbose,memory=self.memory_conversation,prompt=prompt)\n",
    "\n",
    "        return chain\n",
    "\n",
    "\n",
    "    def get_retriever(self,**kwargs):\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        complexity = kwargs.get(\"complexity\",\"simple\")\n",
    "        \n",
    "        if method.lower()=='graph':\n",
    "            if self.type.lower()==\"langchain\":\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                if complexity.lower()=='simple':\n",
    "                    print(\"SIMPLE RAG LANGCHAIN\")\n",
    "                    if usememory := kwargs.get(\"usememory\") or self.memory_conversation:\n",
    "                        print(\"With Read memory\")\n",
    "                    \n",
    "                    chain = self.build_chain(**kwargs)\n",
    "                    return chain\n",
    "                \n",
    "                    \n",
    "            elif self.type.lower()==\"llama\":\n",
    "                if complexity.lower()=='simple':\n",
    "                    nl2graph = kwargs.get(\"nl2graph\",False)\n",
    "                    print(nl2graph)\n",
    "                    from llama_index.query_engine import RetrieverQueryEngine\n",
    "                    from llama_index.retrievers import KnowledgeGraphRAGRetriever                    \n",
    "                    graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=kwargs.get(\"storage_context\"),\n",
    "                    service_context=kwargs.get(\"service_context\"),\n",
    "                    llm=self.llm,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=nl2graph)\n",
    "                    query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))\n",
    "                    return query_engine\n",
    "\n",
    "                \n",
    "                elif complexity.lower()==\"custom\":\n",
    "                    if self.type.lower()==\"llama\":\n",
    "                        from llama_index import get_response_synthesizer\n",
    "                        from llama_index.query_engine import RetrieverQueryEngine\n",
    "                        # create custom retriever\n",
    "                        vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "                        kg_retriever = KGTableRetriever(index=kg_index, retriever_mode=\"keyword\", include_text=False)\n",
    "                        custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "                        # create response synthesizer\n",
    "                        response_synthesizer = get_response_synthesizer(service_context=service_context,response_mode=\"tree_summarize\")\n",
    "                        custom_query_engine = RetrieverQueryEngine(retriever=custom_retriever,response_synthesizer=response_synthesizer)\n",
    "                        return custom_query_engine\n",
    "                    elif self.type.lower()=='langchain':\n",
    "                        from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "                        simple = self.get_retriever(\"simple\",**kwargs)\n",
    "                        \n",
    "                        tools = [\n",
    "                            Tool(name= \"GraphRAG\",\n",
    "                                 func= simple.run,\n",
    "                                 description= \"Simple RAG from knowledge graph\"),\n",
    "                            #add more RAG tools\n",
    "                        ]\n",
    "        elif method.lower()==\"vector\":\n",
    "            print(\"Getting vector retriever\")\n",
    "            vec_retriever = self.vector_retriever(**kwargs)\n",
    "\n",
    "            chain = self.build_chain(vec_retriever=vec_retriever,**kwargs)\n",
    "            return chain\n",
    "\n",
    "\n",
    "\n",
    "    def response(self,query,complexity=\"simple\",**kwargs):\n",
    "        method = kwargs.get(\"method\",\"vector\")\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "        retriever = self.get_retriever(**kwargs)\n",
    "        try:\n",
    "            if method.lower()==\"vector\":\n",
    "                print(\"I AM USING VECTOR LETS GO\")\n",
    "                result =  retriever({\"question\": query}).get('answer')\n",
    "                \n",
    "            elif method.lower()==\"graph\":\n",
    "                if self.type.lower()=='langchain':\n",
    "                    print('using retriever({\"query\": query}).get(\"answer\")')\n",
    "                    #result = retriever({\"question\": query}).get('answer')#\n",
    "                    result = retriever.invoke({\"question\": query}).get('result') \n",
    "                    pickled_str = pickle.dumps(self.memory_conversation)\n",
    "                elif self.type.lower()=='llama':\n",
    "                    result = retriever.query(query)\n",
    "                    if 'Empty Response' in result.response:\n",
    "                        raise(\"Error empty result or no information found\")\n",
    "                    else:\n",
    "                        return result.response\n",
    "            \n",
    "            if not result or \"I'm sorry, but I don't have\" in result:\n",
    "                print('Not using RAG')\n",
    "                raise (\"Error empty result or no information found in the KG\")\n",
    "            elif \"I'm sorry, but I don't have\" in result:\n",
    "                raise(\"ERROR 404\")\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if self.type.lower()=='langchain':\n",
    "                from langchain.schema import HumanMessage\n",
    "                #message = HumanMessage(content=query)\n",
    "                from langchain.chains import ConversationChain\n",
    "                from langchain.prompts.prompt import PromptTemplate\n",
    "                if self.usememory:\n",
    "                    print(\"Normal LLM response with same memory\")\n",
    "                    prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {query}\")\n",
    "                    chat = ConversationChain(llm=self.llm,verbose=kwargs.get(\"verbose\",True),prompt=prompt,memory=self.memory_conversation ,input_key=\"question\")\n",
    "                    return chat.invoke({\"question\":query}).get(\"response\")\n",
    "                else:\n",
    "                    from langchain.schema import HumanMessage\n",
    "                    message = HumanMessage(content=query)\n",
    "                    return self.llm.invoke([message])\n",
    "\n",
    "            else:\n",
    "                #return response with llama index self.llm(response)\n",
    "                pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d65e16c-4c52-4307-abb0-565660ef9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getllm(openai=True, azure=False, mode=\"langchain\",**kwargs):\n",
    "    print(\"Note: to use the LLM use it as msg = HumanMessage(content= your query) \\n llm(messages = [msg]) \")\n",
    "    if openai:\n",
    "        temperature = kwargs.get(\"temperature\",0)\n",
    "        if azure:\n",
    "            print(\"Using Azure OpenAI\")\n",
    "            # from langchain.chat_models import AzureChatOpenAI\n",
    "            from langchain_openai import AzureChatOpenAI\n",
    "            auth = kwargs.get(\"auth\")\n",
    "            if auth.lower()==\"token\":\n",
    "                if mode.lower()==\"langchain\":\n",
    "                    params = {\"model\": kwargs.get(\"model\"),\n",
    "                        \"azure_deployment\": kwargs.get(\"dep_name\"),\n",
    "                        \"openai_api_version\": kwargs.get(\"api_version\"),\n",
    "                        \"temperature\": temperature,\n",
    "                        \"openai_api_key\": kwargs.get(\"api_key\"),\n",
    "                        \"azure_endpoint\": kwargs.get(\"api_base\"),\n",
    "                        \"max_tokens\":kwargs.get(\"max_tokens\",256)}\n",
    "                    llm = AzureChatOpenAI(**params)\n",
    "                elif mode.lower()==\"llama\":\n",
    "                    print(\"Llama's version\")\n",
    "                    #from llama_index.llms import AzureOpenAI\n",
    "                    from llama_index.llms.azure_openai import AzureOpenAI\n",
    "                    import openai\n",
    "                    openai.api_type = \"azure\"\n",
    "                    # openai.api_base =  kwargs.get(\"api_base\") #\"INSERT AZURE API BASE\"\n",
    "                    # openai.api_version = kwargs.get(\"api_version\",\"2023-05-15\")\n",
    "                    \n",
    "                    llm = AzureOpenAI(engine= kwargs.get(\"dep_name\"),temperature=0,model=kwargs.get(\"model\"),\n",
    "                                     azure_deployment =kwargs.get(\"dep_name\"),azure_endpoint = kwargs.get(\"api_base\"),\n",
    "                                     api_version = kwargs.get(\"api_version\",\"2023-05-15\"),api_key= kwargs.get(\"api_key\"))\n",
    "                return llm\n",
    "            elif auth.lower() == \"azure_ad\":\n",
    "                #from azure.identity import DefaultAzureCredential\n",
    "                #credential = DefaultAzureCredential()\n",
    "                #Set the API type to `azure_ad`\n",
    "                #os.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n",
    "               ##Set the API_KEY to the token from the Azure credential\n",
    "                #os.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token            elif auth.lower()==\"azure_ad_provider\":\n",
    "                llm = \"Not implemented\"\n",
    "            elif auth == \"azure ad_provider\":\n",
    "                #managed_identity:\n",
    "                #from azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential   \n",
    "                #credential = ChainedTokenCredential(\n",
    "                #ManagedIdentityCredential(),\n",
    "                #AzureCliCredential())\n",
    "                llm = \"not implemented\"\n",
    "        else:\n",
    "            from langchain_openai import OpenAI\n",
    "            api_base = kwargs.get(\"api_base_url\")\n",
    "            api_key =kwargs.get(\"api_key\",\"NULL\")\n",
    "            llm = OpenAI(\n",
    "                         base_url = api_base,\n",
    "                         openai_api_key = api_key,**kwargs)\n",
    "            return llm \n",
    "    else: #huggingface model\n",
    "        pass    \n",
    "\n",
    "\n",
    "\n",
    "def llm_cost(llm,message) -> \"Total cost in USD\":\n",
    "    from langchain.callbacks import get_openai_callback\n",
    "    with get_openai_callback() as cb:\n",
    "        llm([message])\n",
    "        cost=format(cb.total_cost, '.6f')\n",
    "        print(f\"Total Cost (USD): ${cost}\")\n",
    "    return cost\n",
    "\n",
    "\n",
    "\n",
    "    def get_embeddings(self,documents=None,lib='langchain'):\n",
    "#         if not documents:\n",
    "#             documents = pass\n",
    "        #huggingface or langchain\n",
    "        if lib=='langchain':\n",
    "            from langchain.embeddings import OpenAIEmbeddings\n",
    "            embeddings_model = OpenAIEmbeddings()\n",
    "            embeddings = embeddings_model.embed_documents(documents)\n",
    "            return emebeddings\n",
    "        else:\n",
    "            from langchain.embeddings import HuggingFaceEmbeddings\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=self.huggingembed)\n",
    "            return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a157a285-ee20-4dda-98cc-21029df6c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_graphdb(args,**kwargs):\n",
    "    graphdb = kwargs.get(\"graphdb\")\n",
    "    url,username,password,database = kwargs.get(\"url\"), kwargs.get(\"username\"), kwargs.get(\"password\"), kwargs.get(\"database\")\n",
    "    if args.lower()=='llama':\n",
    "        #from llama_index.storage.storage_context import StorageContext\n",
    "        from llama_index.core import StorageContext\n",
    "        print('Using LLama Index graph class')\n",
    "        if graphdb.lower()=='neo4j' or 'neo4j' in url:\n",
    "            from llama_index.graph_stores.neo4j import Neo4jGraphStore\n",
    "\n",
    "            graph_store = Neo4jGraphStore(\n",
    "                username=username,\n",
    "                password=password,\n",
    "                url=url,\n",
    "                database=database,\n",
    "            )\n",
    "            #build storage context\n",
    "            storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "            return graph_store, storage_context\n",
    "        elif graphdb.lower()=='nebula':\n",
    "            from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "            #TBD get varibales from kwargs\n",
    "            graph_store = NebulaGraphStore(\n",
    "            space_name=kwargs.get(\"space_name\"),\n",
    "            edge_types=kwargs.get(\"edge_types\"),\n",
    "            rel_prop_names=kwargs.get(\"rel_prop_names\"),\n",
    "            tags=kwargs.get(\"tags\"))\n",
    "            storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "            return graph_store, storage_context\n",
    "        \n",
    "        else:\n",
    "            raise(\"Error not implemented for that graph database\")\n",
    "    \n",
    "\n",
    "    elif args.lower()=='langchain':\n",
    "        print('Using langchain graph class')\n",
    "        if graphdb.lower()=='neo4j' or 'neo4j' in url:\n",
    "            from langchain.graphs import Neo4jGraph\n",
    "            graph= Neo4jGraph(\n",
    "               url=url,\n",
    "               username=username,\n",
    "               password=password\n",
    "            )\n",
    "    \n",
    "            return graph\n",
    "        elif graphdb.lower()==\"falkordb\":\n",
    "            print(\"Using falkordb\")\n",
    "            from langchain.graphs import FalkorDBGraph\n",
    "            graph = FalkorDBGraph(database = database,host=url,port=6379, username=username,password = pwd)\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise(\"Error not implemented for that graph database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749eb20-c10e-4c53-ab6b-e8add1cbb6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "51c0c388-b1a0-4fb1-ae85-ce9bbce98394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to use the LLM use it as msg = HumanMessage(content= your query) \n",
      " llm(messages = [msg]) \n",
      "Using Azure OpenAI\n",
      "Note: to use the LLM use it as msg = HumanMessage(content= your query) \n",
      " llm(messages = [msg]) \n",
      "Using Azure OpenAI\n",
      "Llama's version\n"
     ]
    }
   ],
   "source": [
    "#loading \n",
    "llmazure =getllm(openai=True,azure =True,mode = \"langchain\", auth=\"token\",model=\"gpt-35-turbo-16k\",dep_name = \"chat-bot-trail\",            \n",
    "                  api_version= \"2023-05-15\", api_key = \"1bbc7a5ceb0e4677888f22d1eb4c8617\",api_base = \"https://chat-bot-iot.openai.azure.com/\")#([message])\n",
    "#LLAMA'S VERSION\n",
    "llmazure2 =getllm(openai=True,azure =True,mode = \"llama\", auth=\"token\",model=\"gpt-35-turbo-16k\",dep_name = \"chat-bot-trail\",            \n",
    "                  api_version= \"2023-05-15\", api_key = \"1bbc7a5ceb0e4677888f22d1eb4c8617\",api_base = \"https://chat-bot-iot.openai.azure.com/\")#([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "14ddb8fd-c907-4ea3-987a-eb7b1f293439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using langchain graph class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to write data to connection ResolvedIPv4Address(('34.121.155.65', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n",
      "Failed to write data to connection IPv4Address(('b75e0bd8.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLama Index graph class\n"
     ]
    }
   ],
   "source": [
    "#INITIATE THE GRAPH STORE\n",
    "url = \"neo4j+s://b75e0bd8.databases.neo4j.io\"\n",
    "username= \"neo4j\"\n",
    "password = \"aziMmumvuVru-rgeKuO0zT7mEtTxu29Jgrmp6lFKv0w\"\n",
    "graph_store = get_graphdb(\"langchain\",graphdb=\"neo4j\",url=url,username=username,password=password)\n",
    "\n",
    "#GRAPH STORE LLAMA \n",
    "graph_store_llama, storage_context = get_graphdb(\"llama\",graphdb=\"neo4j\",url=url,username=username,password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "45d267e1-325b-443f-a9d6-52ae97922a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag2 = RAG2(llmazure2,graph_store_llama,type=\"llama\")\n",
    "service_context= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d9630d1e-31c2-4967-abed-1c5fc7761072",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=storage_context,\n",
    "                    llm=llmazure2,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=True)\n",
    "#query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "83e46cce-9a6f-4951-ad10-8d894982d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import KnowledgeGraphQueryEngine\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import KnowledgeGraphRAGRetriever\n",
    "\n",
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=storage_context,\n",
    "                    llm=llmazure2,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "84e62cda-4fec-4eba-b306-ebe74ad79771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureOpenAI(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000002BF51BAAF90>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x000002BF50A7EDE0>, completion_to_prompt=<function default_completion_to_prompt at 0x000002BF50AE5D00>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='gpt-35-turbo-16k', temperature=0.0, max_tokens=None, additional_kwargs={}, max_retries=3, timeout=60.0, default_headers=None, reuse_client=True, api_key='1bbc7a5ceb0e4677888f22d1eb4c8617', api_base='https://api.openai.com/v1', api_version='2023-05-15', engine='chat-bot-trail', azure_endpoint='https://chat-bot-iot.openai.azure.com/', azure_deployment='chat-bot-trail', use_azure_ad=False)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmazure2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e5e3d41d-7a75-409a-9917-80451f5bcbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    graph_rag_retriever,\n",
    "    llm = llmazure2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b3a93430-eefd-47e5-8a82-acaa12288645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Empty Response', source_nodes=[], metadata=None)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine.query(\n",
    "    \"tell me about hurricane IAN\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d3d69df8-f956-4e76-9bd9-0873061b10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag_retriever_with_nl2graphquery = KnowledgeGraphRAGRetriever(\n",
    "    storage_context=storage_context,\n",
    "    verbose=True,\n",
    "    with_nl2graphquery=True,\n",
    "    llm = llmazure2\n",
    ")\n",
    "\n",
    "query_engine_with_nl2graphquery = RetrieverQueryEngine.from_args(\n",
    "    graph_rag_retriever_with_nl2graphquery,\n",
    "    llm = llmazure2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7e977b64-12bd-43d3-8982-9540894bc499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='Empty Response', source_nodes=[], metadata=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine_with_nl2graphquery.query(\n",
    "    \"tell me about research\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "932060df-425a-46ce-abb3-32e7cf0e2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add it to langchain tools\n",
    "from llama_index.core.langchain_helpers.agents import (\n",
    "    IndexToolConfig,\n",
    "    LlamaIndexTool,\n",
    ")\n",
    "\n",
    "tool_config = IndexToolConfig(\n",
    "    query_engine=query_engine,\n",
    "    name=f\"KG Index\",\n",
    "    description=f\"useful for when you want to answer queries about neo4j\",\n",
    "    tool_kwargs={\"return_direct\": True},\n",
    ")\n",
    "\n",
    "tool = LlamaIndexTool.from_tool_config(tool_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6f32b41b-e05d-410f-9c5b-11e686a6c78b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but based on the given context information, there is no specific mention or information about researchers. Therefore, I cannot provide any details about researchers.\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.run(\"tell me about researchers\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6d097ed0-40b4-40ee-be88-5722ecd0e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, create_structured_chat_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e3ff353c-dfd4-4f1f-8fd3-4b134442c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "52c780fb-f2f8-4009-842a-0c2e50536173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llmazure.bind_tools(tool)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LlamaIndex\",\n",
    "        func=lambda q: str(query_engine.query(q)),\n",
    "        description=\"useful for when you want to answer questions about neo4j kg.\",\n",
    "        return_direct=True,\n",
    "    ),\n",
    "]\n",
    "\n",
    "llm_with_tools= llmazure.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5bcdd7ae-e994-4868-8769-8675411bf489",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'Unrecognized request argument supplied: tools', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm_with_tools\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4064\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   4059\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4060\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   4061\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4062\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   4063\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 4064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   4065\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   4066\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   4067\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   4068\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:166\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    163\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    165\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    167\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    168\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    169\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    170\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    171\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    172\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    173\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    174\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    175\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:544\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    543\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:408\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    407\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 408\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    409\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    410\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    412\u001b[0m ]\n\u001b[0;32m    413\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 398\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    399\u001b[0m                 m,\n\u001b[0;32m    400\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    401\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    402\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    403\u001b[0m             )\n\u001b[0;32m    404\u001b[0m         )\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:577\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m     )\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    578\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    579\u001b[0m     )\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:438\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    433\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    437\u001b[0m }\n\u001b[1;32m--> 438\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\openai\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:663\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    661\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    662\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 663\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    664\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    665\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    666\u001b[0m             {\n\u001b[0;32m    667\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    668\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    669\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    670\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    671\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    687\u001b[0m             },\n\u001b[0;32m    688\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    689\u001b[0m         ),\n\u001b[0;32m    690\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    691\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    692\u001b[0m         ),\n\u001b[0;32m    693\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    694\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    695\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    696\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\openai\\_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1199\u001b[0m     )\n\u001b[1;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\openai\\_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    890\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    891\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    892\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    893\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    894\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    895\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\openai\\_base_client.py:980\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    977\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    979\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    983\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    984\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    987\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    988\u001b[0m )\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'Unrecognized request argument supplied: tools', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ea4b0dab-5b23-401e-8699-aeb25e2f2ca2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      2\u001b[0m     {\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_scratchpad\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: format_to_openai_tool_messages(\n\u001b[0;32m      5\u001b[0m             x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mintermediate_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      6\u001b[0m         ),\n\u001b[0;32m      7\u001b[0m     }\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;241m|\u001b[39m prompt\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m|\u001b[39m llm_with_tools\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;241m|\u001b[39m OpenAIToolsAgentOutputParser()\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentExecutor\n\u001b[0;32m     15\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(agent\u001b[38;5;241m=\u001b[39magent, tools\u001b[38;5;241m=\u001b[39mtools, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful manufacturing AI Assistant, you specialize in Quality department but have expertise in general manufacturing\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    "\n",
    "    \n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97d154-e2d9-4e90-b991-840cdc6d2cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
