{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3d1ee1-3372-4984-979b-b76705b5928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade llama_index llama-index-llms-azure-openai llama-index-graph-stores-neo4j llama-index-embeddings-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "823ef567-bc8f-4e2b-92c7-9c5b7c389191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilitiesPOC import *\n",
    "\n",
    "from auths import openaisecret, embeddings_secret, adls_secret\n",
    "\n",
    "storage_account =  '{\"storage_account_key\": \"\",\"storage_container\": \"manufacturing-copilot\",\"storage_account\": \"euwdsrg03rsta07dls01\"}'\n",
    "path = \"Documents/Embeddings/\"\n",
    "\n",
    "\n",
    "\n",
    "llmconfig= '{\"mode\": \"langchain\", \"auth\": \"token\", \"model\": \"gpt-35-turbo-16k\", \"deployment_name\": \"mxdrca\",\"api_version\": \"2023-05-15\",\"api_key_secret\":\"\",\"api_endpoint\": \"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "embedconfig= '{\"embedmode\": \"langchain\",\"model\":\"text-embedding-ada-002\",\"deployment_name\":\"azure_embedding\",\"api_key_secret\":\"\",\"embeddingurl\":\"https://usedoai0efaoa03.openai.azure.com/\"}'\n",
    "\n",
    "\n",
    "storage_account = json.loads(storage_account)\n",
    "storage_account_key, container, storage_account = storage_account.get('storage_account_key'),storage_account.get('storage_container'),storage_account.get('storage_account')\n",
    "\n",
    "#read from file\n",
    "storage_account_key = adls_secret\n",
    "\n",
    "llmconfig = json.loads(llmconfig)\n",
    "embedconfig = json.loads(embedconfig)\n",
    "\n",
    "embedmode, embedmodel,embed_deployment,embed_key, embed_url = embedconfig.values()\n",
    "mode, auth, model, deployment_name, api_version, api_key_secret, api_endpoint = llmconfig.values()\n",
    "\n",
    "api_key_secret = openaisecret\n",
    "embed_key = embeddings_secret\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#GETTING THE SECRETS\n",
    "api_key = api_key_secret\n",
    "\n",
    "\n",
    "embed_key = embed_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b16c8ade-2780-4e66-ba5d-288c0c652ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 13:14:10 [INFO]: Request URL: 'https://euwdsrg03rsta07dls01.dfs.core.windows.net/manufacturing-copilot?resource=REDACTED&directory=REDACTED&recursive=REDACTED'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Accept': 'application/json'\n",
      "    'User-Agent': 'azsdk-python-storage-dfs/12.14.0 Python/3.11.0 (Windows-10-10.0.22621-SP0)'\n",
      "    'x-ms-date': 'REDACTED'\n",
      "    'x-ms-client-request-id': 'f0b8282d-d1be-11ee-9a98-141333031892'\n",
      "    'Authorization': 'REDACTED'\n",
      "No body was attached to the request\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to use the LLM use it as msg = HumanMessage(content= your query) \n",
      " llm(messages = [msg]) \n",
      "Using Azure OpenAI\n",
      "Using Azure OpenAI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 13:14:11 [INFO]: Response status: 200\n",
      "Response headers:\n",
      "    'Transfer-Encoding': 'chunked'\n",
      "    'Content-Type': 'application/json;charset=utf-8'\n",
      "    'Server': 'Windows-Azure-HDFS/1.0 Microsoft-HTTPAPI/2.0'\n",
      "    'x-ms-request-id': 'e9086007-101f-0046-24cb-65058e000000'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'x-ms-client-request-id': 'f0b8282d-d1be-11ee-9a98-141333031892'\n",
      "    'Date': 'Thu, 22 Feb 2024 20:14:10 GMT'\n",
      "2024-02-22 13:14:11 [INFO]: Request URL: 'https://euwdsrg03rsta07dls01.blob.core.windows.net/manufacturing-copilot/Documents/Embeddings/embeddings_12022024.pkl'\n",
      "Request method: 'GET'\n",
      "Request headers:\n",
      "    'x-ms-range': 'REDACTED'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'Accept': 'application/xml'\n",
      "    'User-Agent': 'azsdk-python-storage-dfs/12.14.0 Python/3.11.0 (Windows-10-10.0.22621-SP0)'\n",
      "    'x-ms-date': 'REDACTED'\n",
      "    'x-ms-client-request-id': 'f133e319-d1be-11ee-a48b-141333031892'\n",
      "    'Authorization': 'REDACTED'\n",
      "No body was attached to the request\n",
      "2024-02-22 13:14:12 [INFO]: Response status: 206\n",
      "Response headers:\n",
      "    'Content-Length': '73944'\n",
      "    'Content-Type': 'application/octet-stream'\n",
      "    'Content-Range': 'REDACTED'\n",
      "    'Last-Modified': 'Mon, 12 Feb 2024 11:04:17 GMT'\n",
      "    'Accept-Ranges': 'REDACTED'\n",
      "    'ETag': '\"0x8DC2BBA5B6060C1\"'\n",
      "    'Server': 'Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0'\n",
      "    'x-ms-request-id': '54f3ac19-d01e-003b-69cb-6574ad000000'\n",
      "    'x-ms-client-request-id': 'f133e319-d1be-11ee-a48b-141333031892'\n",
      "    'x-ms-version': 'REDACTED'\n",
      "    'x-ms-resource-type': 'REDACTED'\n",
      "    'x-ms-creation-time': 'REDACTED'\n",
      "    'x-ms-blob-content-md5': 'REDACTED'\n",
      "    'x-ms-lease-status': 'REDACTED'\n",
      "    'x-ms-lease-state': 'REDACTED'\n",
      "    'x-ms-blob-type': 'REDACTED'\n",
      "    'Content-Disposition': 'REDACTED'\n",
      "    'x-ms-server-encrypted': 'REDACTED'\n",
      "    'x-ms-owner': 'REDACTED'\n",
      "    'x-ms-group': 'REDACTED'\n",
      "    'x-ms-permissions': 'REDACTED'\n",
      "    'x-ms-acl': 'REDACTED'\n",
      "    'Date': 'Thu, 22 Feb 2024 20:14:10 GMT'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using langchain graph class\n",
      "Getting vector retriever\n",
      "Building vector chain \n",
      "I AM USING VECTOR LETS GO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 13:14:13 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  \n",
      "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "                ----\n",
      "                ﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0001198\n",
      "Issue Description: Variations in the quality and accuracy of produced machined parts on Assembly Line 7\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.51 ohms, Voltage fluctuation : 4%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "                ----\n",
      "                \n",
      "Human: Question:```where and why did the production stoppage occurred```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 13:14:16 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Not using RAG\n",
      "exceptions must derive from BaseException\n",
      "Normal LLM response with same memory\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "3 validation errors for ChatPromptTemplate\nmessages -> 0\n  value is not a valid dict (type=type_error.dict)\nmessages -> 0\n  value is not a valid dict (type=type_error.dict)\nmessages -> 0\n  value is not a valid dict (type=type_error.dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 342\u001b[0m, in \u001b[0;36mRAG2.response\u001b[1;34m(self, query, complexity, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot using RAG\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError empty result or no information found in the KG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm sorry, but I don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n",
      "\u001b[1;31mTypeError\u001b[0m: exceptions must derive from BaseException",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 412\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    406\u001b[0m     \n\u001b[0;32m    407\u001b[0m \n\u001b[0;32m    408\u001b[0m     \u001b[38;5;66;03m# print(vectorchain({\"question\": \"where and why did the production stoppage occurred\"}).get(\"answer\")) #if use memory we call it like this \u001b[39;00m\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# print(vectorchain({\"question\": \"what was my previous question?\"}).get(\"answer\"))\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;66;03m# #\u001b[39;00m\n\u001b[0;32m    411\u001b[0m     vecrag \u001b[38;5;241m=\u001b[39m rag2(llmrca,graph_store,\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain\u001b[39m\u001b[38;5;124m\"\u001b[39m,embeddings \u001b[38;5;241m=\u001b[39m embeddings_model,usememory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28mprint\u001b[39m(vecrag\u001b[38;5;241m.\u001b[39mresponse(query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere and why did the production stoppage occurred\u001b[39m\u001b[38;5;124m\"\u001b[39m,method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m\"\u001b[39m,store\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss\u001b[39m\u001b[38;5;124m\"\u001b[39m,text_embeddings \u001b[38;5;241m=\u001b[39m loaded_embeddings))\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28mprint\u001b[39m(vecrag\u001b[38;5;241m.\u001b[39mresponse(query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat was my previous question?\u001b[39m\u001b[38;5;124m\"\u001b[39m,method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m\"\u001b[39m,store\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss\u001b[39m\u001b[38;5;124m\"\u001b[39m,text_embeddings \u001b[38;5;241m=\u001b[39m loaded_embeddings))\n",
      "Cell \u001b[1;32mIn[3], line 375\u001b[0m, in \u001b[0;36mRAG2.response\u001b[1;34m(self, query, complexity, **kwargs)\u001b[0m\n\u001b[0;32m    369\u001b[0m general_user_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion:```\u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    370\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    371\u001b[0m             SystemMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(general_system_template),\n\u001b[0;32m    372\u001b[0m             HumanMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(general_user_template)\n\u001b[0;32m    373\u001b[0m ]\n\u001b[1;32m--> 375\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate(messages\u001b[38;5;241m=\u001b[39m[messages])\n\u001b[0;32m    377\u001b[0m chat \u001b[38;5;241m=\u001b[39m ConversationChain(llm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm,verbose\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m),prompt\u001b[38;5;241m=\u001b[39mprompt,memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_conversation ,get_chat_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m h :h,)\u001b[38;5;66;03m#input_key=\"question\")\u001b[39;00m\n\u001b[0;32m    378\u001b[0m chat\u001b[38;5;241m.\u001b[39minput_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\langchain_core\\load\\serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32m~\\.conda\\envs\\copilotrca\\Lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 3 validation errors for ChatPromptTemplate\nmessages -> 0\n  value is not a valid dict (type=type_error.dict)\nmessages -> 0\n  value is not a valid dict (type=type_error.dict)\nmessages -> 0\n  value is not a valid dict (type=type_error.dict)"
     ]
    }
   ],
   "source": [
    "\n",
    "# DBTITLE 1,Define LLM \n",
    "#DEFINE LLM USING AZURE OPEN AI\n",
    "llmrca = getllm(openai=True,azure =True,mode = mode, auth=auth,model=model,dep_name = deployment_name,api_version= api_version,\n",
    "                     api_key =api_key,api_base = api_endpoint)\n",
    "\n",
    "\n",
    "embeddings_model  = getembeddings(model=embedmodel,dep_name=embed_deployment,api_key=embed_key,api_base=embed_url)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "mount_point = '/mnt/copilotembedds'\n",
    "\n",
    "\n",
    "#@async_retry\n",
    "def read_embeddings(fromadls=False,dbricks=False,**kwargs):\n",
    "    if fromadls:\n",
    "        if dbricks:\n",
    "            #reads it into a dataframe first from the mount point\n",
    "            storage_account=storage_account,container=container,mount_point=mount_point,path=path,storage_account_key=storage_account_key\n",
    "            for mount in dbutils.fs.mounts():\n",
    "                if mount.mountPoint.startswith(mount_point):\n",
    "                    print(\"Unmounting mnt point\")\n",
    "                    dbutils.fs.unmount(mount.mountPoint)\n",
    "            \n",
    "            #mount to path\n",
    "            dbutils.fs.refreshMounts() #refresh mounts to avoid errors\n",
    "\n",
    "            mntfiles = mount_to_local(storage_account, container, mount_point,path, storage_account_key)\n",
    "            #reads into dataframe\n",
    "            adlsembedspath = [e.path for e in dbutils.fs.ls(mntfiles)][-1] #read last file \n",
    "            adlsembedspath= adlsembedspath.replace(\"dbfs:\",\"/dbfs\")\n",
    "            df = pd.read_pickle(adlsembedspath)\n",
    "\n",
    "        else:\n",
    "            #read from adls but not from databricks\n",
    "            from azure.storage.filedatalake import DataLakeServiceClient\n",
    "            storage_account,container,path,storage_account_key=kwargs.get(\"storage_account\"),kwargs.get(\"container\"),kwargs.get(\"path\"),kwargs.get(\"storage_account_key\")\n",
    "            connstr = f\"DefaultEndpointsProtocol=https;AccountName={storage_account};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "            service = DataLakeServiceClient.from_connection_string(conn_str=connstr)\n",
    "            file_system_client = service.get_file_system_client(container)\n",
    "            paths = file_system_client.get_paths(path=path)\n",
    "            files = [p for p in paths if not p.is_directory]\n",
    "            files.sort(key=lambda x: x.last_modified)\n",
    "            last_file = files[-1].name\n",
    "            file_client = file_system_client.get_file_client(last_file)\n",
    "            \n",
    "            # Download the file content and load it as a Python object\n",
    "            download = file_client.download_file()\n",
    "            downloaded_bytes = download.readall()\n",
    "            df = pickle.loads(downloaded_bytes)\n",
    "            #df = pd.read_pickle(data)\n",
    "            \n",
    "    else:\n",
    "        #read from local FS\n",
    "        adlsembedspath = [n for n in os.listdir(path)][-1]\n",
    "        \n",
    "        df = pd.read_pickle(adlsembedspath)\n",
    "    #returns the texts and embeddings\n",
    "    txtlist = [t for t in df.texts]\n",
    "    embeddings = [e for e in df.embeddings]\n",
    "    return txtlist,embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_embeddings_fromfs(path):\n",
    "    #service = DataLakeServiceClient.from_connection_string(conn_str=\"my_connection_string\")\n",
    "\n",
    "    adlsembedspath = [n for n in os.listdir(path)][-1]        \n",
    "    df = pd.read_pickle(path+\"/\"+adlsembedspath)\n",
    "    #returns the texts and embeddings\n",
    "    txtlist = [t for t in df.texts]\n",
    "    embeddings = [e for e in df.embeddings]\n",
    "    return txtlist,embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "loaded_embeddings = read_embeddings(fromadls=True,dbricks=False,storage_account=storage_account,container=container,path=path,storage_account_key=storage_account_key)#read_embeddings_fromfs(path)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import ClassVar\n",
    "import pickle\n",
    "\n",
    "@dataclass \n",
    "class RAG2:\n",
    "    memory_conversation  = None\n",
    "    def __init__(self,llm,graph,type=\"langchain\",embeddings=None,usememory=False):\n",
    "        self.llm = llm\n",
    "        self.graph = graph\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "        self.usememory = usememory\n",
    "\n",
    "    @staticmethod\n",
    "    def memory(type='langchain',memory_key=\"chat_history\",output_key='result',rmessages=True,readmem=None):\n",
    "        if readmem:\n",
    "            if type.lower()==\"langchain\":\n",
    "                from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "                memory = ConversationBufferMemory(\n",
    "                memory_key=memory_key, return_messages=rmessages)\n",
    "                #output_key=output_key)\n",
    "                \n",
    "                readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "\n",
    "                return memory\n",
    "        else: \n",
    "            return None\n",
    "    \n",
    "\n",
    "    def vector_retriever(self,store=\"chroma\",**kwargs):\n",
    "        texts = kwargs.get(\"texts\")\n",
    "        if store.lower()==\"chroma\":\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "\n",
    "            from langchain.chains import RetrievalQA\n",
    "            persist_directory = kwargs.get(\"persist_directory\",\"filesdb\")\n",
    "            search_kwargs = kwargs.get(\"search_kwargs\",{\"k\": 3})\n",
    "            from langchain.vectorstores import Chroma\n",
    "            \n",
    "            db = Chroma.from_documents(texts,self.embeddings,persist_directory= persist_directory)\n",
    "            vec_retriever = db.as_retriever(search_kwargs=search_kwargs)\n",
    "         \n",
    "            return vec_retriever  \n",
    "\n",
    "            \n",
    "        elif store.lower()==\"faiss\":\n",
    "            #to be implemented\n",
    "            from langchain_community.vectorstores import FAISS\n",
    "            from langchain_core.output_parsers import StrOutputParser\n",
    "            from langchain_core.prompts import ChatPromptTemplate\n",
    "            from langchain_core.runnables import RunnablePassthrough\n",
    "            from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "            if kwargs.get(\"text_embeddings\"):\n",
    "                text_embeddings =kwargs.get(\"text_embeddings\")\n",
    "                vectorstore = FAISS.from_embeddings(text_embeddings=list(zip(text_embeddings[0],text_embeddings[1])), embedding = self.embeddings)\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                return retriever\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                template = \"\"\"Answer the question based only on the following context:{context} \n",
    "                Question: {query}\"\"\"\n",
    "                prompt = ChatPromptTemplate.from_template(template)\n",
    "                retrieval_chain = ({\"context\": retriever, \"query\": RunnablePassthrough()} | prompt\n",
    "                                    | self.llm| StrOutputParser())\n",
    "                return retrieval_chain\n",
    "        \n",
    "    def build_chain(self,**kwargs):\n",
    "        #memory = self.memory() if (readmem := self.memory()) else None\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        \n",
    "        #PROMPT TEMPLATE        \n",
    "        from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate,PromptTemplate\n",
    "        #####\n",
    "\n",
    "        memory = self.usememory\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "\n",
    "        if memory:\n",
    "            prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {question}\")\n",
    "        else:\n",
    "            prompt = PromptTemplate(input_variables=[\"question\"],template =\"{question}\")\n",
    "\n",
    "        self.memory_conversation = self.memory(readmem=memory) if not self.memory_conversation else self.memory_conversation\n",
    "\n",
    "        if kwargs.get(\"falkor\"):\n",
    "            from langchain.chains import FalkorDBQAChain\n",
    "            chain = FalkorDBQAChain.from_llm(llm=self.llm, graph=self.graph, verbose=verbose, memory=self.memory_conversation, **kwargs)\n",
    "        else:\n",
    "            if method.lower()==\"graph\":\n",
    "                #general chain for graphs\n",
    "                print(\"building graph chain\")\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                #Prompt template\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "\n",
    "                chain = GraphCypherQAChain.from_llm(llm=self.llm, prompt=prompt,graph=self.graph,validate_cypher = True, \n",
    "                                                    verbose=verbose,memory=self.memory_conversation, promptTemplate=qa_prompt,**kwargs)\n",
    "                chain.input_key = \"question\"\n",
    "\n",
    "            elif method.lower()==\"vector\":\n",
    "                print(\"Building vector chain \")\n",
    "                retriever = kwargs.get(\"vec_retriever\")\n",
    "                from langchain.chains import RetrievalQA\n",
    "                from langchain.chains import ConversationalRetrievalChain\n",
    "                \n",
    "                # template = (\"Combine the chat history and follow up question into \"\n",
    "                # \"a standalone question. Chat History: {chat_history}\"\n",
    "                # \"Follow up question: {query} \")\n",
    "                template = (\"Every time you respond you should start with This is Copilot your AI assistant\")\n",
    "                prompt = PromptTemplate.from_template(template)\n",
    "                \n",
    "\n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,return_source_documents=True,verbose=verbose)\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                self.qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "                \n",
    "                chain = ConversationalRetrievalChain.from_llm(self.llm,retriever,memory=self.memory_conversation,get_chat_history=lambda h :h,\n",
    "                                                              verbose=verbose,\n",
    "                                                              combine_docs_chain_kwargs={\"prompt\": self.qa_prompt}) \n",
    "                \n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,\n",
    "                #                                    return_source_documents=True,verbose=verbose,memory=self.memory_conversation,prompt=prompt)\n",
    "\n",
    "        return chain\n",
    "\n",
    "    def get_agent_executor(self,**kwargs):\n",
    "        #combines vector and graph retrievers\n",
    "        vector = self.get_retriever(method=\"graph\")\n",
    "        pass\n",
    "\n",
    "\n",
    "    def get_retriever(self,**kwargs):\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        complexity = kwargs.get(\"complexity\",\"simple\")\n",
    "        \n",
    "        if method.lower()=='graph':\n",
    "            if self.type.lower()==\"langchain\":\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                if complexity.lower()=='simple':\n",
    "                    print(\"SIMPLE RAG LANGCHAIN\")\n",
    "                    if usememory := kwargs.get(\"usememory\") or self.memory_conversation:\n",
    "                        print(\"With Read memory\")\n",
    "                    \n",
    "                    chain = self.build_chain(**kwargs)\n",
    "                    return chain\n",
    "                \n",
    "                    \n",
    "            elif self.type.lower()==\"llama\":\n",
    "                if complexity.lower()=='simple':\n",
    "                    nl2graph = kwargs.get(\"nl2graph\",False)\n",
    "                    print(nl2graph)\n",
    "                    from llama_index.query_engine import RetrieverQueryEngine\n",
    "                    from llama_index.retrievers import KnowledgeGraphRAGRetriever                    \n",
    "                    graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=kwargs.get(\"storage_context\"),\n",
    "                    service_context=kwargs.get(\"service_context\"),\n",
    "                    llm=self.llm,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=nl2graph)\n",
    "                    query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))\n",
    "                    return query_engine\n",
    "\n",
    "                \n",
    "                elif complexity.lower()==\"custom\":\n",
    "                    if self.type.lower()==\"llama\":\n",
    "                        from llama_index import get_response_synthesizer\n",
    "                        from llama_index.query_engine import RetrieverQueryEngine\n",
    "                        # create custom retriever\n",
    "                        vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "                        kg_retriever = KGTableRetriever(index=kg_index, retriever_mode=\"keyword\", include_text=False)\n",
    "                        custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "                        # create response synthesizer\n",
    "                        response_synthesizer = get_response_synthesizer(service_context=service_context,response_mode=\"tree_summarize\")\n",
    "                        custom_query_engine = RetrieverQueryEngine(retriever=custom_retriever,response_synthesizer=response_synthesizer)\n",
    "                        return custom_query_engine\n",
    "                    elif self.type.lower()=='langchain':\n",
    "                        from langchain.agents import initialize_agent, Tool\n",
    "                        #combine vector and graph retrievers\n",
    "                        simple = self.get_retriever(\"simple\",**kwargs)\n",
    "                        \n",
    "                        tools = [\n",
    "                            Tool(name= \"GraphRAG\",\n",
    "                                 func= simple.run,\n",
    "                                 description= \"Simple RAG from knowledge graph\"),\n",
    "                            #add more RAG tools\n",
    "                        ]\n",
    "        elif method.lower()==\"vector\":\n",
    "            print(\"Getting vector retriever\")\n",
    "            vec_retriever = self.vector_retriever(**kwargs)\n",
    "\n",
    "            chain = self.build_chain(vec_retriever=vec_retriever,**kwargs)\n",
    "            return chain\n",
    "\n",
    "\n",
    "\n",
    "    def response(self,query,complexity=\"simple\",**kwargs):\n",
    "        method = kwargs.get(\"method\",\"vector\")\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "        retriever = self.get_retriever(**kwargs)\n",
    "        try:\n",
    "            if method.lower()==\"vector\":\n",
    "                print(\"I AM USING VECTOR LETS GO\")\n",
    "                result =  retriever({\"question\": query}).get('answer')\n",
    "                \n",
    "            elif method.lower()==\"graph\":\n",
    "                if self.type.lower()=='langchain':\n",
    "                    print('using retriever({\"query\": query}).get(\"answer\")')\n",
    "                    #result = retriever({\"question\": query}).get('answer')#\n",
    "                    result = retriever.invoke({\"question\": query}).get('result') \n",
    "                    pickled_str = pickle.dumps(self.memory_conversation)\n",
    "                elif self.type.lower()=='llama':\n",
    "                    result = retriever.query(query)\n",
    "                    if 'Empty Response' in result.response:\n",
    "                        raise(\"Error empty result or no information found\")\n",
    "                    else:\n",
    "                        return result.response\n",
    "            \n",
    "            if not result or \"I'm sorry, but I don't have\" or \"I apologize\" in result:\n",
    "                print('Not using RAG')\n",
    "                raise (\"Error empty result or no information found in the KG\")\n",
    "            elif \"I'm sorry, but I don't have\" in result:\n",
    "                raise(\"ERROR 404\")\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if self.type.lower()=='langchain':\n",
    "                from langchain.schema import HumanMessage\n",
    "                #message = HumanMessage(content=query)\n",
    "                from langchain.chains import ConversationChain\n",
    "                from langchain.prompts.prompt import PromptTemplate\n",
    "                if self.usememory:\n",
    "                    print(\"Normal LLM response with same memory\")\n",
    "                    #prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {query}\")\n",
    "                    from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "                    from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate,PromptTemplate\n",
    "\n",
    "                    general_system_template = r\"\"\" \n",
    "                    Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                    Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                    you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                    ----\n",
    "                    {context}\n",
    "                    ----\n",
    "                    \"\"\"\n",
    "                    general_user_template = \"Question:```{question}```\"\n",
    "                    messages = [\n",
    "                                SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                                HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                    ]\n",
    "\n",
    "                    prompt = ChatPromptTemplate(messages=[messages])\n",
    "\n",
    "                    chat = ConversationChain(llm=self.llm,verbose=kwargs.get(\"verbose\",True),prompt=prompt,memory=self.memory_conversation ,get_chat_history=lambda h :h,)#input_key=\"question\")\n",
    "                    chat.input_key = \"question\"\n",
    "                    return chat.invoke({\"question\":query}).get(\"response\")\n",
    "                else:\n",
    "                    from langchain.schema import HumanMessage\n",
    "                    message = HumanMessage(content=query)\n",
    "                    return self.llm.invoke([message])\n",
    "\n",
    "            else:\n",
    "                #return response with llama index self.llm(response)\n",
    "                pass\n",
    "            \n",
    "\n",
    "# COMMAND ----------\n",
    "#start graph\n",
    "url =\"neo4j+s://b75e0bd8.databases.neo4j.io\"\n",
    "username =\"neo4j\"\n",
    "password = \"aziMmumvuVru-rgeKuO0zT7mEtTxu29Jgrmp6lFKv0w\"\n",
    "graph_store = get_graphdb(\"langchain\",graphdb = \"neo4j\",url=url,username=username,password=password)\n",
    "\n",
    "\n",
    "#call RAG class initialize vector class \n",
    "rag2 = RAG2\n",
    "vecrag = rag2(llmrca,graph_store,type=\"langchain\",embeddings = embeddings_model,usememory=True)\n",
    "#vectorchain = vecrag.get_retriever(method=\"vector\",text_embeddings = loaded_embeddings,store=\"faiss\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "\n",
    "    # print(vectorchain({\"question\": \"where and why did the production stoppage occurred\"}).get(\"answer\")) #if use memory we call it like this \n",
    "    # print(vectorchain({\"question\": \"what was my previous question?\"}).get(\"answer\"))\n",
    "    # #\n",
    "    vecrag = rag2(llmrca,graph_store,type=\"langchain\",embeddings = embeddings_model,usememory=True)\n",
    "    print(vecrag.response(query=\"where and why did the production stoppage occurred\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "    print(vecrag.response(query=\"what was my previous question?\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "# COMMAND ----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032827ad-4f2f-4c03-aa24-97404d1cda00",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag2 = RAG2\n",
    "vecrag = rag2(llmrca,graph_store,type=\"langchain\",embeddings = embeddings_model,usememory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f95529-57cb-4674-b450-c501a713a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vecrag.response(query=\"where and why did the production stoppage occurred\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "print(vecrag.response(query=\"what was my previous question?\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e29698d9-c3c0-49c5-b5bc-05c9cc9d4318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vector retriever\n",
      "Building vector chain \n",
      "I AM USING VECTOR LETS GO\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "[HumanMessage(content='where and why did the production stoppage occurred'), AIMessage(content='Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a thorough analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, the presence of dust accumulation and the frequency of maintenance logs could also be contributing factors. A detailed investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.'), HumanMessage(content='what was my previous question?'), AIMessage(content='I apologize, but I am an AI assistant and do not have the capability to remember previous questions. Is there anything else I can assist you with?')]\n",
      "Follow Up Input: how many tickets are there?\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 12:37:31 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 12:37:31 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/azure_embedding/embeddings?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem:  \n",
      "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
      "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
      "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
      "                ----\n",
      "                ﻿Incident Ticket ID: INC0003557\n",
      "Issue Description: A Sudden and unplanned production stoppage occurred on Assembly Line 3 impacting the deliverables\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 2 mm thickness, Lubrication frequency : 100 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0007654\n",
      "Issue Description: Production Line 2 experienced a sudden disruption during the morning shift due to unexpected shutdown of Benchmill impacting the production schedule.\n",
      "Parameter Values: Airflow RAirflow Rate : 70 CFM, Ambient Temperature : 35 degree celsius, Motor surface temperature : 80 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hoursate\n",
      "\n",
      "﻿Incident Ticket ID: INC0004823\n",
      "Issue Description: Excessive vibrations or unusual noise occurred on Production Line 11 and lead to poor product quality.\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  90% , User workload : 82%, Voltage stability : 5%, Power wiring resistance : 0.01 ohms, Voltage fluctuation : 1%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "\n",
      "﻿Incident Ticket ID: INC0001198\n",
      "Issue Description: Variations in the quality and accuracy of produced machined parts on Assembly Line 7\n",
      "Parameter Values: Airflow Rate : 70 CFM, Ambient Temperature : 22 degree celsius, Motor surface temperature : 50 degree celsius, Motor rated power : 1.5 HP,\n",
      "Load on motor :  70% , User workload : 60%, Voltage stability : 5%, Power wiring resistance : 0.51 ohms, Voltage fluctuation : 4%, \n",
      "Maintenace logs shows monthly frequency, Dust accumulation : 0.5 mm thickness, Lubrication frequency : 50 hours\n",
      "                ----\n",
      "                \n",
      "Human: Question:```How many incident tickets are there?```\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 12:37:31 [INFO]: HTTP Request: POST https://usedoai0efaoa03.openai.azure.com//openai/deployments/mxdrca/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "There are 4 incident tickets.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='where and why did the production stoppage occurred'), AIMessage(content='Based on the provided incident tickets, the production stoppage occurred on Assembly Line 3. The root cause of the stoppage can be attributed to a sudden and unplanned event. To determine the exact reason for the stoppage, a thorough analysis using Root Cause Analysis (RCA) and Failure Mode and Effects Analysis (FMEA) would be required. However, some potential factors that could have contributed to the stoppage include issues with motor performance, such as high motor surface temperature and load on the motor, as well as potential voltage stability and power wiring resistance issues. Additionally, the presence of dust accumulation and the frequency of maintenance logs could also be contributing factors. A detailed investigation using RCA and FMEA methodologies would be necessary to identify the specific cause of the production stoppage.'), HumanMessage(content='what was my previous question?'), AIMessage(content='I apologize, but I am an AI assistant and do not have the capability to remember previous questions. Is there anything else I can assist you with?'), HumanMessage(content='how many tickets are there?'), AIMessage(content='There are 4 incident tickets.')]), return_messages=True, memory_key='chat_history')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vecrag.response(query=\"how many tickets are there?\",method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings))\n",
    "vecrag.memory_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3f3aa-0cbe-4a44-ace6-0f2e12b4a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector\n",
    "vectorchain= vecrag.get_retriever(method=\"vector\",store=\"faiss\",text_embeddings = loaded_embeddings)\n",
    "## graoh CHAIN \n",
    "graphchain= vecrag.get_retriever(method=\"graph\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6991a7-573e-4060-b94a-b5312907937b",
   "metadata": {},
   "source": [
    "Testing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61e05bd-2d29-4d4f-8513-597a3055ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorchain.invoke({\"question\":\"How many tickets are there?\"}).get(\"answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c691db-5002-42ee-bd21-02664bd71074",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphchain.invoke({\"question\":\"what did the study concluded\"}).get(\"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a69ac14-c683-446d-a8bb-3bead5a61510",
   "metadata": {},
   "source": [
    "## Langchain tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f06435e-7b5b-4161-bea8-08047cfa8b51",
   "metadata": {},
   "source": [
    "To combine vector retriever and Knowledge Graphs we can use langchain tools and an agent executor which combines it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c88e66-6bbc-45ba-85a9-19d50fd8ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, create_structured_chat_agent\n",
    "\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da30b2-6611-4fbb-829c-2653dc6603ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding vector retriever as tool\n",
    "\n",
    "\n",
    "\n",
    "toolslgc = [\n",
    "    \n",
    "    Tool(\n",
    "        name=\"copilot_retriever\",\n",
    "        func=lambda q:vectorchain.invoke({\"question\":q}).get(\"answer\"),\n",
    "        description=\"useful for when you want to answer questions about RCA documents and tickets using FAISS.\",\n",
    "        return_direct=False,\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Copilot_Knowledge\",\n",
    "        func=lambda q:graphchain.invoke({\"question\":q}).get(\"result\"),\n",
    "        description=\"useful for when you want to answer questions about neo4j kg using langchain.\",\n",
    "        return_direct=False,\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "llm_with_tools= llmrca.bind_tools(toolslgc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42899724-1336-443a-8889-fa8a20be9ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(toolslgc[0].run(\"what is the first incident ticket about?\"))\n",
    "print(toolslgc[1].run(\"what did the study concluded?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3af6041-20e8-477f-ad71-885d7eef7cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a86dc3-f437-4811-aa7c-96f37e5ee284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "agent = create_openai_tools_agent(llmazure, toolslgc, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=toolslgc, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45ccc5-e6c0-44e3-9441-6d97aab79dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "querytemplate= \"try using the functions Copilot_Knowledge first to answer the users query then the copilot retriever, this is the query:{question}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318c3a35-3c2d-4317-9653-5758b6a9858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS ANSWER SHOULD COME FROM THE GRAPH\n",
    "agent_executor.invoke({\"input\": \n",
    "                       querytemplate.format(question=\"tell me everything you know about\")}).get(\"output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12c371e-03fc-4170-954e-544302fb0270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS ANSWER SHOULD COME FROM THE VECTOR\n",
    "agent_executor.invoke({\"input\": \n",
    "                       querytemplate.format(question=\"Give me an RCA on ticket 1\")}).get(\"output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994bd6f-b814-4eb6-8346-afdb77b59782",
   "metadata": {},
   "source": [
    "### LLama index graph test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e260804f-e0b7-4603-91fb-aacebee1f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass \n",
    "class RAG2:\n",
    "    memory_conversation  = None\n",
    "    def __init__(self,llm,graph,type=\"langchain\",embeddings=None,usememory=False):\n",
    "        self.llm = llm\n",
    "        self.graph = graph\n",
    "        self.type = type\n",
    "        self.embeddings = embeddings\n",
    "        self.usememory = usememory\n",
    "\n",
    "    @staticmethod\n",
    "    def memory(type='langchain',memory_key=\"chat_history\",output_key='result',rmessages=True,readmem=None):\n",
    "        if readmem:\n",
    "            if type.lower()==\"langchain\":\n",
    "                from langchain.memory import ConversationBufferMemory, ReadOnlySharedMemory\n",
    "                memory = ConversationBufferMemory(\n",
    "                memory_key=memory_key, return_messages=rmessages)\n",
    "                #output_key=output_key)\n",
    "                \n",
    "                readonlymemory = ReadOnlySharedMemory(memory=memory)\n",
    "\n",
    "                return memory\n",
    "        else: \n",
    "            return None\n",
    "    \n",
    "\n",
    "    def vector_retriever(self,store=\"chroma\",**kwargs):\n",
    "        texts = kwargs.get(\"texts\")\n",
    "        if store.lower()==\"chroma\":\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "\n",
    "            from langchain.chains import RetrievalQA\n",
    "            persist_directory = kwargs.get(\"persist_directory\",\"filesdb\")\n",
    "            search_kwargs = kwargs.get(\"search_kwargs\",{\"k\": 3})\n",
    "            from langchain.vectorstores import Chroma\n",
    "            \n",
    "            db = Chroma.from_documents(texts,self.embeddings,persist_directory= persist_directory)\n",
    "            vec_retriever = db.as_retriever(search_kwargs=search_kwargs)\n",
    "         \n",
    "            return vec_retriever  \n",
    "\n",
    "            \n",
    "        elif store.lower()==\"faiss\":\n",
    "            #to be implemented\n",
    "            from langchain_community.vectorstores import FAISS\n",
    "            from langchain_core.output_parsers import StrOutputParser\n",
    "            from langchain_core.prompts import ChatPromptTemplate\n",
    "            from langchain_core.runnables import RunnablePassthrough\n",
    "            from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "            if not self.embeddings:\n",
    "                self.embeddings = getembeddings(model=kwargs.get(\"model\"),dep_name = kwargs.get(\"dep_name\"),api_key = kwargs.get(\"api_key\"),api_base= kwargs.get(\"api_base\"))\n",
    "            \n",
    "            if kwargs.get(\"text_embeddings\"):\n",
    "                text_embeddings =kwargs.get(\"text_embeddings\")\n",
    "                vectorstore = FAISS.from_embeddings(text_embeddings=list(zip(text_embeddings[0],text_embeddings[1])), embedding = self.embeddings)\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                return retriever\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                vectorstore = FAISS.from_texts(texts, embedding=OpenAIEmbeddings())\n",
    "                retriever = vectorstore.as_retriever()\n",
    "                template = \"\"\"Answer the question based only on the following context:{context} \n",
    "                Question: {query}\"\"\"\n",
    "                prompt = ChatPromptTemplate.from_template(template)\n",
    "                retrieval_chain = ({\"context\": retriever, \"query\": RunnablePassthrough()} | prompt\n",
    "                                    | self.llm| StrOutputParser())\n",
    "                return retrieval_chain\n",
    "        \n",
    "    def build_chain(self,**kwargs):\n",
    "        #memory = self.memory() if (readmem := self.memory()) else None\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        \n",
    "        #PROMPT TEMPLATE        \n",
    "        from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate,PromptTemplate\n",
    "\n",
    "        \n",
    "        #####\n",
    "\n",
    "        memory = self.usememory\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "\n",
    "        if memory:\n",
    "            prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {question}\")\n",
    "        else:\n",
    "            prompt = PromptTemplate(input_variables=[\"question\"],template =\"{question}\")\n",
    "\n",
    "        self.memory_conversation = self.memory(readmem=memory) if not self.memory_conversation else self.memory_conversation\n",
    "\n",
    "        if kwargs.get(\"falkor\"):\n",
    "            from langchain.chains import FalkorDBQAChain\n",
    "            chain = FalkorDBQAChain.from_llm(llm=self.llm, graph=self.graph, verbose=verbose, memory=self.memory_conversation, **kwargs)\n",
    "        else:\n",
    "            if method.lower()==\"graph\":\n",
    "                #general chain for graphs\n",
    "                print(\"building graph chain\")\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                #Prompt template\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "\n",
    "                chain = GraphCypherQAChain.from_llm(llm=self.llm, prompt=prompt,graph=self.graph,validate_cypher = True, \n",
    "                                                    verbose=verbose,memory=self.memory_conversation, promptTemplate=qa_prompt,**kwargs)\n",
    "                chain.input_key = \"question\"\n",
    "\n",
    "            elif method.lower()==\"vector\":\n",
    "                print(\"Building vector chain \")\n",
    "                retriever = kwargs.get(\"vec_retriever\")\n",
    "                from langchain.chains import RetrievalQA\n",
    "                from langchain.chains import ConversationalRetrievalChain\n",
    "                \n",
    "                # template = (\"Combine the chat history and follow up question into \"\n",
    "                # \"a standalone question. Chat History: {chat_history}\"\n",
    "                # \"Follow up question: {query} \")\n",
    "                template = (\"Every time you respond you should start with This is Copilot your AI assistant\")\n",
    "                prompt = PromptTemplate.from_template(template)\n",
    "                \n",
    "\n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,return_source_documents=True,verbose=verbose)\n",
    "                        \n",
    "                general_system_template = r\"\"\" \n",
    "                Given a specific context, please give a short answer to the question talking like a professional Manufacturing AI Assistant.\n",
    "                Your answers should be concise, precise and should easily demonstrate you are an expert. Remember you are also a specialist in Root Cause Analysis and FMEA\n",
    "                you can perform RCA and FMEA basing yourself on the reference values retrieved and comparing them to the incidents. \n",
    "                ----\n",
    "                {context}\n",
    "                ----\n",
    "                \"\"\"\n",
    "                general_user_template = \"Question:```{question}```\"\n",
    "                messages = [\n",
    "                            SystemMessagePromptTemplate.from_template(general_system_template),\n",
    "                            HumanMessagePromptTemplate.from_template(general_user_template)\n",
    "                ]\n",
    "                qa_prompt = ChatPromptTemplate.from_messages( messages )\n",
    "                \n",
    "                chain = ConversationalRetrievalChain.from_llm(self.llm,retriever,memory=self.memory_conversation,get_chat_history=lambda h :h,\n",
    "                                                              verbose=verbose,\n",
    "                                                              combine_docs_chain_kwargs={\"prompt\": qa_prompt}) \n",
    "                \n",
    "                #chain = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever,\n",
    "                #                                    return_source_documents=True,verbose=verbose,memory=self.memory_conversation,prompt=prompt)\n",
    "\n",
    "        return chain\n",
    "\n",
    "\n",
    "    def get_retriever(self,**kwargs):\n",
    "        method = kwargs.get(\"method\",\"graph\")\n",
    "        complexity = kwargs.get(\"complexity\",\"simple\")\n",
    "        \n",
    "        if method.lower()=='graph':\n",
    "            if self.type.lower()==\"langchain\":\n",
    "                from langchain.chains import GraphCypherQAChain\n",
    "                if complexity.lower()=='simple':\n",
    "                    print(\"SIMPLE RAG LANGCHAIN\")\n",
    "                    if usememory := kwargs.get(\"usememory\") or self.memory_conversation:\n",
    "                        print(\"With Read memory\")\n",
    "                    \n",
    "                    chain = self.build_chain(**kwargs)\n",
    "                    return chain\n",
    "                \n",
    "                    \n",
    "            elif self.type.lower()==\"llama\":\n",
    "                if complexity.lower()=='simple':\n",
    "                    nl2graph = kwargs.get(\"nl2graph\",False)\n",
    "                    print(nl2graph)\n",
    "                    from llama_index.query_engine import RetrieverQueryEngine\n",
    "                    from llama_index.retrievers import KnowledgeGraphRAGRetriever                    \n",
    "                    graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=kwargs.get(\"storage_context\"),\n",
    "                    service_context=kwargs.get(\"service_context\"),\n",
    "                    llm=self.llm,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=nl2graph)\n",
    "                    query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))\n",
    "                    return query_engine\n",
    "\n",
    "                \n",
    "                elif complexity.lower()==\"custom\":\n",
    "                    if self.type.lower()==\"llama\":\n",
    "                        from llama_index import get_response_synthesizer\n",
    "                        from llama_index.query_engine import RetrieverQueryEngine\n",
    "                        # create custom retriever\n",
    "                        vector_retriever = VectorIndexRetriever(index=vector_index)\n",
    "                        kg_retriever = KGTableRetriever(index=kg_index, retriever_mode=\"keyword\", include_text=False)\n",
    "                        custom_retriever = CustomRetriever(vector_retriever, kg_retriever)\n",
    "                        # create response synthesizer\n",
    "                        response_synthesizer = get_response_synthesizer(service_context=service_context,response_mode=\"tree_summarize\")\n",
    "                        custom_query_engine = RetrieverQueryEngine(retriever=custom_retriever,response_synthesizer=response_synthesizer)\n",
    "                        return custom_query_engine\n",
    "                    elif self.type.lower()=='langchain':\n",
    "                        from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "                        simple = self.get_retriever(\"simple\",**kwargs)\n",
    "                        \n",
    "                        tools = [\n",
    "                            Tool(name= \"GraphRAG\",\n",
    "                                 func= simple.run,\n",
    "                                 description= \"Simple RAG from knowledge graph\"),\n",
    "                            #add more RAG tools\n",
    "                        ]\n",
    "        elif method.lower()==\"vector\":\n",
    "            print(\"Getting vector retriever\")\n",
    "            vec_retriever = self.vector_retriever(**kwargs)\n",
    "\n",
    "            chain = self.build_chain(vec_retriever=vec_retriever,**kwargs)\n",
    "            return chain\n",
    "\n",
    "\n",
    "\n",
    "    def response(self,query,complexity=\"simple\",**kwargs):\n",
    "        method = kwargs.get(\"method\",\"vector\")\n",
    "        verbose = kwargs.get(\"verbose\",True)\n",
    "        retriever = self.get_retriever(**kwargs)\n",
    "        try:\n",
    "            if method.lower()==\"vector\":\n",
    "                print(\"I AM USING VECTOR LETS GO\")\n",
    "                result =  retriever({\"question\": query}).get('answer')\n",
    "                \n",
    "            elif method.lower()==\"graph\":\n",
    "                if self.type.lower()=='langchain':\n",
    "                    print('using retriever({\"query\": query}).get(\"answer\")')\n",
    "                    #result = retriever({\"question\": query}).get('answer')#\n",
    "                    result = retriever.invoke({\"question\": query}).get('result') \n",
    "                    pickled_str = pickle.dumps(self.memory_conversation)\n",
    "                elif self.type.lower()=='llama':\n",
    "                    result = retriever.query(query)\n",
    "                    if 'Empty Response' in result.response:\n",
    "                        raise(\"Error empty result or no information found\")\n",
    "                    else:\n",
    "                        return result.response\n",
    "            \n",
    "            if not result or \"I'm sorry, but I don't have\" in result:\n",
    "                print('Not using RAG')\n",
    "                raise (\"Error empty result or no information found in the KG\")\n",
    "            elif \"I'm sorry, but I don't have\" in result:\n",
    "                raise(\"ERROR 404\")\n",
    "            else:\n",
    "                return result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if self.type.lower()=='langchain':\n",
    "                from langchain.schema import HumanMessage\n",
    "                #message = HumanMessage(content=query)\n",
    "                from langchain.chains import ConversationChain\n",
    "                from langchain.prompts.prompt import PromptTemplate\n",
    "                if self.usememory:\n",
    "                    print(\"Normal LLM response with same memory\")\n",
    "                    from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "                    system_message = SystemMessage(content=\"Ypu are a helpful AI manufacturing assistant\")\n",
    "\n",
    "                    prompt = ChatPromptTemplate(messages=[system_message])\n",
    "\n",
    "                    #prompt = PromptTemplate(input_variables=[\"chat_history\", \"question\"], template=\"{chat_history} {query}\")\n",
    "                    chat = ConversationChain(llm=self.llm,verbose=kwargs.get(\"verbose\",True),prompt=prompt,memory=self.memory_conversation ,input_key=\"question\")\n",
    "                    return chat.invoke({\"question\":query}).get(\"response\")\n",
    "                else:\n",
    "                    from langchain.schema import HumanMessage\n",
    "                    message = HumanMessage(content=query)\n",
    "                    return self.llm.invoke([message])\n",
    "\n",
    "            else:\n",
    "                #return response with llama index self.llm(response)\n",
    "                pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51c0c388-b1a0-4fb1-ae85-ce9bbce98394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: to use the LLM use it as msg = HumanMessage(content= your query) \n",
      " llm(messages = [msg]) \n",
      "Using Azure OpenAI\n",
      "Note: to use the LLM use it as msg = HumanMessage(content= your query) \n",
      " llm(messages = [msg]) \n",
      "Using Azure OpenAI\n",
      "Llama's version\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\o_hol\\AppData\\Roaming\\Python\\Python311\\site-packages\\scipy\\__init__.py:169: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-02-22 13:04:48 [ERROR]: Failed to write data to connection ResolvedIPv4Address(('34.121.155.65', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n",
      "2024-02-22 13:04:48 [ERROR]: Failed to write data to connection IPv4Address(('b75e0bd8.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n",
      "2024-02-22 13:04:48 [ERROR]: Failed to write data to connection ResolvedIPv4Address(('34.121.155.65', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n",
      "2024-02-22 13:04:48 [ERROR]: Failed to write data to connection IPv4Address(('b75e0bd8.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n",
      "2024-02-22 13:04:48 [ERROR]: Failed to write data to connection ResolvedIPv4Address(('34.121.155.65', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n",
      "2024-02-22 13:04:48 [ERROR]: Failed to write data to connection IPv4Address(('b75e0bd8.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n",
      "2024-02-22 13:04:48 [ERROR]: Failed to write data to connection ResolvedIPv4Address(('34.121.155.65', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n",
      "2024-02-22 13:04:48 [ERROR]: Failed to write data to connection IPv4Address(('b75e0bd8.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n",
      "2024-02-22 13:04:48 [ERROR]: Failed to write data to connection ResolvedIPv4Address(('34.121.155.65', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n",
      "2024-02-22 13:04:48 [ERROR]: Failed to write data to connection IPv4Address(('b75e0bd8.databases.neo4j.io', 7687)) (ResolvedIPv4Address(('34.121.155.65', 7687)))\n"
     ]
    }
   ],
   "source": [
    "#loading \n",
    "llmazure =getllm(openai=True,azure =True,mode = \"langchain\", auth=\"token\",model=\"gpt-35-turbo-16k\",dep_name = \"chat-bot-trail\",            \n",
    "                  api_version= \"2023-12-01-preview\", api_key = \"1bbc7a5ceb0e4677888f22d1eb4c8617\",api_base = \"https://chat-bot-iot.openai.azure.com/\")#([message])\n",
    "#LLAMA'S VERSION\n",
    "llmazure2 =getllm(openai=True,azure =True,mode = \"llama\", auth=\"token\",model=\"gpt-35-turbo-16k\",dep_name = \"chat-bot-trail\",            \n",
    "                  api_version= \"2023-05-15\", api_key = \"1bbc7a5ceb0e4677888f22d1eb4c8617\",api_base = \"https://chat-bot-iot.openai.azure.com/\")#([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d65e16c-4c52-4307-abb0-565660ef9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getllm(openai=True, azure=False, mode=\"langchain\",**kwargs):\n",
    "#     print(\"Note: to use the LLM use it as msg = HumanMessage(content= your query) \\n llm(messages = [msg]) \")\n",
    "#     if openai:\n",
    "#         temperature = kwargs.get(\"temperature\",0)\n",
    "#         if azure:\n",
    "#             print(\"Using Azure OpenAI\")\n",
    "#             # from langchain.chat_models import AzureChatOpenAI\n",
    "#             from langchain_openai import AzureChatOpenAI\n",
    "#             auth = kwargs.get(\"auth\")\n",
    "#             if auth.lower()==\"token\":\n",
    "#                 if mode.lower()==\"langchain\":\n",
    "#                     params = {\"model\": kwargs.get(\"model\"),\n",
    "#                         \"azure_deployment\": kwargs.get(\"dep_name\"),\n",
    "#                         \"openai_api_version\": kwargs.get(\"api_version\"),\n",
    "#                         \"temperature\": temperature,\n",
    "#                         \"openai_api_key\": kwargs.get(\"api_key\"),\n",
    "#                         \"azure_endpoint\": kwargs.get(\"api_base\"),\n",
    "#                         \"max_tokens\":kwargs.get(\"max_tokens\",256)}\n",
    "#                     llm = AzureChatOpenAI(**params)\n",
    "#                 elif mode.lower()==\"llama\":\n",
    "#                     print(\"Llama's version\")\n",
    "#                     #from llama_index.llms import AzureOpenAI\n",
    "#                     from llama_index.llms.azure_openai import AzureOpenAI\n",
    "#                     import openai\n",
    "#                     openai.api_type = \"azure\"\n",
    "#                     # openai.api_base =  kwargs.get(\"api_base\") #\"INSERT AZURE API BASE\"\n",
    "#                     # openai.api_version = kwargs.get(\"api_version\",\"2023-05-15\")\n",
    "                    \n",
    "#                     llm = AzureOpenAI(engine= kwargs.get(\"dep_name\"),temperature=0,model=kwargs.get(\"model\"),\n",
    "#                                      azure_deployment =kwargs.get(\"dep_name\"),azure_endpoint = kwargs.get(\"api_base\"),\n",
    "#                                      api_version = kwargs.get(\"api_version\",\"2023-05-15\"),api_key= kwargs.get(\"api_key\"))\n",
    "#                 return llm\n",
    "#             elif auth.lower() == \"azure_ad\":\n",
    "#                 #from azure.identity import DefaultAzureCredential\n",
    "#                 #credential = DefaultAzureCredential()\n",
    "#                 #Set the API type to `azure_ad`\n",
    "#                 #os.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n",
    "#                ##Set the API_KEY to the token from the Azure credential\n",
    "#                 #os.environ[\"OPENAI_API_KEY\"] = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token            elif auth.lower()==\"azure_ad_provider\":\n",
    "#                 llm = \"Not implemented\"\n",
    "#             elif auth == \"azure ad_provider\":\n",
    "#                 #managed_identity:\n",
    "#                 #from azure.identity import ChainedTokenCredential, ManagedIdentityCredential, AzureCliCredential   \n",
    "#                 #credential = ChainedTokenCredential(\n",
    "#                 #ManagedIdentityCredential(),\n",
    "#                 #AzureCliCredential())\n",
    "#                 llm = \"not implemented\"\n",
    "#         else:\n",
    "#             from langchain_openai import OpenAI\n",
    "#             api_base = kwargs.get(\"api_base_url\")\n",
    "#             api_key =kwargs.get(\"api_key\",\"NULL\")\n",
    "#             llm = OpenAI(\n",
    "#                          base_url = api_base,\n",
    "#                          openai_api_key = api_key,**kwargs)\n",
    "#             return llm \n",
    "#     else: #huggingface model\n",
    "#         pass    \n",
    "\n",
    "\n",
    "\n",
    "# def llm_cost(llm,message) -> \"Total cost in USD\":\n",
    "#     from langchain.callbacks import get_openai_callback\n",
    "#     with get_openai_callback() as cb:\n",
    "#         llm([message])\n",
    "#         cost=format(cb.total_cost, '.6f')\n",
    "#         print(f\"Total Cost (USD): ${cost}\")\n",
    "#     return cost\n",
    "\n",
    "\n",
    "\n",
    "#     def get_embeddings(self,documents=None,lib='langchain'):\n",
    "# #         if not documents:\n",
    "# #             documents = pass\n",
    "#         #huggingface or langchain\n",
    "#         if lib=='langchain':\n",
    "#             from langchain.embeddings import OpenAIEmbeddings\n",
    "#             embeddings_model = OpenAIEmbeddings()\n",
    "#             embeddings = embeddings_model.embed_documents(documents)\n",
    "#             return emebeddings\n",
    "#         else:\n",
    "#             from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#             embeddings = HuggingFaceEmbeddings(model_name=self.huggingembed)\n",
    "#             return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a157a285-ee20-4dda-98cc-21029df6c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_graphdb(args,**kwargs):\n",
    "#     graphdb = kwargs.get(\"graphdb\")\n",
    "#     url,username,password,database = kwargs.get(\"url\"), kwargs.get(\"username\"), kwargs.get(\"password\"), kwargs.get(\"database\")\n",
    "#     if args.lower()=='llama':\n",
    "#         #from llama_index.storage.storage_context import StorageContext\n",
    "#         from llama_index.core import StorageContext\n",
    "#         print('Using LLama Index graph class')\n",
    "#         if graphdb.lower()=='neo4j' or 'neo4j' in url:\n",
    "#             from llama_index.graph_stores.neo4j import Neo4jGraphStore\n",
    "\n",
    "#             graph_store = Neo4jGraphStore(\n",
    "#                 username=username,\n",
    "#                 password=password,\n",
    "#                 url=url,\n",
    "#                 database=database,\n",
    "#             )\n",
    "#             #build storage context\n",
    "#             storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "#             return graph_store, storage_context\n",
    "#         elif graphdb.lower()=='nebula':\n",
    "#             from llama_index.graph_stores.nebula import NebulaGraphStore\n",
    "#             #TBD get varibales from kwargs\n",
    "#             graph_store = NebulaGraphStore(\n",
    "#             space_name=kwargs.get(\"space_name\"),\n",
    "#             edge_types=kwargs.get(\"edge_types\"),\n",
    "#             rel_prop_names=kwargs.get(\"rel_prop_names\"),\n",
    "#             tags=kwargs.get(\"tags\"))\n",
    "#             storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "#             return graph_store, storage_context\n",
    "        \n",
    "#         else:\n",
    "#             raise(\"Error not implemented for that graph database\")\n",
    "    \n",
    "\n",
    "#     elif args.lower()=='langchain':\n",
    "#         print('Using langchain graph class')\n",
    "#         if graphdb.lower()=='neo4j' or 'neo4j' in url:\n",
    "#             from langchain.graphs import Neo4jGraph\n",
    "#             graph= Neo4jGraph(\n",
    "#                url=url,\n",
    "#                username=username,\n",
    "#                password=password\n",
    "#             )\n",
    "    \n",
    "#             return graph\n",
    "#         elif graphdb.lower()==\"falkordb\":\n",
    "#             print(\"Using falkordb\")\n",
    "#             from langchain.graphs import FalkorDBGraph\n",
    "#             graph = FalkorDBGraph(database = database,host=url,port=6379, username=username,password = pwd)\n",
    "\n",
    "\n",
    "#         else:\n",
    "#             raise(\"Error not implemented for that graph database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749eb20-c10e-4c53-ab6b-e8add1cbb6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ddb8fd-c907-4ea3-987a-eb7b1f293439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIATE THE GRAPH STORE\n",
    "url = \"neo4j+s://b75e0bd8.databases.neo4j.io\"\n",
    "username= \"neo4j\"\n",
    "password = \"aziMmumvuVru-rgeKuO0zT7mEtTxu29Jgrmp6lFKv0w\"\n",
    "graph_store = get_graphdb(\"langchain\",graphdb=\"neo4j\",url=url,username=username,password=password)\n",
    "#GRAPH STORE LLAMA \n",
    "graph_store_llama, storage_context = get_graphdb(\"llama\",graphdb=\"neo4j\",url=url,username=username,password=password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee31228-85f8-4707-86b2-94bde63dba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import KnowledgeGraphQueryEngine\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import KnowledgeGraphRAGRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d267e1-325b-443f-a9d6-52ae97922a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag2 = RAG2(llmazure2,graph_store_llama,type=\"llama\")\n",
    "service_context= \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9630d1e-31c2-4967-abed-1c5fc7761072",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=storage_context,\n",
    "                    llm=llmazure2,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=True)\n",
    "\n",
    "#query_engine = RetrieverQueryEngine.from_args(graph_rag_retriever, service_context=kwargs.get(\"service_context\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e46cce-9a6f-4951-ad10-8d894982d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag_retriever = KnowledgeGraphRAGRetriever(        \n",
    "                    storage_context=storage_context,\n",
    "                    llm=llmazure2,\n",
    "                    verbose=True,\n",
    "                    with_nl2graphquery=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e62cda-4fec-4eba-b306-ebe74ad79771",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmazure2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3d41d-7a75-409a-9917-80451f5bcbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine.from_args(\n",
    "    graph_rag_retriever,\n",
    "    llm = llmazure2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a93430-eefd-47e5-8a82-acaa12288645",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.query(\n",
    "    \"what did the study concluded\",\n",
    ").response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d69df8-f956-4e76-9bd9-0873061b10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rag_retriever_with_nl2graphquery = KnowledgeGraphRAGRetriever(\n",
    "    storage_context=storage_context,\n",
    "    verbose=True,\n",
    "    with_nl2graphquery=True,\n",
    "    llm = llmazure2\n",
    ")\n",
    "\n",
    "query_engine_with_nl2graphquery = RetrieverQueryEngine.from_args(\n",
    "    graph_rag_retriever_with_nl2graphquery,\n",
    "    llm = llmazure2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e977b64-12bd-43d3-8982-9540894bc499",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_with_nl2graphquery.query(\n",
    "    \"what did the study concluded\",\n",
    ").response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932060df-425a-46ce-abb3-32e7cf0e2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add it to langchain tools\n",
    "from llama_index.core.langchain_helpers.agents import (\n",
    "    IndexToolConfig,\n",
    "    LlamaIndexTool,\n",
    ")\n",
    "\n",
    "tool_config = IndexToolConfig(\n",
    "    query_engine=query_engine_with_nl2graphquery,\n",
    "    name=f\"KG Index\",\n",
    "    description=f\"useful for when you want to answer queries about neo4j\",\n",
    "    tool_kwargs={\"return_direct\": False},\n",
    ")\n",
    "\n",
    "tool = LlamaIndexTool.from_tool_config(tool_config)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f32b41b-e05d-410f-9c5b-11e686a6c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool.run(\"tell me all you know about a study\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa833d45-a18a-4ef1-9570-6c8e384bc66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa5b50-9096-4603-8efa-837e7b08f496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1744d16e-332e-489a-97d2-440681c883e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6b1a6-966c-41f9-83b1-4230307c9d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b1105-67e5-4799-8f73-b73da416dbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d298b499-ac0c-4fc7-afac-e540346eef65",
   "metadata": {},
   "source": [
    "## Other test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ff353c-dfd4-4f1f-8fd3-4b134442c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, create_structured_chat_agent\n",
    "\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c780fb-f2f8-4009-842a-0c2e50536173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llmazure.bind_tools(tool)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"LlamaIndex\",\n",
    "        func=lambda q: query_engine_with_nl2graphquery.query(q),\n",
    "        description=\"useful for when you want to answer questions about neo4j kg.\",\n",
    "        return_direct=False,\n",
    "    ),\n",
    "]\n",
    "\n",
    "llm_with_tools= llmazure.bind_tools(tools)\n",
    "\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7378a32b-8678-40e3-a233-989f0a604ed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tools[0].run(\"what did the study concluded?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d37981-5ee0-4b42-baae-b28495943933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits.conversational_retrieval.openai_functions import create_conversational_retrieval_agent\n",
    "\n",
    "agent_executor = create_conversational_retrieval_agent(llm=llmazure, tools=[\n",
    "    tools[0]], verbose=True)\n",
    "agent_executor({\"input\": \"what did the study concluded about the lifespan?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcdd7ae-e994-4868-8769-8675411bf489",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke(\"what did the study concluded?\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b0dab-5b23-401e-8699-aeb25e2f2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful manufacturing AI Assistant, you specialize in Quality department but have expertise in general manufacturing\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "    \n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97d154-e2d9-4e90-b991-840cdc6d2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5dd06-4151-41a3-ab4c-67accf75cec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
